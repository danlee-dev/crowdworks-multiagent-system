# -*- coding: utf-8 -*-
"""
ë©€í‹° ì¸ë±ìŠ¤ RAG ê²€ìƒ‰ ì—”ì§„ (JSON ê²°ê³¼ ë²„ì „) with Query-time ë™ì˜ì–´ í™•ì¥
- multi_index_search_engine.pyì˜ ëª¨ë“  ë¡œì§ í¬í•¨
- ê²°ê³¼ë¥¼ HTML ëŒ€ì‹  VectorDB.json íŒŒì¼ë¡œ ì €ì¥
- rag_config.pyì™€ synonyms.jsonì´ ê°™ì€ ë””ë ‰í† ë¦¬ì— ìˆì–´ì•¼ ë™ì‘
"""
import sys
import os
import json
import torch
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from elasticsearch import Elasticsearch
import openai
from datetime import datetime
import re
from rag_config import RAGConfig
from sentence_transformers import SentenceTransformer, CrossEncoder
import cohere

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# UTF-8 ì¶œë ¥ ì„¤ì •
if hasattr(sys.stdout, 'reconfigure'):
    sys.stdout.reconfigure(encoding='utf-8')
if hasattr(sys.stdin, 'reconfigure'):
    sys.stdin.reconfigure(encoding='utf-8')
    
BGE_RERANKER = CrossEncoder(
    "dragonkue/bge-reranker-v2-m3-ko",
    activation_fn=torch.nn.Sigmoid(),
    device=DEVICE
)

class MultiIndexRAGSearchEngine:
    def __init__(
        self,
        openai_api_key: str = None,
        cohere_api_key: str = None,
        es_host: str = None,
        es_user: str = None,
        es_password: str = None,
        config: RAGConfig = None
    ):
        # ì„¤ì • ë¡œë“œ
        if config is None:
            config = RAGConfig()
        api_key = openai_api_key or config.OPENAI_API_KEY
        if api_key == "your-openai-api-key-here":
            raise ValueError("OpenAI API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš” (rag_config.py ë˜ëŠ” ì´ˆê¸°í™” íŒŒë¼ë¯¸í„°)")
        
        # Cohere API í‚¤ ì„¤ì •
        cohere_key = cohere_api_key or os.getenv("COHERE_API_KEY")
        if not cohere_key:
            raise ValueError("Cohere API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš” (í™˜ê²½ë³€ìˆ˜ COHERE_API_KEY ë˜ëŠ” ì´ˆê¸°í™” íŒŒë¼ë¯¸í„°)")
        
        # OpenAI & Elasticsearch & Cohere í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = openai.OpenAI(api_key=api_key)
        self.cohere_client = cohere.Client(cohere_key)
        self.es = Elasticsearch(
            es_host or config.ELASTICSEARCH_HOST,
            basic_auth=(es_user or config.ELASTICSEARCH_USER,
                        es_password or config.ELASTICSEARCH_PASSWORD)
        )
        
        # Hugging Face ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        self.hf_model = SentenceTransformer("dragonkue/bge-m3-ko")
        self.reranker = BGE_RERANKER
        # ì¸ë±ìŠ¤ ì„¤ì •
        self.TEXT_INDEX = "page_text"
        self.TABLE_INDEX = "page_table"
        # Config íŒŒë¼ë¯¸í„°
        self.config = config
        self.HYBRID_ALPHA = config.HYBRID_ALPHA
        self.TOP_K_RETRIEVAL = config.TOP_K_RETRIEVAL
        self.TOP_K_RERANK = config.TOP_K_RERANK
        self.TOP_K_FINAL = config.TOP_K_FINAL
        self.USE_HYDE = config.USE_HYDE
        self.USE_RERANKING = config.USE_RERANKING
        self.USE_SUMMARIZATION = config.USE_SUMMARIZATION
        self.HYDE_MAX_TOKENS = config.HYDE_MAX_TOKENS
        self.HYDE_TEMPERATURE = config.HYDE_TEMPERATURE
        self.HYDE_MODEL = config.HYDE_MODEL
        self.SUMMARIZATION_MAX_TOKENS = config.SUMMARIZATION_MAX_TOKENS
        self.SUMMARIZATION_RATIO = config.SUMMARIZATION_RATIO
        self.DOMAIN_KEYWORDS = config.DOMAIN_KEYWORDS

        # ë™ì˜ì–´ ì‚¬ì „ ë¡œë“œ (synonyms.json)
        syn_path = os.path.join(os.path.dirname(__file__), "synonyms.json")
        if os.path.exists(syn_path):
            with open(syn_path, encoding='utf-8') as f:
                self.synonym_dict = json.load(f)
        else:
            self.synonym_dict = {}

    def embed_text(self, text: str) -> List[float]:
        try:
            safe_text = text.encode('utf-8', errors='ignore').decode('utf-8')
            embedding = self.hf_model.encode(safe_text)
            return embedding.tolist()
        except Exception as e:
            print(f"ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {e}")
            return []

    def query_enhancement_hyde_text(self, query: str) -> str:
        # ... ê¸°ì¡´ HyDE text ë¡œì§ ê·¸ëŒ€ë¡œ ìœ ì§€ ...
        try:
            prompt = f"""
ì‹í’ˆ ë„ë©”ì¸ì— ê´€ë ¨ëœ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ìƒì„¸í•˜ê³  ì •í™•í•œ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”. 
ì´ ë‹µë³€ì€ ê²€ìƒ‰ì„ ê°œì„ í•˜ê¸° ìœ„í•œ ê²ƒì´ë¯€ë¡œ ê°€ëŠ¥í•œ í•œ êµ¬ì²´ì ì´ê³  ì „ë¬¸ì ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
ë§ˆí¬ë‹¤ìš´ ë¬¸ë²•ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  ì¤„ê¸€ë¡œ ì¨ì£¼ì„¸ìš”.
ìµœëŒ€ {self.HYDE_MAX_TOKENS} í† í° ì´ë‚´ë¡œ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.

ì§ˆë¬¸: {query}
ë‹µë³€:"""
            response = self.client.chat.completions.create(
                model=self.HYDE_MODEL,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=self.HYDE_MAX_TOKENS,
                temperature=self.HYDE_TEMPERATURE
            )
            hypothetical_doc = response.choices[0].message.content.strip()
            return f"{query} {hypothetical_doc}"
        except Exception as e:
            print(f"HyDE(text) ì˜¤ë¥˜: {e}")
            return query

    def query_enhancement_hyde_table(self, query: str) -> str:
        # ... ê¸°ì¡´ HyDE table ë¡œì§ ê·¸ëŒ€ë¡œ ìœ ì§€ ...
        try:
            prompt = f"""
ì•„ë˜ ì§ˆë¬¸ì— ëŒ€í•´ í‘œ í˜•ì‹ì˜ ê°€ìƒ í†µê³„ ë°ì´í„°ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
- ì²« ì¤„ì—ëŠ” í‘œ ì œëª©ì„ ì¨ì£¼ì„¸ìš”.
- ë‘ ë²ˆì§¸ ì¤„ì—ëŠ” ì—´ ì´ë¦„(í—¤ë”)ì„ ê³µë°±ìœ¼ë¡œ êµ¬ë¶„í•´ì„œ ì¨ì£¼ì„¸ìš”.
- ê·¸ ì•„ë˜ì—ëŠ” ê° í–‰ì˜ ë°ì´í„°ë¥¼ ê³µë°±ìœ¼ë¡œ êµ¬ë¶„í•´ì„œ **ìµœì†Œ 10ì¤„ ì´ìƒ** ì¨ì£¼ì„¸ìš”.
- í‘œëŠ” ë§ˆí¬ë‹¤ìš´, HTML, íŒŒì´í”„(|) ì—†ì´, ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ í‘œ í˜•ì‹(ê³µë°± êµ¬ë¶„)ìœ¼ë¡œë§Œ ì‘ì„±í•´ì£¼ì„¸ìš”.
- ê°€ëŠ¥í•œ í•œ ë§ì€ ì •ë³´ë¥¼ í¬í•¨í•˜ê³ , ê° í–‰ì˜ ë‚´ìš©ë„ êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {query}

ì˜ˆì‹œ:
í‘œì œëª©: 2024ë…„ ì‹í’ˆ í†µê³„ ì£¼ìš” í•­ëª©
í•­ëª©ëª… ìˆ˜ì¹˜ ë‹¨ìœ„ ë¹„ê³ 
ìŒ€ ìƒì‚°ëŸ‰ 370ë§Œ í†¤ ì „êµ­ ê¸°ì¤€
ë°€ ìˆ˜ì…ëŸ‰ 250ë§Œ í†¤ ì£¼ìš” 5ê°œêµ­
ì™¸ì‹ì—… ë§¤ì¶œ 120ì¡° ì› 2024ë…„ ê¸°ì¤€
ì‹í’ˆ ìˆ˜ì¶œì•¡ 80ì–µ ë‹¬ëŸ¬ ì „ë…„ ëŒ€ë¹„ 5% ì¦ê°€

ì´ì™€ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
"""
            response = self.client.chat.completions.create(
                model=self.HYDE_MODEL,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=self.HYDE_MAX_TOKENS,
                temperature=self.HYDE_TEMPERATURE
            )
            hypothetical_doc = response.choices[0].message.content.strip()
            return f"{query} {hypothetical_doc}"
        except Exception as e:
            print(f"HyDE(table) ì˜¤ë¥˜: {e}")
            return query

    # ========== ë™ì˜ì–´ í™•ì¥ í—¬í¼ ==========
    def expand_terms(self, tokens: List[str]) -> Dict[str, List[str]]:
        """
        í† í° ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ì„œ ë™ì˜ì–´ ì‚¬ì „ ê¸°ë°˜ìœ¼ë¡œ í™•ì¥ëœ variants ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
        {ì›ì–´: [ì›ì–´, ë™ì˜ì–´1, ë™ì˜ì–´2, ...], ...}
        """
        expanded: Dict[str, List[str]] = {}
        for t in tokens:
            variants = [t]
            if t in self.synonym_dict:
                variants.extend(self.synonym_dict[t])
            expanded[t] = variants
        return expanded

    def build_synonym_expanded_query(self, query: str, top_k: int) -> Dict[str, Any]:
        """
        ì›ë³¸ ì¿¼ë¦¬ì™€ ë™ì˜ì–´ ì‚¬ì „ì„ ê¸°ë°˜ìœ¼ë¡œ bool should í™•ì¥ ì¿¼ë¦¬ ìƒì„±
        """
        tokens = query.strip().split()
        expanded = self.expand_terms(tokens)

        # ê¸°ë³¸ cross_fields ë§¤ì¹­
        should_clauses: List[Dict[str, Any]] = [
            {
                "multi_match": {
                    "query": query,
                    "type": "cross_fields",
                    "fields": [
                        "page_content^2",
                        "page_content.ngram^1",
                        "name^3",
                        "meta_data.document_title^2"
                    ],
                    "operator": "and"
                }
            }
        ]
        # ê° í† í°ê³¼ variantsë¡œ match/phrase ì¶”ê°€
        for variants in expanded.values():
            for v in variants:
                should_clauses.append({"match": {"name": {"query": v, "boost": 2.5}}})
                should_clauses.append({"match_phrase": {"meta_data.document_title": {"query": v, "boost": 2.0}}})
                should_clauses.append({"match": {"page_content.ngram": {"query": v, "boost": 1.0, "operator": "and"}}})

        bool_query = {"bool": {"should": should_clauses, "minimum_should_match": 1}}
        return {
            "size": top_k,
            "query": bool_query,
            "_source": ["page_content", "name", "meta_data"]
        }

    # ========== Retrieval ==========
    def dense_retrieval(self, query: str, top_k: int = 100) -> List[Dict]:
        # ... ê¸°ì¡´ dense_retrieval ë¡œì§ ìœ ì§€ (opt: í†µí•© ê°€ëŠ¥) ...
        vector = self.embed_text(query)
        if not vector:
            return []
        results = []
        for index in [self.TEXT_INDEX, self.TABLE_INDEX]:
            body = {
                "size": top_k,
                "knn": {"field": "embedding", "query_vector": vector, "k": top_k, "num_candidates": min(top_k*2, 200)},
                "_source": ["page_content", "name", "meta_data"]
            }
            try:
                response = self.es.search(index=index, body=body)
                hits = response.get("hits", {}).get("hits", [])
                for hit in hits:
                    src = hit["_source"]
                    results.append({
                        "score": hit["_score"],
                        "page_content": src.get("page_content", ""),
                        "name": src.get("name", ""),
                        "meta_data": src.get("meta_data", {}),
                        "search_type": "dense",
                        "_index": index
                    })
            except Exception as e:
                print(f"Dense ê²€ìƒ‰ ì˜¤ë¥˜({index}): {e}")
        return results

    def sparse_retrieval(self, query: str, top_k: int = 100) -> List[Dict]:
        """
        indexë³„ë¡œ query-time ë™ì˜ì–´ í™•ì¥ ì¿¼ë¦¬ë¥¼ ì‚¬ìš©í•œ sparse ê²€ìƒ‰
        """
        results = []
        for index in [self.TEXT_INDEX, self.TABLE_INDEX]:
            body = self.build_synonym_expanded_query(query, top_k)
            try:
                response = self.es.search(index=index, body=body)
                hits = response.get("hits", {}).get("hits", [])
                for hit in hits:
                    src = hit["_source"]
                    results.append({
                        "score": hit["_score"],
                        "page_content": src.get("page_content", ""),
                        "name": src.get("name", ""),
                        "meta_data": src.get("meta_data", {}),
                        "search_type": "sparse",
                        "_index": index
                    })
            except Exception as e:
                print(f"Sparse ê²€ìƒ‰ ì˜¤ë¥˜({index}): {e}")
        return results

    def normalize_scores(self, results: List[Dict], score_field: str = "score") -> List[Dict]:
        if not results:
            return results
        scores = [r[score_field] for r in results]
        if not scores or max(scores) == min(scores):
            return results
        min_score, max_score = min(scores), max(scores)
        for r in results:
            orig = r[score_field]
            r[f"normalized_{score_field}"] = (orig - min_score) / (max_score - min_score)
        return results

    def hybrid_search(
        self,
        query: str,
        alpha: float = None,
        top_k: int = 100,
        enhanced_query_text: Optional[str] = None,
        enhanced_query_table: Optional[str] = None
    ) -> List[Dict]:
        # ... ê¸°ì¡´ hybrid_search ë¡œì§ ìœ ì§€, ë‚´ë¶€ sparse_retrieval_index í˜¸ì¶œì´ synonyms í¬í•¨ ...
        if alpha is None:
            alpha = self.HYBRID_ALPHA
        if not (self.USE_HYDE and enhanced_query_text and enhanced_query_table):
            enhanced_query_text = query
            enhanced_query_table = query

        # dense + sparse
        dense_results = self.dense_retrieval_index(enhanced_query_text, self.TEXT_INDEX, top_k)
        sparse_results = self.sparse_retrieval_index(enhanced_query_text, self.TEXT_INDEX, top_k)
        dense_results += self.dense_retrieval_index(enhanced_query_table, self.TABLE_INDEX, top_k)
        sparse_results += self.sparse_retrieval_index(enhanced_query_table, self.TABLE_INDEX, top_k)

        dense_results = self.normalize_scores(dense_results)
        sparse_results = self.normalize_scores(sparse_results)
        # ... score combine ìƒëµ (ì›ë³¸ê³¼ ë™ì¼) ...
        doc_scores: Dict[str, Any] = {}
        for res in dense_results:
            doc_id = self._get_doc_id(res)
            doc_scores.setdefault(doc_id, {"doc": res, "dense_score": 0.0, "sparse_score": 0.0})
            doc_scores[doc_id]["dense_score"] = res.get("normalized_score", 0)
        for res in sparse_results:
            doc_id = self._get_doc_id(res)
            doc_scores.setdefault(doc_id, {"doc": res, "dense_score": 0.0, "sparse_score": 0.0})
            doc_scores[doc_id]["sparse_score"] = res.get("normalized_score", 0)

        hybrid_results: List[Dict] = []
        for doc_id, scores in doc_scores.items():
            hybrid_score = alpha * scores["sparse_score"] + (1 - alpha) * scores["dense_score"]
            r = scores["doc"].copy()
            r.update({
                "hybrid_score": hybrid_score,
                "dense_component": scores["dense_score"],
                "sparse_component": scores["sparse_score"],
                "search_type": "hybrid"
            })
            hybrid_results.append(r)
        hybrid_results.sort(key=lambda x: x["hybrid_score"], reverse=True)
        return hybrid_results[:top_k]

    def dense_retrieval_index(self, query: str, index: str, top_k: int = 100) -> List[Dict]:
        # ... ê¸°ì¡´ dense_retrieval_index ë¡œì§ ê·¸ëŒ€ë¡œ ìœ ì§€ ...
        vector = self.embed_text(query)
        if not vector:
            return []
        results = []
        body = {
            "size": top_k,
            "knn": {"field": "embedding", "query_vector": vector, "k": top_k, "num_candidates": min(top_k*2, 200)},
            "_source": ["page_content", "name", "meta_data"]
        }
        try:
            response = self.es.search(index=index, body=body)
            hits = response.get("hits", {}).get("hits", [])
            for hit in hits:
                src = hit["_source"]
                results.append({
                    "score": hit["_score"],
                    "page_content": src.get("page_content", ""),
                    "name": src.get("name", ""),
                    "meta_data": src.get("meta_data", {}),
                    "search_type": "dense",
                    "_index": index
                })
        except Exception as e:
            print(f"Dense ê²€ìƒ‰ ì˜¤ë¥˜({index}): {e}")
        return results

    def sparse_retrieval_index(self, query: str, index: str, top_k: int = 100) -> List[Dict]:
        # multi-index RAG hybrid_search ë‚´ì—ì„œ ì‚¬ìš©ë˜ëŠ” sparse ê²€ìƒ‰
        results = []
        # Query-time ë™ì˜ì–´ í™•ì¥ ì¿¼ë¦¬
        body = self.build_synonym_expanded_query(query, top_k)
        try:
            response = self.es.search(index=index, body=body)
            hits = response.get("hits", {}).get("hits", [])
            for hit in hits:
                src = hit["_source"]
                results.append({
                    "score": hit["_score"],
                    "page_content": src.get("page_content", ""),
                    "name": src.get("name", ""),
                    "meta_data": src.get("meta_data", {}),
                    "search_type": "sparse",
                    "_index": index
                })
        except Exception as e:
            print(f"Sparse ê²€ìƒ‰ ì˜¤ë¥˜({index}): {e}")
        return results

    def _get_doc_id(self, result: Dict) -> str:
        meta = result.get("meta_data", {})
        chunk_id = meta.get("chunk_id", "")
        name = result.get("name", "")
        content_hash = str(hash(result.get("page_content", "")))
        return f"{chunk_id}_{name}_{content_hash}"

    def cohere_reranking(self, results: List[Dict], query: str, top_k: int = 20) -> List[Dict]:
        """
        Cohere rerankerë¥¼ ì‚¬ìš©í•œ ì¬ìˆœìœ„í™”
        """
        if not results or len(results) == 0:
            return results
            
        try:
            # Cohere rerankerì— ì „ë‹¬í•  ë¬¸ì„œë“¤ ì¤€ë¹„
            documents = []
            for r in results[:top_k]:
                content = r.get("page_content", "")
                name = r.get("name", "")
                # ë¬¸ì„œ ì œëª©ê³¼ ë‚´ìš©ì„ ê²°í•©
                doc_text = f"ì œëª©: {name}\në‚´ìš©: {content}"
                documents.append(doc_text)
            
            if not documents:
                return results
                
            # Cohere reranker í˜¸ì¶œ
            response = self.cohere_client.rerank(
                query=query,
                documents=documents,
                top_n=len(documents),
                model="rerank-v3.5"  # ìµœì‹  ë‹¤êµ­ì–´ ì§€ì› ëª¨ë¸
            )
            
            # ê²°ê³¼ ì¬ì •ë ¬
            reranked_results = []
            for i, result in enumerate(response.results):
                original_index = result.index
                if original_index < len(results):
                    r = results[original_index].copy()
                    r["rerank_score"] = result.relevance_score
                    r["rerank_rank"] = i + 1
                    reranked_results.append(r)
            
            # rerank_scoreë¡œ ì •ë ¬
            reranked_results.sort(key=lambda x: x["rerank_score"], reverse=True)
            
            return reranked_results
            
        except Exception as e:
            print(f"Cohere reranker ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ë°œìƒì‹œ ì›ë³¸ ê²°ê³¼ ë°˜í™˜
            return results[:top_k]
    
    
    def BGE_rerank(self, results, query, top_k=20):
        if not results:
            return results

        subset = results[:top_k]
        try: 
            pairs = []
            for r in subset:
                content = r.get("page_content", "") or ""
                name = r.get("name", "") or ""
                doc_text = f"ì œëª©: {name}\në‚´ìš©: {content}".strip()
                pairs.append([query, doc_text if doc_text else content])

            if not pairs:
                return subset

            scores = self.reranker.predict(pairs, batch_size=32)
            
            # minmax ì •ê·œí™”
            min_s = min(scores)
            max_s = max(scores)
            if max_s != min_s:
                scores = [(s - min_s) / (max_s - min_s) for s in scores]
            else:
                scores = [0.0 for _ in scores]
            
            
            reranked = []
            for r, s in zip(subset, scores):
                item = r.copy()
                item["rerank_score"] = float(s)
                reranked.append(item)

            reranked.sort(key=lambda x: x["rerank_score"], reverse=True)
            for i, r in enumerate(reranked, start=1):
                r["rerank_rank"] = i

            return reranked
        
        except Exception as e:
            print(f"Cohere reranker ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ë°œìƒì‹œ ì›ë³¸ ê²°ê³¼ ë°˜í™˜
            return subset

    def simple_reranking(self, results: List[Dict], query: str, top_k: int = 20) -> List[Dict]:
        """
        Cohere rerankerë¥¼ ì‚¬ìš©í•œ ì¬ìˆœìœ„í™” (ê¸°ì¡´ simple_reranking ëŒ€ì²´)
        """
        with open('VectorDB_Original.json', 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        #return self.cohere_reranking(results, query, top_k)
        return self.BGE_rerank(results, query, top_k)

    def document_summarization(self, results: List[Dict], query: str) -> List[Dict]:
        # ... ê¸°ì¡´ document_summarization ë¡œì§ ìœ ì§€ ...
        summarized = []
        for r in results:
            content = r.get("page_content", "")
            if len(content) > 500:
                try:
                    prompt = f"""
ë‹¤ìŒ ë¬¸ì„œë¥¼ ì£¼ì–´ì§„ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ í•µì‹¬ ë‚´ìš© ìœ„ì£¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.
ìš”ì•½ ê¸¸ì´: ì›ë³¸ì˜ {self.SUMMARIZATION_RATIO}% ì •ë„
ì§ˆë¬¸: {query}
ë¬¸ì„œ ë‚´ìš©:
{content}
ìš”ì•½:"""
                    response = self.client.chat.completions.create(
                        model="gpt-3.5-turbo",
                        messages=[{"role": "user", "content": prompt}],
                        max_tokens=self.SUMMARIZATION_MAX_TOKENS,
                        temperature=0.3
                    )
                    r["summarized_content"] = response.choices[0].message.content.strip()
                except Exception as e:
                    print(f"ìš”ì•½ ì˜¤ë¥˜: {e}")
                    r["summarized_content"] = content
            else:
                r["summarized_content"] = content
            summarized.append(r)
        return summarized

    def advanced_rag_search(self, query: str) -> Dict[str, Any]:
        start = datetime.now()
        if self.USE_HYDE:
            q_text = self.query_enhancement_hyde_text(query)
            q_table = self.query_enhancement_hyde_table(query)
        else:
            q_text, q_table = query, query
        hybrid = self.hybrid_search(query, enhanced_query_text=q_text, enhanced_query_table=q_table)
        reranked = hybrid
        if self.USE_RERANKING and len(hybrid) > 5:
            reranked = self.simple_reranking(hybrid, query, self.TOP_K_RERANK)
        final = reranked[:self.TOP_K_FINAL]
        if self.USE_SUMMARIZATION:
            final = self.document_summarization(final, query)
        duration = (datetime.now() - start).total_seconds()
        return {
            "query": query,
            "enhanced_query": {"text": q_text, "table": q_table},
            "results": final,
            "total_candidates": len(hybrid),
            "final_count": len(final),
            "processing_time": duration,
            "config": {
                "hybrid_alpha": self.HYBRID_ALPHA,
                "use_hyde": self.USE_HYDE,
                "use_reranking": self.USE_RERANKING,
                "use_summarization": self.USE_SUMMARIZATION
            }
        }

def search(query: str, top_k: int = 50) -> Optional[List[Dict]]:
    config = RAGConfig()
    
    # OpenAI API í‚¤ í™•ì¸
    if config.OPENAI_API_KEY == "your-openai-api-key-here":
        print("âš ï¸ rag_config.pyì—ì„œ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        api_key = input("OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ë˜ëŠ” Enterë¡œ ìŠ¤í‚µ): ").strip()
        if not api_key:
            print("âŒ OpenAI API í‚¤ ì—†ì´ëŠ” í…ŒìŠ¤íŠ¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
    else:
        api_key = config.OPENAI_API_KEY
    
    # Cohere API í‚¤ í™•ì¸
    cohere_key = os.getenv("COHERE_API_KEY")
    if not cohere_key:
        print("âš ï¸ í™˜ê²½ë³€ìˆ˜ì—ì„œ Cohere API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        cohere_key = input("Cohere API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ë˜ëŠ” Enterë¡œ ìŠ¤í‚µ): ").strip()
        if not cohere_key:
            print("âŒ Cohere API í‚¤ ì—†ì´ëŠ” rerankingì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
    
    engine = MultiIndexRAGSearchEngine(openai_api_key=api_key, cohere_api_key=cohere_key, config=config)
    if not query:
        print("ê²€ìƒ‰ì–´ê°€ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None
    print("ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")
    results = engine.advanced_rag_search(query)
    top_results = results.get('results', [])[:top_k]
    with open('VectorDB.json', 'w', encoding='utf-8') as f:
        json.dump(top_results, f, ensure_ascii=False, indent=2)
    print(f"ê²€ìƒ‰ ê²°ê³¼ê°€ VectorDB.json íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. (ì´ {len(top_results)}ê°œ)")
    return top_results


def main():
    print("ğŸ¯ ë©€í‹° ì¸ë±ìŠ¤ RAG ê²€ìƒ‰ ì—”ì§„ (JSON ê²°ê³¼)")
    try:
        query = input("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
        if not query:
            print("ê²€ìƒ‰ì–´ê°€ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        top_k = 50
        top_k_input = input("ì¶œë ¥í•  ê²°ê³¼ ê°œìˆ˜(ê¸°ë³¸ 50): ").strip()
        if top_k_input.isdigit():
            top_k = int(top_k_input)
        search(query, top_k)
    except KeyboardInterrupt:
        print("\nğŸ›‘ ê²€ìƒ‰ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback; traceback.print_exc()

if __name__ == "__main__":
    main()
