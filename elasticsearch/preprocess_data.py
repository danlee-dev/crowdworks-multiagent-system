# -*- coding: utf-8 -*-
import json
import re
from collections import defaultdict, Counter
import tiktoken

# íŒŒì¼ ê²½ë¡œ
INPUT_DIR = 'datas'
OUTPUT_DIR = 'preprocessed_datas'

# ì²­í¬ í¬ê¸° ì œí•œ ì„¤ì •
# MAX_CHILDREN_PER_CHUNK = 6  # ìµœëŒ€ í•˜ìœ„ í•­ëª© ìˆ˜ (ì œê±°)
MAX_CONTENT_LENGTH = 5000   # ìµœëŒ€ ë‚´ìš© ê¸¸ì´

# Chunking ì„¤ì •
CHUNKING_CONFIG = {
    "max_tokens_per_chunk": 1500,
    "overlap_tokens": 200,
    "short_text_threshold": 500,  # ì´ ë¯¸ë§Œì€ "ì§§ì€" text
    "min_texts_for_chunking": 2,  # ìµœì†Œ í•©ì¹  text ê°œìˆ˜
    "max_texts_per_chunk": 8      # í•œ ì²­í¬ì— í¬í•¨í•  ìµœëŒ€ text ê°œìˆ˜
}

# í† í° ê³„ì‚°ìš© ì¸ì½”ë” (OpenAI tiktoken)
try:
    tokenizer = tiktoken.get_encoding("cl100k_base")  # GPT-4, GPT-3.5-turbo ê¸°ë³¸ ì¸ì½”ë”©
except:
    tokenizer = None

def clean_essential_only(text):
    """ìµœì†Œí•œì˜ í…ìŠ¤íŠ¸ ì •ë¦¬ (ì–‘ë ê³µë°± ì œê±°ë§Œ)"""
    if not text:
        return text
    return text.strip()

def is_meaningful_data(item):
    """ì˜ë¯¸ìˆëŠ” ë°ì´í„°ì¸ì§€ íŒë‹¨"""
    page_content = item.get('page_content', '').strip()
    meta_data = item.get('meta_data', {})
    item_label = meta_data.get('item_label', '')
    
    # 1. ë¹ˆ ë‚´ìš©
    if not page_content:
        return False
    
    # 2. ëª©ì°¨ íŒ¨í„´ (í‘œ ëª©ë¡)
    if "<í‘œ" in page_content and "Â·Â·Â·" in page_content:
        return False
    
    # 3. ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ - hierarchy ë³‘í•© í›„ ì ìš©
    if item_label == "text" and len(page_content) < 30:
        return False
    
    # 4. ìˆ«ì/ê¸°í˜¸ë§Œ ìˆëŠ” ë‚´ìš©
    if re.match(r'^[\d\s\-\|\.\Â·\(\)]+$', page_content):
        return False
        
    return True

def merge_hierarchy_chunks(documents):
    """hierarchy ê´€ê³„ì— ìˆëŠ” itemë“¤ì„ í•˜ë‚˜ì˜ chunkë¡œ í•©ì¹˜ê¸° (í¬ê¸° ì œí•œ ì ìš©)"""
    
    # chunk_idë¡œ ë¬¸ì„œ ë§¤í•‘
    doc_map = {}
    for doc in documents:
        chunk_id = doc['meta_data']['chunk_id']
        doc_map[chunk_id] = doc
    
    # hierarchy ê´€ê³„ ë¶„ì„
    hierarchy_groups = defaultdict(list)
    orphans = []  # hierarchy ì—†ëŠ” ë…ë¦½ ë¬¸ì„œë“¤
    used_as_parent = set()  # ë¶€ëª¨ë¡œ ì‚¬ìš©ëœ ë¬¸ì„œë“¤ ì¶”ì 
    
    for doc in documents:
        hierarchy = doc['meta_data'].get('hierarchy')
        if hierarchy:
            hierarchy_groups[hierarchy].append(doc)
            # ë¶€ëª¨ ë¬¸ì„œ ì¶”ì 
            used_as_parent.add(hierarchy)
        else:
            orphans.append(doc)
    
    print(f"Hierarchy ë³‘í•©: {len(hierarchy_groups)}ê°œ ê·¸ë£¹, {len(orphans)}ê°œ í›„ë³´ ë…ë¦½ ë¬¸ì„œ")
    print(f"ë¶€ëª¨ë¡œ ì‚¬ìš©ëœ ë¬¸ì„œ: {len(used_as_parent)}ê°œ")
    
    merged_documents = []
    
    # 1. Hierarchy ê·¸ë£¹ë“¤ì„ í•©ì¹˜ê¸° (í¬ê¸° ì œí•œ ì ìš©)
    for parent_chunk_id, children in hierarchy_groups.items():
        parent_doc = doc_map.get(parent_chunk_id)
        
        if not parent_doc:
            print(f"âš ï¸ ë¶€ëª¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {parent_chunk_id}")
            # ìì‹ë“¤ì„ ê°œë³„ì ìœ¼ë¡œ ì²˜ë¦¬
            merged_documents.extend(children)
            continue
        
        # ìì‹ë“¤ì„ index ìˆœìœ¼ë¡œ ì •ë ¬
        children_sorted = sorted(children, key=lambda x: x['meta_data']['index'])
        
        # ëª¨ë“  ìì‹ë“¤ì„ í•˜ë‚˜ë¡œ ë³‘í•© (í¬ê¸° ì œí•œ ì—†ìŒ)
        merged_doc = create_single_merged_chunk(parent_doc, children_sorted)
        merged_documents.append(merged_doc)
    
    # 2. ë…ë¦½ ë¬¸ì„œë“¤ ì¶”ê°€ (ë¶€ëª¨ë¡œ ì‚¬ìš©ë˜ì§€ ì•Šì€ ê²ƒë“¤ë§Œ)
    actual_orphans = []
    for doc in orphans:
        chunk_id = doc['meta_data']['chunk_id']
        if chunk_id not in used_as_parent:
            actual_orphans.append(doc)
    
    for doc in actual_orphans:
        # name í•„ë“œ ì„¤ì •: tableì€ ë³„ë„ ì²˜ë¦¬, ë‚˜ë¨¸ì§€ëŠ” ì œëª© ì¶”ì¶œ
        name = ""
        item_label = doc['meta_data']['item_label']
        original_page_content = doc['page_content']
        
        if item_label == 'text':
            name = extract_title_from_content(doc['page_content'])
            processed_content = clean_essential_only(doc['page_content'])  # í•„ìˆ˜ ì •ë¦¬ ì ìš©
            
        elif item_label == 'table':
            # ë…ë¦½ í…Œì´ë¸”ì˜ ê²½ìš°
            name = extract_table_title_from_content(doc['page_content'])
            processed_content = optimize_table_for_search(doc['page_content'])
            # ì›ë³¸ ì •ë³´ë¥¼ merged_childrenì— ì €ì¥ (ë…ë¦½ ë¬¸ì„œë„ ë™ì¼í•œ êµ¬ì¡°)
            doc['meta_data']['merged_children'] = [{
                'item_label': 'table',
                'chunk_id': doc['meta_data']['chunk_id'],
                'content': doc['meta_data'].get('original_page_content', original_page_content),  # ì™„ì „ ì›ë³¸
                'summary': doc['meta_data'].get('summary', '')  # ì›ë˜ table summary ì‚¬ìš©
            }]
            
        else:
            name = extract_title_from_content(doc['page_content'])
            processed_content = clean_essential_only(doc['page_content'])  # í•„ìˆ˜ ì •ë¦¬ ì ìš©
        
        merged_doc = {
            "page_content": processed_content,
            "name": name,
            "meta_data": doc['meta_data']
        }
        merged_documents.append(merged_doc)
    
    print(f"ì‹¤ì œ ë…ë¦½ ë¬¸ì„œ: {len(actual_orphans)}ê°œ (ì¤‘ë³µ ì œê±°: {len(orphans) - len(actual_orphans)}ê°œ)")
    print(f"Hierarchy ë³‘í•© ì™„ë£Œ: {len(merged_documents)}ê°œ ë¬¸ì„œ")
    return merged_documents

def extract_table_title_from_content(content):
    """í‘œ ë‚´ìš©ì—ì„œ ì œëª© ì¶”ì¶œ ì‹œë„ (HTML í…Œì´ë¸” ìº¡ì…˜ ë˜ëŠ” ê¸°ì¡´ í˜•ì‹ ì§€ì›)"""
    if not content:
        return ""
    
    # HTML í…Œì´ë¸”ì—ì„œ caption ì¶”ì¶œ
    caption_match = re.search(r'<caption[^>]*>(.*?)</caption>', content, re.IGNORECASE | re.DOTALL)
    if caption_match:
        caption = caption_match.group(1).strip()
        # HTML íƒœê·¸ ì œê±°
        caption = re.sub(r'<[^>]+>', '', caption)
        if caption:
            return caption
    
    # ê¸°ì¡´ <í‘œ X-X> íŒ¨í„´ë„ ì§€ì› (í•˜ìœ„ í˜¸í™˜ì„±)
    lines = content.strip().split('\n')
    for line in lines:
        if '<í‘œ' in line and '>' in line:
            title = re.sub(r'<í‘œ[^>]*>', '', line)
            title = title.replace('|', '').strip()
            if title:
                return title
    
    return ""

def create_single_merged_chunk(parent_doc, children, suffix=""):
    """ë‹¨ì¼ merged ì²­í¬ ìƒì„±"""
    # í•©ì³ì§„ content ìƒì„±
    merged_content = create_merged_content(parent_doc, children)
    
    # ë‚´ìš©ì´ ë„ˆë¬´ ê¸¸ë©´ ì¶•ì†Œ
    if len(merged_content) > MAX_CONTENT_LENGTH:
        merged_content = merged_content[:MAX_CONTENT_LENGTH]
    
    # chunk_id ìƒì„±
    base_chunk_id = parent_doc['meta_data']['chunk_id']
    new_chunk_id = f"{base_chunk_id}{suffix}" if suffix else base_chunk_id
    
    # ìƒˆë¡œìš´ merged document ìƒì„±
    merged_doc = {
        "page_content": merged_content,
        "name": extract_title_from_content(parent_doc['page_content']),  # ë¶€ëª¨ ì œëª©ì´ name
        "meta_data": create_merged_metadata(parent_doc, children, new_chunk_id)
    }
    
    return merged_doc

def optimize_table_for_search(table_content):
    """í‘œ ë‚´ìš©ì„ ì„ë² ë”©/ê²€ìƒ‰ì— ìµœì í™”ëœ í˜•íƒœë¡œ ë³€í™˜ (HTML í…Œì´ë¸” ì§€ì›)"""
    if not table_content:
        return table_content
    
    # HTML í…Œì´ë¸”ì¸ì§€ í™•ì¸
    if '<table' in table_content.lower():
        return optimize_html_table_for_search(table_content)
    else:
        # ê¸°ì¡´ íŒŒì´í”„ êµ¬ë¶„ì í˜•ì‹ ì²˜ë¦¬ (í•˜ìœ„ í˜¸í™˜ì„±)
        return optimize_pipe_table_for_search(table_content)

def optimize_html_table_for_search(table_content):
    """HTML í…Œì´ë¸”ì„ ë‹¨ìˆœ ë‚˜ì—´ ë°©ì‹ìœ¼ë¡œ ë³€í™˜ (ë°ì´í„° ì •í™•ì„± ìš°ì„ )"""
    optimized_parts = []
    
    # ìº¡ì…˜ ì¶”ì¶œ
    caption = ""
    caption_match = re.search(r'<caption[^>]*>(.*?)</caption>', table_content, re.IGNORECASE | re.DOTALL)
    if caption_match:
        caption = re.sub(r'<[^>]+>', '', caption_match.group(1)).strip()
        if caption:
            optimized_parts.append(f"í‘œì œëª©: {caption}")
    
    # tbodyê°€ ìˆìœ¼ë©´ tbody ë‚´ìš©, ì—†ìœ¼ë©´ ì „ì²´ í…Œì´ë¸”ì—ì„œ tr ì¶”ì¶œ
    tbody_match = re.search(r'<tbody[^>]*>(.*?)</tbody>', table_content, re.IGNORECASE | re.DOTALL)
    if tbody_match:
        content_to_parse = tbody_match.group(1)
    else:
        content_to_parse = table_content
    
    # ëª¨ë“  tr íƒœê·¸ì—ì„œ ë‚´ìš© ì¶”ì¶œ (í—¤ë”/ë°ì´í„° êµ¬ë¶„ ì—†ì´)
    tr_matches = re.findall(r'<tr[^>]*>(.*?)</tr>', content_to_parse, re.IGNORECASE | re.DOTALL)
    
    row_parts = []
    row_num = 1
    for tr_content in tr_matches:
        # ëª¨ë“  ì…€(th, td) ë‚´ìš© ì¶”ì¶œ
        cells = re.findall(r'<t[dh][^>]*>(.*?)</t[dh]>', tr_content, re.IGNORECASE | re.DOTALL)
        
        # ì…€ ë‚´ìš© ì •ë¦¬
        cleaned_cells = []
        for cell in cells:
            cell_text = re.sub(r'<[^>]+>', '', cell).strip()
            if cell_text:  # ë¹„ì–´ìˆì§€ ì•Šì€ ì…€ë§Œ ì¶”ê°€
                cleaned_cells.append(clean_table_cell(cell_text))
        
        # ìœ íš¨í•œ ë°ì´í„°ê°€ ìˆëŠ” í–‰ë§Œ ì¶”ê°€
        if cleaned_cells:
            # ì„ë² ë”© ìµœì í™”: í–‰ ë²ˆí˜¸ì™€ êµ¬ë¶„ì ì œê±°, ê³µë°±ìœ¼ë¡œë§Œ êµ¬ë¶„
            row_content = " ".join(cleaned_cells)
            row_parts.append(row_content)
            row_num += 1
    
    # ì œëª©ê³¼ í–‰ë“¤ì„ ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ì—°ê²°
    if optimized_parts and row_parts:
        return optimized_parts[0] + "\n" + "\n".join(row_parts)
    elif row_parts:
        return "\n".join(row_parts)
    else:
        return ' '.join(optimized_parts) if optimized_parts else ""

def optimize_pipe_table_for_search(table_content):
    """ê¸°ì¡´ íŒŒì´í”„ êµ¬ë¶„ì í…Œì´ë¸” ì²˜ë¦¬ (í•˜ìœ„ í˜¸í™˜ì„±)"""
    lines = table_content.strip().split('\n')
    optimized_parts = []
    
    for line in lines:
        if '|' in line:
            # í‘œ ë¼ì¸: êµ¬ë¶„ì ì™„ì „ ì œê±°í•˜ê³  ì••ì¶•
            cells = [cell.strip() for cell in line.split('|') if cell.strip()]
            if cells:
                # ì„ë² ë”© ìµœì í™”: ê³µë°±ê³¼ íŠ¹ìˆ˜ë¬¸ì ìµœì†Œí™”
                cleaned_cells = []
                for cell in cells:
                    cleaned = clean_table_cell(cell)
                    cleaned_cells.append(cleaned)
                optimized_parts.append(' '.join(cleaned_cells))
        else:
            # í‘œê°€ ì•„ë‹Œ ë¼ì¸ (ì œëª©, ì¶œì²˜ ë“±)
            cleaned_line = line.strip()
            if cleaned_line and not re.match(r'^[\-\+\|\s]+$', cleaned_line):
                optimized_parts.append(cleaned_line)
    
    return ' '.join(optimized_parts)

def clean_table_cell(cell_content):
    """í…Œì´ë¸” ì…€ ë‚´ìš© ì •ë¦¬ (ìˆ«ì ì‰¼í‘œ ì œê±°, ê³µë°± ì••ì¶•)"""
    if not cell_content:
        return ""
    
    # ìˆ«ìì—ì„œ ì‰¼í‘œ ì œê±°, ê³µë°± ì••ì¶•
    cleaned = cell_content.replace(',', '').replace('  ', ' ').strip()
    return cleaned

def create_merged_content(parent_doc, children):
    """ë¶€ëª¨ì™€ ìì‹ë“¤ì˜ contentë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸° (ê²€ìƒ‰ ìµœì í™”)"""
    
    # ë¶€ëª¨ contentë¡œ ì‹œì‘ (í•„ìˆ˜ ì •ë¦¬ ì ìš©)
    parent_content = clean_essential_only(parent_doc['page_content'])
    content_parts = [parent_content]
    
    for child in children:
        item_label = child['meta_data']['item_label']
        child_content = child['page_content']  # ì›ë³¸ ìƒíƒœ
        
        if item_label == 'table':
            # ê²€ìƒ‰ ìµœì í™”ëœ í‘œ ë‚´ìš©ì„ page_contentì— ì¶”ê°€
            optimized_table = optimize_table_for_search(child_content)
            content_parts.append(f"\n{optimized_table}")
            
        elif item_label == 'text':
            # ì¶”ê°€ í…ìŠ¤íŠ¸ (ì¶œì²˜, ì£¼ì„ ë“±) - í•„ìˆ˜ ì •ë¦¬ ì ìš©
            if child_content.strip():
                cleaned_text = clean_essential_only(child_content)
                content_parts.append(f"\n{cleaned_text}")
    
    return '\n'.join(content_parts)

def extract_title_from_content(content):
    """contentì—ì„œ ì œëª© ì¶”ì¶œ (ìµœì†Œí•œì˜ ì •ë¦¬ë§Œ)"""
    if not content:
        return ""
    
    lines = content.strip().split('\n')
    if lines:
        # ë§ˆí¬ë‹¤ìš´ í—¤ë” ì œê±°ë§Œ
        title = lines[0].replace('#', '').strip()
        return title
    return ""

def create_merged_metadata(parent_doc, children, chunk_id):
    """í•©ì³ì§„ ë¬¸ì„œì˜ metadata ìƒì„±"""
    
    # ë¶€ëª¨ì˜ metadataë¥¼ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©
    merged_meta = parent_doc['meta_data'].copy()
    
    # chunk_id ì—…ë°ì´íŠ¸
    merged_meta['chunk_id'] = chunk_id
    
    # item_labelì„ "merged"ë¡œ ë³€ê²½
    merged_meta['item_label'] = 'merged'
    
    # ìì‹ë“¤ì˜ ì •ë³´ë¥¼ merged_childrenì— í†µí•© (contentì™€ summary í¬í•¨)
    merged_children = []
    
    for child in children:
        child_meta = {
            'item_label': child['meta_data']['item_label'],
            'chunk_id': child['meta_data']['chunk_id'],
            'content': child['meta_data'].get('original_page_content', child['page_content'])  # ì™„ì „ ì›ë³¸ ë‚´ìš©
        }
        
        # tableì¸ ê²½ìš° summary ì¶”ê°€ (ì›ë˜ table summary ì‚¬ìš©)
        if child['meta_data']['item_label'] == 'table':
            child_meta['summary'] = child['meta_data'].get('summary', '')
        
        merged_children.append(child_meta)
    
    merged_meta['merged_children'] = merged_children
    merged_meta['merged_count'] = len(children) + 1  # ë¶€ëª¨ í¬í•¨
    
    # í˜ì´ì§€ ë²”ìœ„ ê³„ì‚°
    all_pages = set(parent_doc['meta_data']['page_number'])
    for child in children:
        all_pages.update(child['meta_data']['page_number'])
    
    merged_meta['page_number'] = sorted(list(all_pages))
    
    return merged_meta

def count_tokens(text):
    """í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ê³„ì‚°"""
    if not text or not tokenizer:
        # tiktokenì´ ì—†ìœ¼ë©´ ëŒ€ëµì ìœ¼ë¡œ ê³„ì‚° (í•œêµ­ì–´: 1ê¸€ì â‰ˆ 1.5 í† í°)
        return int(len(text) * 1.5)
    
    try:
        return len(tokenizer.encode(text))
    except:
        return int(len(text) * 1.5)

def chunk_short_texts(documents):
    """ì§§ì€ text itemë“¤ì„ ì ì ˆí•œ í¬ê¸°ë¡œ chunking"""
    
    print(f"\n=== ğŸ“ Step 4: Short Text Chunking ì‹œì‘ ===")
    print(f"ì…ë ¥ ë¬¸ì„œ: {len(documents)}ê°œ")
    
    # text itemë“¤ë§Œ í•„í„°ë§í•˜ê³  í† í° ìˆ˜ ê³„ì‚°
    text_docs = []
    non_text_docs = []
    
    for doc in documents:
        item_label = doc['meta_data']['item_label']
        if item_label == 'text' and not doc['meta_data'].get('hierarchy'):
            # hierarchyê°€ ì—†ëŠ” ë…ë¦½ textë§Œ chunking ëŒ€ìƒ
            token_count = count_tokens(doc['page_content'])
            doc['meta_data']['token_count'] = token_count
            text_docs.append(doc)
        else:
            non_text_docs.append(doc)
    
    print(f"Chunking ëŒ€ìƒ text ë¬¸ì„œ: {len(text_docs)}ê°œ")
    print(f"Chunking ë¹„ëŒ€ìƒ ë¬¸ì„œ: {len(non_text_docs)}ê°œ (table, merged, hierarchy í¬í•¨)")
    
    # index ìˆœìœ¼ë¡œ ì •ë ¬
    text_docs.sort(key=lambda x: x['meta_data']['index'])
    
    # ì§§ì€ textë“¤ ì‹ë³„
    short_texts = []
    normal_texts = []
    
    for doc in text_docs:
        token_count = doc['meta_data']['token_count']
        if token_count < CHUNKING_CONFIG['short_text_threshold']:
            short_texts.append(doc)
        else:
            normal_texts.append(doc)
    
    print(f"ì§§ì€ text (< {CHUNKING_CONFIG['short_text_threshold']} tokens): {len(short_texts)}ê°œ")
    print(f"ì¼ë°˜ text (>= {CHUNKING_CONFIG['short_text_threshold']} tokens): {len(normal_texts)}ê°œ")
    
    # ì—°ì†ëœ ì§§ì€ textë“¤ ê·¸ë£¹í™”
    chunked_docs = create_chunks_from_short_texts(short_texts)
    
    # ì¤‘ë³µ ì œê±° í†µê³„
    original_short_count = len(short_texts)
    chunked_count = len([doc for doc in chunked_docs if doc['meta_data']['item_label'] == 'chunked_text'])
    individual_count = len([doc for doc in chunked_docs if doc['meta_data']['item_label'] == 'text'])
    
    # ê²°ê³¼ í†µí•©
    final_docs = non_text_docs + normal_texts + chunked_docs
    
    print(f"Chunking ì™„ë£Œ: {len(final_docs)}ê°œ ë¬¸ì„œ")
    print(f"  - ì§§ì€ í…ìŠ¤íŠ¸ {original_short_count}ê°œ â†’ ì²­í¬ {chunked_count}ê°œ + ê°œë³„ {individual_count}ê°œ")
    print(f"  - ì „ì²´ chunked ë¬¸ì„œ: {len(chunked_docs)}ê°œ")
    
    return final_docs

def create_chunks_from_short_texts(short_texts):
    """ì§§ì€ textë“¤ì„ ì—°ì†ì„± ê¸°ì¤€ìœ¼ë¡œ chunking"""
    
    if not short_texts:
        return []
    
    # ì—°ì†ëœ ê·¸ë£¹ë“¤ ì°¾ê¸°
    consecutive_groups = find_consecutive_groups(short_texts)
    
    chunked_docs = []
    chunk_counter = 1
    
    for group in consecutive_groups:
        if len(group) < CHUNKING_CONFIG['min_texts_for_chunking']:
            # ê·¸ë£¹ì´ ë„ˆë¬´ ì‘ìœ¼ë©´ ì›ë³¸ ê·¸ëŒ€ë¡œ ìœ ì§€
            chunked_docs.extend(group)
            continue
        
        # ê·¸ë£¹ì„ ì ì ˆí•œ í¬ê¸°ë¡œ ë¶„í• í•˜ì—¬ ì²­í¬ ìƒì„±
        chunks = create_overlapping_chunks(group, chunk_counter)
        
        # ìœ íš¨í•œ ì²­í¬ë“¤ë§Œ ì¶”ê°€ (None ì œì™¸)
        valid_chunks = [chunk for chunk in chunks if chunk is not None]
        chunked_docs.extend(valid_chunks)
        chunk_counter += len(valid_chunks)
    
    return chunked_docs

def find_consecutive_groups(texts):
    """ì—°ì†ëœ indexë¥¼ ê°€ì§„ textë“¤ì„ ê·¸ë£¹í™” (ê°œì„ ëœ ë¶„ì„)"""
    if not texts:
        return []
    
    print(f"  ğŸ” ì—°ì† ê·¸ë£¹ ë¶„ì„ ì‹œì‘: {len(texts)}ê°œ ì§§ì€ text")
    
    groups = []
    current_group = [texts[0]]
    
    for i in range(1, len(texts)):
        prev_index = current_group[-1]['meta_data']['index']
        curr_index = texts[i]['meta_data']['index']
        gap = curr_index - prev_index
        
        # ì—°ì†ëœ indexì¸ì§€ í™•ì¸ (ìµœëŒ€ gap í—ˆìš©: 2)
        if gap <= 2:
            current_group.append(texts[i])
        else:
            # ìƒˆë¡œìš´ ê·¸ë£¹ ì‹œì‘ (gapì´ í¬ë©´ ëŠì–´ì§)
            if len(current_group) >= CHUNKING_CONFIG['min_texts_for_chunking']:
                group_tokens = sum(t['meta_data']['token_count'] for t in current_group)
                print(f"    âœ… ê·¸ë£¹ {len(groups)+1}: {len(current_group)}ê°œ text, {group_tokens} tokens (index {current_group[0]['meta_data']['index']}-{current_group[-1]['meta_data']['index']})")
                groups.append(current_group)
            else:
                # ë„ˆë¬´ ì‘ì€ ê·¸ë£¹ì€ ê°œë³„ ì²˜ë¦¬
                print(f"    âšª ì†Œê·¸ë£¹: {len(current_group)}ê°œ text (ê°œë³„ ì²˜ë¦¬)")
                groups.extend([[doc] for doc in current_group])
            
            current_group = [texts[i]]
            print(f"    ğŸ”„ Gap {gap} ê°ì§€, ìƒˆ ê·¸ë£¹ ì‹œì‘ (index {curr_index})")
    
    # ë§ˆì§€ë§‰ ê·¸ë£¹ ì²˜ë¦¬
    if len(current_group) >= CHUNKING_CONFIG['min_texts_for_chunking']:
        group_tokens = sum(t['meta_data']['token_count'] for t in current_group)
        print(f"    âœ… ê·¸ë£¹ {len(groups)+1}: {len(current_group)}ê°œ text, {group_tokens} tokens (index {current_group[0]['meta_data']['index']}-{current_group[-1]['meta_data']['index']})")
        groups.append(current_group)
    else:
        print(f"    âšª ë§ˆì§€ë§‰ ì†Œê·¸ë£¹: {len(current_group)}ê°œ text (ê°œë³„ ì²˜ë¦¬)")
        groups.extend([[doc] for doc in current_group])
    
    chunking_groups = [g for g in groups if len(g) >= CHUNKING_CONFIG['min_texts_for_chunking']]
    individual_groups = [g for g in groups if len(g) < CHUNKING_CONFIG['min_texts_for_chunking']]
    
    print(f"  ğŸ“Š ê·¸ë£¹í™” ì™„ë£Œ: {len(chunking_groups)}ê°œ chunking ê·¸ë£¹, {len(individual_groups)}ê°œ ê°œë³„ ê·¸ë£¹")
    
    return groups

def create_overlapping_chunks(texts, chunk_counter):
    """ì—°ì†ëœ textë“¤ì„ ì „ì²´ì ìœ¼ë¡œ ê³ ë ¤í•˜ì—¬ ìµœì  chunking (text ë¬´ê²°ì„± ë³´ì¥)"""
    
    if len(texts) < CHUNKING_CONFIG['min_texts_for_chunking']:
        return texts
    
    # 1. ì „ì²´ í† í° ìˆ˜ ê³„ì‚° ë° ì²­í¬ ê³„íš ìˆ˜ë¦½
    chunk_plan = plan_optimal_chunking(texts)
    
    # 2. ê³„íšì— ë”°ë¼ ì²­í¬ ìƒì„±
    chunks = []
    
    for i, chunk_texts in enumerate(chunk_plan):
        chunk_doc = create_single_chunk(chunk_texts, chunk_counter + i)
        if chunk_doc:
            chunks.append(chunk_doc)
    
    return chunks

def plan_optimal_chunking(texts):
    """ì—°ì†ëœ textë“¤ì„ ì „ì²´ì ìœ¼ë¡œ ê³ ë ¤í•´ ìµœì  chunking ê³„íš ìˆ˜ë¦½"""
    
    # 1. ì „ì²´ í† í° ìˆ˜ ë° ê¸°ë³¸ ì •ë³´ ê³„ì‚°
    total_tokens = sum(text['meta_data']['token_count'] for text in texts)
    max_tokens = CHUNKING_CONFIG['max_tokens_per_chunk']
    overlap_tokens = CHUNKING_CONFIG['overlap_tokens']
    
    print(f"    ğŸ“Š ì—°ì† í…ìŠ¤íŠ¸ ê·¸ë£¹ ë¶„ì„: {len(texts)}ê°œ, ì´ {total_tokens} tokens")
    
    # 2. í•„ìš”í•œ ì²­í¬ ê°œìˆ˜ ì¶”ì •
    estimated_chunks = max(1, (total_tokens - overlap_tokens) // (max_tokens - overlap_tokens))
    
    # 3. ê·¸ë¦¬ë”” ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì²­í¬ ë¶„í•  (text ë¬´ê²°ì„± ë³´ì¥)
    chunks = []
    current_chunk = []
    current_tokens = 0
    
    for i, text in enumerate(texts):
        text_tokens = text['meta_data']['token_count']
        
        # í˜„ì¬ textë¥¼ ì¶”ê°€í•  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸
        can_add = (current_tokens + text_tokens <= max_tokens)
        
        # ë§ˆì§€ë§‰ textì¸ ê²½ìš° ë˜ëŠ” ì¶”ê°€ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°
        if not can_add and current_chunk:
            # í˜„ì¬ ì²­í¬ ì™„ì„±
            chunks.append(current_chunk)
            current_chunk = [text]
            current_tokens = text_tokens
        else:
            # í˜„ì¬ ì²­í¬ì— ì¶”ê°€
            current_chunk.append(text)
            current_tokens += text_tokens
    
    # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
    if current_chunk:
        chunks.append(current_chunk)
    
    # 4. Overlapping ì ìš© (text ë‹¨ìœ„)
    overlapped_chunks = apply_text_level_overlapping(chunks)
    
    # 5. ê²°ê³¼ ì¶œë ¥
    chunk_sizes = [sum(t['meta_data']['token_count'] for t in chunk) for chunk in overlapped_chunks]
    print(f"    ğŸ¯ ì²­í¬ ê³„íš: {len(overlapped_chunks)}ê°œ ì²­í¬, í¬ê¸°: {chunk_sizes}")
    
    return overlapped_chunks

def apply_text_level_overlapping(chunks):
    """ì²­í¬ ê°„ text ë‹¨ìœ„ overlapping ì ìš©"""
    
    if len(chunks) <= 1:
        return chunks
    
    overlapped_chunks = []
    overlap_texts_count = min(2, CHUNKING_CONFIG['overlap_tokens'] // 200)  # ëŒ€ëµ text ê°œìˆ˜ ì¶”ì •
    
    for i, current_chunk in enumerate(chunks):
        chunk_texts = current_chunk[:]  # ë³µì‚¬
        
        # ì´ì „ ì²­í¬ì—ì„œ overlapping text ê°€ì ¸ì˜¤ê¸°
        if i > 0 and overlap_texts_count > 0:
            prev_chunk = chunks[i-1]
            overlap_candidates = prev_chunk[-overlap_texts_count:]
            
            # í˜„ì¬ ì²­í¬ì˜ chunk_idë“¤ ìˆ˜ì§‘ (ì¤‘ë³µ í™•ì¸ìš©)
            current_chunk_ids = {t['meta_data']['chunk_id'] for t in chunk_texts}
            
            # ì¤‘ë³µ í™•ì¸ í›„ ì•ìª½ì— ì¶”ê°€
            for overlap_text in overlap_candidates:
                overlap_chunk_id = overlap_text['meta_data']['chunk_id']
                if overlap_chunk_id not in current_chunk_ids:
                    chunk_texts.insert(0, overlap_text)
                    current_chunk_ids.add(overlap_chunk_id)
        
        overlapped_chunks.append(chunk_texts)
    
    return overlapped_chunks

def create_single_chunk(texts, chunk_id):
    """ì—¬ëŸ¬ textë¥¼ í•˜ë‚˜ì˜ ì²­í¬ë¡œ í•©ì¹˜ê¸° (ì¤‘ë³µ ì œê±°)"""
    
    # ì¤‘ë³µ ì œê±°: chunk_id ê¸°ì¤€ìœ¼ë¡œ uniqueí•œ í…ìŠ¤íŠ¸ë“¤ë§Œ ì‚¬ìš©
    unique_texts = []
    seen_chunk_ids = set()
    
    for text in texts:
        chunk_id_str = text['meta_data']['chunk_id']
        if chunk_id_str not in seen_chunk_ids:
            unique_texts.append(text)
            seen_chunk_ids.add(chunk_id_str)
    
    # ì¤‘ë³µ ì œê±° í›„ í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ê·¸ëŒ€ë¡œ ë°˜í™˜
    if not unique_texts:
        return texts[0] if texts else None
    
    # í•©ì³ì§„ content ìƒì„± (ì¤‘ë³µ ì œê±°ëœ í…ìŠ¤íŠ¸ë“¤ë¡œ)
    content_parts = []
    for text in unique_texts:
        content_parts.append(text['page_content'])
    
    merged_content = '\n\n'.join(content_parts)
    
    # ì²« ë²ˆì§¸ unique textì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©
    base_meta = unique_texts[0]['meta_data'].copy()
    
    # ì²­í¬ ì „ìš© ë©”íƒ€ë°ì´í„° ì„¤ì • (ì¤‘ë³µ ì œê±°ëœ ë°ì´í„°ë¡œ)
    chunk_meta = {
        'chunk_id': unique_texts[0]['meta_data']['chunk_id'],  # ì²« ë²ˆì§¸ textì˜ chunk_id ì‚¬ìš©
        'item_label': 'chunked_text',
        'chunked_from': [text['meta_data']['chunk_id'] for text in unique_texts],
        'chunked_count': len(unique_texts),
        'index': unique_texts[0]['meta_data']['index'],  # ì²« ë²ˆì§¸ textì˜ index ì‚¬ìš©
        'page_number': list(set().union(*[text['meta_data']['page_number'] for text in unique_texts])),
        'document_id': base_meta['document_id'],
        'hierarchy': None,
        'token_count': sum(text['meta_data']['token_count'] for text in unique_texts)
    }
    
    # ê¸°ë³¸ ë©”íƒ€ë°ì´í„°ì™€ ë³‘í•©
    for key, value in base_meta.items():
        if key not in chunk_meta:
            chunk_meta[key] = value
    
    # ì œëª© ìƒì„± (ì²« ë²ˆì§¸ unique textì—ì„œ ì¶”ì¶œ)
    chunk_name = extract_title_from_content(unique_texts[0]['page_content'])
    
    # ë‹¨ì¼ í…ìŠ¤íŠ¸ì¸ ê²½ìš° chunkingí•˜ì§€ ì•Šê³  ì›ë³¸ ë°˜í™˜
    if len(unique_texts) == 1:
        return unique_texts[0]
    
    return {
        "page_content": merged_content,
        "name": chunk_name,
        "meta_data": chunk_meta
    }

def analyze_final_results(final_docs):
    """ìµœì¢… ê²°ê³¼ ë¶„ì„"""
    
    print(f"\n=== ğŸ“Š ìµœì¢… ì „ì²˜ë¦¬ ê²°ê³¼ ë¶„ì„ ===")
    print(f"ìµœì¢… ë¬¸ì„œ ìˆ˜: {len(final_docs)}")
    
    # item_labelë³„ í†µê³„
    label_counts = Counter(doc['meta_data']['item_label'] for doc in final_docs)
    
    for label, count in label_counts.items():
        print(f"{label}: {count}ê°œ")
    
    # merged ë° chunked ë¬¸ì„œë“¤ì˜ ìƒì„¸ ì •ë³´
    merged_docs = [doc for doc in final_docs if doc['meta_data']['item_label'] == 'merged']
    chunked_docs = [doc for doc in final_docs if doc['meta_data']['item_label'] == 'chunked_text']
    
    if merged_docs:
        print(f"\n=== ğŸ”— í•©ì³ì§„ ë¬¸ì„œ ìƒì„¸ (ìƒìœ„ 10ê°œ) ===")
        
        # í¬ê¸°ë³„ ë¶„í¬
        size_distribution = {'small': 0, 'medium': 0, 'large': 0}
        
        for i, doc in enumerate(merged_docs, 1):
            name = doc.get('name', 'ì œëª© ì—†ìŒ')
            merged_count = doc['meta_data'].get('merged_count', 1)
            content_length = len(doc['page_content'])
            
            # í¬ê¸° ë¶„ë¥˜
            if content_length < 1000:
                size_category = 'small'
            elif content_length < 3000:
                size_category = 'medium'
            else:
                size_category = 'large'
            size_distribution[size_category] += 1
            
            if i <= 10:  # ì²˜ìŒ 10ê°œë§Œ ìƒì„¸ ì¶œë ¥
                print(f"{i}. {name}")
                print(f"   - í•©ì³ì§„ í•­ëª© ìˆ˜: {merged_count}")
                print(f"   - ë‚´ìš© ê¸¸ì´: {content_length}ì ({size_category})")
                
                # ìì‹ ì •ë³´
                children = doc['meta_data'].get('merged_children', [])
                child_types = Counter(child['item_label'] for child in children)
                child_info = ', '.join(f"{k}:{v}" for k, v in child_types.items())
                print(f"   - í¬í•¨ ìš”ì†Œ: {child_info}")
        
        print(f"\n=== ğŸ“ Merged ì²­í¬ í¬ê¸° ë¶„í¬ ===")
        print(f"Small (<1Kì): {size_distribution['small']}ê°œ")
        print(f"Medium (1K-3Kì): {size_distribution['medium']}ê°œ") 
        print(f"Large (>3Kì): {size_distribution['large']}ê°œ")
    
    # chunked ë¬¸ì„œë“¤ì˜ ìƒì„¸ ì •ë³´
    if chunked_docs:
        print(f"\n=== ğŸ”— Chunked ë¬¸ì„œ ìƒì„¸ (ìƒìœ„ 10ê°œ) ===")
        
        # í¬ê¸°ë³„ ë¶„í¬
        chunked_size_distribution = {'small': 0, 'medium': 0, 'large': 0}
        
        for i, doc in enumerate(chunked_docs, 1):
            name = doc.get('name', 'ì œëª© ì—†ìŒ')
            chunked_count = doc['meta_data'].get('chunked_count', 1)
            content_length = len(doc['page_content'])
            token_count = doc['meta_data'].get('token_count', 0)
            
            # í¬ê¸° ë¶„ë¥˜
            if content_length < 1000:
                size_category = 'small'
            elif content_length < 3000:
                size_category = 'medium'
            else:
                size_category = 'large'
            chunked_size_distribution[size_category] += 1
            
            if i <= 10:  # ì²˜ìŒ 10ê°œë§Œ ìƒì„¸ ì¶œë ¥
                print(f"{i}. {name}")
                print(f"   - í•©ì³ì§„ text ìˆ˜: {chunked_count}")
                print(f"   - ë‚´ìš© ê¸¸ì´: {content_length}ì ({size_category})")
                print(f"   - í† í° ìˆ˜: {token_count}")
        
        print(f"\n=== ğŸ“ Chunked í…ìŠ¤íŠ¸ í¬ê¸° ë¶„í¬ ===")
        print(f"Small (<1Kì): {chunked_size_distribution['small']}ê°œ")
        print(f"Medium (1K-3Kì): {chunked_size_distribution['medium']}ê°œ") 
        print(f"Large (>3Kì): {chunked_size_distribution['large']}ê°œ")

def process_single_file(input_file_path, output_file_path, doc_metadata_no_toc):
    """ë‹¨ì¼ íŒŒì¼ì„ ì „ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜"""
    print(f"\n=== ğŸ“‚ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {input_file_path} ===")
    
    with open(input_file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    documents = data['documents']
    doc_metadata = data.get('metadata', {})
    
    # í˜„ì¬ íŒŒì¼ì˜ document_title ê°€ì ¸ì˜¤ê¸°
    current_document_title = doc_metadata.get('document_title', '')
    print(f"ì›ë³¸ ë¬¸ì„œ: {len(documents)}ê°œ")
    print(f"ë¬¸ì„œ ì œëª©: {current_document_title}")

    # Step 0: picture íƒœê·¸ ë°ì´í„° í•„í„°ë§
    non_picture_docs = []
    picture_count = 0
    for doc in documents:
        item_label = doc.get('meta_data', {}).get('item_label', '')
        if item_label == 'picture':
            picture_count += 1
        else:
            non_picture_docs.append(doc)
    
    print(f"Step 0 ì™„ë£Œ: picture ë°ì´í„° ì œê±° ({picture_count}ê°œ ì œê±°, {len(non_picture_docs)}ê°œ ë‚¨ìŒ)")

    # Step 1: ê¸°ë³¸ ì „ì²˜ë¦¬ (metadata í•©ì¹˜ê¸° + ì›ë³¸ ë³´ì¡´)
    preprocessed = []
    for doc in non_picture_docs:
        merged_meta = doc.get('meta_data', {}).copy()
        merged_meta.update(doc_metadata_no_toc)
        
        # ì›ë³¸ page_content ë³´ì¡´
        original_page_content = doc.get('page_content', '')
        
        # page_contentëŠ” ì „ì²˜ë¦¬ ì ìš©
        processed_page_content = clean_essential_only(original_page_content)
        
        # summaryë„ í•„ìˆ˜ ì •ë¦¬ë§Œ
        if 'summary' in merged_meta and merged_meta['summary']:
            merged_meta['summary'] = clean_essential_only(merged_meta['summary'])
        
        # ì›ë³¸ì„ meta_dataì— ì €ì¥
        merged_meta['original_page_content'] = original_page_content
        
        # meta_dataì˜ document_titleì„ í˜„ì¬ íŒŒì¼ì˜ ì˜¬ë°”ë¥¸ ê°’ìœ¼ë¡œ ì—…ë°ì´íŠ¸
        if current_document_title:
            merged_meta['document_title'] = current_document_title
        
        preprocessed.append({
            'page_content': processed_page_content,  # ì „ì²˜ë¦¬ëœ ë‚´ìš©
            'embedding': None,
            'meta_data': merged_meta
        })

    print(f"Step 1 ì™„ë£Œ: ê¸°ë³¸ ì „ì²˜ë¦¬ + ì›ë³¸ ë³´ì¡´ ({len(preprocessed)}ê°œ)")

    # Step 2: Hierarchy ë³‘í•© (ì œëª©-í‘œ ë§¤ì¹­ í¬í•¨)
    merged_docs = merge_hierarchy_chunks(preprocessed)
    print(f"Step 2 ì™„ë£Œ: Hierarchy ë³‘í•© ({len(merged_docs)}ê°œ)")

    # Step 3: ì˜ë¯¸ì—†ëŠ” ë°ì´í„° í•„í„°ë§ (ì§§ì€ text í¬í•¨)
    filtered_docs = [doc for doc in merged_docs if is_meaningful_data(doc)]
    print(f"Step 3 ì™„ë£Œ: í•„í„°ë§ ({len(filtered_docs)}ê°œ, ì œê±°: {len(merged_docs) - len(filtered_docs)}ê°œ)")

    # Step 4: Short Text Chunking
    chunked_docs = chunk_short_texts(filtered_docs)
    print(f"Step 4 ì™„ë£Œ: Short Text Chunking ({len(chunked_docs)}ê°œ)")

    # Step 5: ì œëª©ì„ page_contentì— ì¶”ê°€
    final_docs = []
    for doc in chunked_docs:
        doc_copy = doc.copy()
        current_document_title = doc_copy['meta_data'].get('document_title', '')
        
        if current_document_title:
            # document_titleì„ page_content ë§¨ ì•ì— ì¶”ê°€
            doc_copy['page_content'] = f"{current_document_title}\n\n{doc_copy['page_content']}"
        
        final_docs.append(doc_copy)
    
    print(f"Step 5 ì™„ë£Œ: ì œëª© ì¶”ê°€ ({len(final_docs)}ê°œ)")

    # ìµœì¢… ê²°ê³¼ ì €ì¥
    with open(output_file_path, 'w', encoding='utf-8') as f:
        json.dump(final_docs, f, ensure_ascii=False, indent=2)

    print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {output_file_path}")
    print(f"ğŸ“ picture ë°ì´í„° ì œê±°ë¨, text/table ë°ì´í„°ë§Œ ë³´ì¡´")
    
    # ìµœì¢… ê²°ê³¼ ë¶„ì„
    analyze_final_results(final_docs)
    
    return len(final_docs)

def main():
    import os
    import glob
    
    print(f"=== ğŸš€ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘ ===")
    
    # ì…ë ¥ ë””ë ‰í† ë¦¬ í™•ì¸
    if not os.path.exists(INPUT_DIR):
        print(f"âŒ ì…ë ¥ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {INPUT_DIR}")
        return
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„± (ë¹„ìš°ì§€ ì•ŠìŒ)
    if not os.path.exists(OUTPUT_DIR):
        print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±: {OUTPUT_DIR}")
        os.makedirs(OUTPUT_DIR)
    
    # datas í´ë”ì˜ ëª¨ë“  JSON íŒŒì¼ ì°¾ê¸° (ì´ë¯¸ 'å‡¸'ë¡œ ì‹œì‘í•˜ëŠ” íŒŒì¼ì€ ì œì™¸)
    input_files_all = glob.glob(os.path.join(INPUT_DIR, "*.json"))
    input_files = [f for f in input_files_all if not os.path.basename(f).startswith('å‡¸')]
    
    if not input_files:
        print(f"âŒ {INPUT_DIR} í´ë”ì— ì²˜ë¦¬í•  JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. (ì´ë¯¸ 'å‡¸' ì²˜ë¦¬ëœ íŒŒì¼ë§Œ ì¡´ì¬)")
        return
    
    print(f"ğŸ“‚ ì²˜ë¦¬í•  íŒŒì¼ ìˆ˜: {len(input_files)}ê°œ")
    
    total_processed = 0
    doc_metadata_no_toc = None
    
    for idx, input_file_path in enumerate(input_files):
        # íŒŒì¼ëª…ì—ì„œ í™•ì¥ì ì œê±°
        base_name = os.path.splitext(os.path.basename(input_file_path))[0]
        output_file_name = f"{base_name}_preprocessed.json"
        output_file_path = os.path.join(OUTPUT_DIR, output_file_name)
        
        # ì²« ë²ˆì§¸ ì²˜ë¦¬ íŒŒì¼ì—ì„œ metadata ì¶”ì¶œ
        if doc_metadata_no_toc is None:
            with open(input_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            doc_metadata = data.get('metadata', {})
            doc_metadata_no_toc = {k: v for k, v in doc_metadata.items() if k != 'toc'}
        
        # íŒŒì¼ ì „ì²˜ë¦¬
        processed_count = process_single_file(input_file_path, output_file_path, doc_metadata_no_toc)
        total_processed += processed_count
        
        # ì›ë³¸ ì…ë ¥ íŒŒì¼ì— 'å‡¸' ì ‘ë‘ì‚¬ ë¶€ì—¬í•˜ì—¬ ì¬ì²˜ë¦¬ ë°©ì§€
        try:
            src_dir = os.path.dirname(input_file_path)
            src_base = os.path.basename(input_file_path)
            if not src_base.startswith('å‡¸'):
                new_src_path = os.path.join(src_dir, f"å‡¸{src_base}")
                os.rename(input_file_path, new_src_path)
                print(f"ğŸ”’ ì¬ì²˜ë¦¬ ë°©ì§€: {src_base} â†’ å‡¸{src_base}")
        except Exception as e:
            print(f"âš ï¸ ì…ë ¥ íŒŒì¼ ì´ë¦„ ë³€ê²½ ì‹¤íŒ¨: {input_file_path} ({e})")
    
    print(f"\nğŸ‰ ëª¨ë“  íŒŒì¼ ì „ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"ğŸ“Š ì´ {len(input_files)}ê°œ íŒŒì¼ ì²˜ë¦¬ë¨")
    print(f"ğŸ“ˆ ì´ {total_processed}ê°œ ë¬¸ì„œ ìƒì„±ë¨")
    print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {OUTPUT_DIR}/")

if __name__ == "__main__":
    main() 