# -*- coding: utf-8 -*-
import json
import re
from collections import defaultdict, Counter
import tiktoken

# íŒŒì¼ ê²½ë¡œ
INPUT_DIR = 'datas'
OUTPUT_DIR = 'preprocessed_datas'

# í† í° ê³„ì‚°ìš© ì¸ì½”ë” (OpenAI tiktoken)
try:
	tokenizer = tiktoken.get_encoding("cl100k_base")
except:
	tokenizer = None


def clean_essential_only(text):
	"""ìµœì†Œí•œì˜ í…ìŠ¤íŠ¸ ì •ë¦¬ (ì–‘ë ê³µë°± ì œê±°ë§Œ)"""
	if not text:
		return text
	return text.strip()


def is_meaningful_data(item):
	"""ì˜ë¯¸ìˆëŠ” ë°ì´í„°ì¸ì§€ íŒë‹¨"""
	page_content = item.get('page_content', '').strip()
	meta_data = item.get('meta_data', {})
	item_label = meta_data.get('item_label', '')
	
	# 1. ë¹ˆ ë‚´ìš©
	if not page_content:
		return False
	
	# 2. ëª©ì°¨ íŒ¨í„´ (í‘œ ëª©ë¡)
	if "<í‘œ" in page_content and "Â·Â·Â·" in page_content:
		return False
	
	# 3. ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ - hierarchy ë³‘í•© í›„ ì ìš©
	if item_label == "text" and len(page_content) < 30:
		return False
	
	# 4. ìˆ«ì/ê¸°í˜¸ë§Œ ìˆëŠ” ë‚´ìš©
	if re.match(r'^[\d\s\-\|\.\Â·\(\)]+$', page_content):
		return False
		
	return True


def merge_hierarchy_chunks(documents):
	"""hierarchy ê´€ê³„ì— ìˆëŠ” itemë“¤ì„ í•˜ë‚˜ì˜ chunkë¡œ í•©ì¹˜ê¸° (í¬ê¸° ì œí•œ ì ìš©)"""
	
	# chunk_idë¡œ ë¬¸ì„œ ë§¤í•‘
	doc_map = {}
	for doc in documents:
		chunk_id = doc['meta_data']['chunk_id']
		doc_map[chunk_id] = doc
	
	# hierarchy ê´€ê³„ ë¶„ì„
	hierarchy_groups = defaultdict(list)
	orphans = []  # hierarchy ì—†ëŠ” ë…ë¦½ ë¬¸ì„œë“¤
	used_as_parent = set()  # ë¶€ëª¨ë¡œ ì‚¬ìš©ëœ ë¬¸ì„œë“¤ ì¶”ì 
	
	for doc in documents:
		hierarchy = doc['meta_data'].get('hierarchy')
		if hierarchy:
			hierarchy_groups[hierarchy].append(doc)
			# ë¶€ëª¨ ë¬¸ì„œ ì¶”ì 
			used_as_parent.add(hierarchy)
		else:
			orphans.append(doc)
	
	print(f"Hierarchy ë³‘í•©: {len(hierarchy_groups)}ê°œ ê·¸ë£¹, {len(orphans)}ê°œ í›„ë³´ ë…ë¦½ ë¬¸ì„œ")
	print(f"ë¶€ëª¨ë¡œ ì‚¬ìš©ëœ ë¬¸ì„œ: {len(used_as_parent)}ê°œ")
	
	merged_documents = []
	
	# 1. Hierarchy ê·¸ë£¹ë“¤ì„ í•©ì¹˜ê¸° (í¬ê¸° ì œí•œ ì ìš©)
	for parent_chunk_id, children in hierarchy_groups.items():
		parent_doc = doc_map.get(parent_chunk_id)
		
		if not parent_doc:
			print(f"âš ï¸ ë¶€ëª¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {parent_chunk_id}")
			# ìì‹ë“¤ì„ ê°œë³„ì ìœ¼ë¡œ ì²˜ë¦¬
			merged_documents.extend(children)
			continue
		
		# ìì‹ë“¤ì„ index ìˆœìœ¼ë¡œ ì •ë ¬
		children_sorted = sorted(children, key=lambda x: x['meta_data']['index'])
		
		# ëª¨ë“  ìì‹ë“¤ì„ í•˜ë‚˜ë¡œ ë³‘í•© (í¬ê¸° ì œí•œ ì—†ìŒ)
		merged_doc = create_single_merged_chunk(parent_doc, children_sorted)
		merged_documents.append(merged_doc)
	
	# 2. ë…ë¦½ ë¬¸ì„œë“¤ ì¶”ê°€ (ë¶€ëª¨ë¡œ ì‚¬ìš©ë˜ì§€ ì•Šì€ ê²ƒë“¤ë§Œ)
	actual_orphans = []
	for doc in orphans:
		chunk_id = doc['meta_data']['chunk_id']
		if chunk_id not in used_as_parent:
			actual_orphans.append(doc)
	
	for doc in actual_orphans:
		# name í•„ë“œ ì„¤ì •: tableì€ ë³„ë„ ì²˜ë¦¬, ë‚˜ë¨¸ì§€ëŠ” ì œëª© ì¶”ì¶œ
		name = ""
		item_label = doc['meta_data']['item_label']
		original_page_content = doc['page_content']
		
		if item_label == 'text':
			name = extract_title_from_content(doc['page_content'])
			processed_content = clean_essential_only(doc['page_content'])  # í•„ìˆ˜ ì •ë¦¬ ì ìš©
			
		elif item_label == 'table':
			# ë…ë¦½ í…Œì´ë¸”ì˜ ê²½ìš°
			name = extract_table_title_from_content(doc['page_content'])
			processed_content = optimize_table_for_search(doc['page_content'])
			# ì›ë³¸ ì •ë³´ë¥¼ merged_childrenì— ì €ì¥ (ë…ë¦½ ë¬¸ì„œë„ ë™ì¼í•œ êµ¬ì¡°)
			doc['meta_data']['merged_children'] = [{
				'item_label': 'table',
				'chunk_id': doc['meta_data']['chunk_id'],
				'content': doc['meta_data'].get('original_page_content', original_page_content),  # ì™„ì „ ì›ë³¸
				'summary': doc['meta_data'].get('summary', '')  # ì›ë˜ table summary ì‚¬ìš©
			}]
			
		else:
			name = extract_title_from_content(doc['page_content'])
			processed_content = clean_essential_only(doc['page_content'])  # í•„ìˆ˜ ì •ë¦¬ ì ìš©
		
		merged_doc = {
			"page_content": processed_content,
			"name": name,
			"meta_data": doc['meta_data']
		}
		merged_documents.append(merged_doc)
	
	print(f"ì‹¤ì œ ë…ë¦½ ë¬¸ì„œ: {len(actual_orphans)}ê°œ (ì¤‘ë³µ ì œê±°: {len(orphans) - len(actual_orphans)}ê°œ)")
	print(f"Hierarchy ë³‘í•© ì™„ë£Œ: {len(merged_documents)}ê°œ ë¬¸ì„œ")
	return merged_documents


def extract_table_title_from_content(content):
	"""í‘œ ë‚´ìš©ì—ì„œ ì œëª© ì¶”ì¶œ ì‹œë„ (HTML í…Œì´ë¸” ìº¡ì…˜ ë˜ëŠ” ê¸°ì¡´ í˜•ì‹ ì§€ì›)"""
	if not content:
		return ""
	
	# HTML í…Œì´ë¸”ì—ì„œ caption ì¶”ì¶œ
	caption_match = re.search(r'<caption[^>]*>(.*?)</caption>', content, re.IGNORECASE | re.DOTALL)
	if caption_match:
		caption = caption_match.group(1).strip()
		# HTML íƒœê·¸ ì œê±°
		caption = re.sub(r'<[^>]+>', '', caption)
		if caption:
			return caption
	
	# ê¸°ì¡´ <í‘œ X-X> íŒ¨í„´ë„ ì§€ì› (í•˜ìœ„ í˜¸í™˜ì„±)
	lines = content.strip().split('\n')
	for line in lines:
		if '<í‘œ' in line and '>' in line:
			title = re.sub(r'<í‘œ[^>]*>', '', line)
			title = title.replace('|', '').strip()
			if title:
				return title
	
	return ""


def create_single_merged_chunk(parent_doc, children, suffix=""):
	"""ë‹¨ì¼ merged ì²­í¬ ìƒì„±"""
	# í•©ì³ì§„ content ìƒì„±
	merged_content = create_merged_content(parent_doc, children)
	
	# chunk_id ìƒì„±
	base_chunk_id = parent_doc['meta_data']['chunk_id']
	new_chunk_id = f"{base_chunk_id}{suffix}" if suffix else base_chunk_id
	
	# ìƒˆë¡œìš´ merged document ìƒì„±
	merged_doc = {
		"page_content": merged_content,
		"name": extract_title_from_content(parent_doc['page_content']),  # ë¶€ëª¨ ì œëª©ì´ name
		"meta_data": create_merged_metadata(parent_doc, children, new_chunk_id)
	}
	
	return merged_doc


def optimize_table_for_search(table_content):
	"""í‘œ ë‚´ìš©ì„ ì„ë² ë”©/ê²€ìƒ‰ì— ìµœì í™”ëœ í˜•íƒœë¡œ ë³€í™˜ (HTML í…Œì´ë¸” ì§€ì›)"""
	if not table_content:
		return table_content
	
	# HTML í…Œì´ë¸”ì¸ì§€ í™•ì¸
	if '<table' in table_content.lower():
		return optimize_html_table_for_search(table_content)
	else:
		# ê¸°ì¡´ íŒŒì´í”„ êµ¬ë¶„ì í˜•ì‹ ì²˜ë¦¬ (í•˜ìœ„ í˜¸í™˜ì„±)
		return optimize_pipe_table_for_search(table_content)


def optimize_html_table_for_search(table_content):
	"""HTML í…Œì´ë¸”ì„ ë‹¨ìˆœ ë‚˜ì—´ ë°©ì‹ìœ¼ë¡œ ë³€í™˜ (ë°ì´í„° ì •í™•ì„± ìš°ì„ )"""
	optimized_parts = []
	
	# ìº¡ì…˜ ì¶”ì¶œ
	caption = ""
	caption_match = re.search(r'<caption[^>]*>(.*?)</caption>', table_content, re.IGNORECASE | re.DOTALL)
	if caption_match:
		caption = re.sub(r'<[^>]+>', '', caption_match.group(1)).strip()
		if caption:
			optimized_parts.append(f"í‘œì œëª©: {caption}")
	
	# tbodyê°€ ìˆìœ¼ë©´ tbody ë‚´ìš©, ì—†ìœ¼ë©´ ì „ì²´ í…Œì´ë¸”ì—ì„œ tr ì¶”ì¶œ
	tbody_match = re.search(r'<tbody[^>]*>(.*?)</tbody>', table_content, re.IGNORECASE | re.DOTALL)
	if tbody_match:
		content_to_parse = tbody_match.group(1)
	else:
		content_to_parse = table_content
	
	# ëª¨ë“  tr íƒœê·¸ì—ì„œ ë‚´ìš© ì¶”ì¶œ (í—¤ë”/ë°ì´í„° êµ¬ë¶„ ì—†ì´)
	tr_matches = re.findall(r'<tr[^>]*>(.*?)</tr>', content_to_parse, re.IGNORECASE | re.DOTALL)
	
	row_parts = []
	row_num = 1
	for tr_content in tr_matches:
		# ëª¨ë“  ì…€(th, td) ë‚´ìš© ì¶”ì¶œ
		cells = re.findall(r'<t[dh][^>]*>(.*?)</t[dh]>', tr_content, re.IGNORECASE | re.DOTALL)
		
		# ì…€ ë‚´ìš© ì •ë¦¬
		cleaned_cells = []
		for cell in cells:
			cell_text = re.sub(r'<[^>]+>', '', cell).strip()
			if cell_text:  # ë¹„ì–´ìˆì§€ ì•Šì€ ì…€ë§Œ ì¶”ê°€
				cleaned_cells.append(clean_table_cell(cell_text))
		
		# ìœ íš¨í•œ ë°ì´í„°ê°€ ìˆëŠ” í–‰ë§Œ ì¶”ê°€
		if cleaned_cells:
			# ì„ë² ë”© ìµœì í™”: í–‰ ë²ˆí˜¸ì™€ êµ¬ë¶„ì ì œê±°, ê³µë°±ìœ¼ë¡œë§Œ êµ¬ë¶„
			row_content = " ".join(cleaned_cells)
			row_parts.append(row_content)
			row_num += 1
	
	# ì œëª©ê³¼ í–‰ë“¤ì„ ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ì—°ê²°
	if optimized_parts and row_parts:
		return optimized_parts[0] + "\n" + "\n".join(row_parts)
	elif row_parts:
		return "\n".join(row_parts)
	else:
		return ' '.join(optimized_parts) if optimized_parts else ""


def optimize_pipe_table_for_search(table_content):
	"""ê¸°ì¡´ íŒŒì´í”„ êµ¬ë¶„ì í…Œì´ë¸” ì²˜ë¦¬ (í•˜ìœ„ í˜¸í™˜ì„±)"""
	lines = table_content.strip().split('\n')
	optimized_parts = []
	
	for line in lines:
		if '|' in line:
			# í‘œ ë¼ì¸: êµ¬ë¶„ì ì™„ì „ ì œê±°í•˜ê³  ì••ì¶•
			cells = [cell.strip() for cell in line.split('|') if cell.strip()]
			if cells:
				# ì„ë² ë”© ìµœì í™”: ê³µë°±ê³¼ íŠ¹ìˆ˜ë¬¸ì ìµœì†Œí™”
				cleaned_cells = []
				for cell in cells:
					cleaned = clean_table_cell(cell)
					cleaned_cells.append(cleaned)
				optimized_parts.append(' '.join(cleaned_cells))
		else:
			# í‘œê°€ ì•„ë‹Œ ë¼ì¸ (ì œëª©, ì¶œì²˜ ë“±)
			cleaned_line = line.strip()
			if cleaned_line and not re.match(r'^[\-\+\|\s]+$', cleaned_line):
				optimized_parts.append(cleaned_line)
	
	return ' '.join(optimized_parts)


def clean_table_cell(cell_content):
	"""í…Œì´ë¸” ì…€ ë‚´ìš© ì •ë¦¬ (ìˆ«ì ì‰¼í‘œ ì œê±°, ê³µë°± ì••ì¶•)"""
	if not cell_content:
		return ""
	
	# ìˆ«ìì—ì„œ ì‰¼í‘œ ì œê±°, ê³µë°± ì••ì¶•
	cleaned = cell_content.replace(',', '').replace('  ', ' ').strip()
	return cleaned


def create_merged_content(parent_doc, children):
	"""ë¶€ëª¨ì™€ ìì‹ë“¤ì˜ contentë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸° (ê²€ìƒ‰ ìµœì í™”)"""
	
	# ë¶€ëª¨ contentë¡œ ì‹œì‘ (í•„ìˆ˜ ì •ë¦¬ ì ìš©)
	parent_content = clean_essential_only(parent_doc['page_content'])
	content_parts = [parent_content]
	
	for child in children:
		item_label = child['meta_data']['item_label']
		child_content = child['page_content']  # ì›ë³¸ ìƒíƒœ
		
		if item_label == 'table':
			# ê²€ìƒ‰ ìµœì í™”ëœ í‘œ ë‚´ìš©ì„ page_contentì— ì¶”ê°€
			optimized_table = optimize_table_for_search(child_content)
			content_parts.append(f"\n{optimized_table}")
			
		elif item_label == 'text':
			# ì¶”ê°€ í…ìŠ¤íŠ¸ (ì¶œì²˜, ì£¼ì„ ë“±) - í•„ìˆ˜ ì •ë¦¬ ì ìš©
			if child_content.strip():
				cleaned_text = clean_essential_only(child_content)
				content_parts.append(f"\n{cleaned_text}")
	
	return '\n'.join(content_parts)


def extract_title_from_content(content):
	"""contentì—ì„œ ì œëª© ì¶”ì¶œ (ìµœì†Œí•œì˜ ì •ë¦¬ë§Œ)"""
	if not content:
		return ""
	
	lines = content.strip().split('\n')
	if lines:
		# ë§ˆí¬ë‹¤ìš´ í—¤ë” ì œê±°ë§Œ
		title = lines[0].replace('#', '').strip()
		return title
	return ""


def create_merged_metadata(parent_doc, children, chunk_id):
	"""í•©ì³ì§„ ë¬¸ì„œì˜ metadata ìƒì„±"""
	
	# ë¶€ëª¨ì˜ metadataë¥¼ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©
	merged_meta = parent_doc['meta_data'].copy()
	
	# chunk_id ì—…ë°ì´íŠ¸
	merged_meta['chunk_id'] = chunk_id
	
	# item_labelì„ "merged"ë¡œ ë³€ê²½
	merged_meta['item_label'] = 'merged'
	
	# ìì‹ë“¤ì˜ ì •ë³´ë¥¼ merged_childrenì— í†µí•© (contentì™€ summary í¬í•¨)
	merged_children = []
	
	for child in children:
		child_meta = {
			'item_label': child['meta_data']['item_label'],
			'chunk_id': child['meta_data']['chunk_id'],
			'content': child['meta_data'].get('original_page_content', child['page_content'])  # ì™„ì „ ì›ë³¸ ë‚´ìš©
		}
		
		# tableì¸ ê²½ìš° summary ì¶”ê°€ (ì›ë˜ table summary ì‚¬ìš©)
		if child['meta_data']['item_label'] == 'table':
			child_meta['summary'] = child['meta_data'].get('summary', '')
		
		merged_children.append(child_meta)
	
	merged_meta['merged_children'] = merged_children
	merged_meta['merged_count'] = len(children) + 1  # ë¶€ëª¨ í¬í•¨
	
	# í˜ì´ì§€ ë²”ìœ„ ê³„ì‚°
	all_pages = set(parent_doc['meta_data']['page_number'])
	for child in children:
		all_pages.update(child['meta_data']['page_number'])
	
	merged_meta['page_number'] = sorted(list(all_pages))
	
	return merged_meta


def count_tokens(text):
	"""í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ê³„ì‚°"""
	if not text or not tokenizer:
		# tiktokenì´ ì—†ìœ¼ë©´ ëŒ€ëµì ìœ¼ë¡œ ê³„ì‚° (í•œêµ­ì–´: 1ê¸€ì â‰ˆ 1.5 í† í°)
		return int(len(text) * 1.5)
	
	try:
		return len(tokenizer.encode(text))
	except:
		return int(len(text) * 1.5)


# ===== í˜ì´ì§€ ë‹¨ìœ„ í…ìŠ¤íŠ¸ ì²­í‚¹ =====

def chunk_texts_by_page(documents):
	"""
	ë™ì¼ í˜ì´ì§€ ë‚´ì—ì„œ ì—°ì†ëœ text itemë“¤ì„ í•˜ë‚˜ì˜ ì²­í¬ë¡œ ë¬¶ëŠ”ë‹¤.
	í‘œ(table)ëŠ” í•­ìƒ ë³„ë„ì˜ ì²­í¬ë¡œ ìœ ì§€í•œë‹¤.
	ì‚¬ì´ì‚¬ì´ì— í‘œê°€ ìˆìœ¼ë©´ í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ ëŠì–´ì„œ [text_chunk, table, text_chunk] ìˆœì„œë¥¼ ìœ ì§€í•œë‹¤.
	"""
	print(f"\n=== ğŸ“ Step 4: í˜ì´ì§€ ë‹¨ìœ„ í…ìŠ¤íŠ¸ ì²­í‚¹ ì‹œì‘ ===")
	print(f"ì…ë ¥ ë¬¸ì„œ: {len(documents)}ê°œ")
	
	# í…ìŠ¤íŠ¸ ë¬¸ì„œì— í† í° ìˆ˜ ë¶€ì—¬ (hierarchy ì—†ëŠ” ë…ë¦½ text ëŒ€ìƒ)
	prepared_docs = []
	for doc in documents:
		label = doc['meta_data']['item_label']
		if label == 'text' and not doc['meta_data'].get('hierarchy'):
			doc['meta_data']['token_count'] = count_tokens(doc['page_content'])
		prepared_docs.append(doc)
	
	# ì „ì²´ ë¬¸ì„œë¥¼ index ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìˆœì„œ ë³´ì¡´
	ordered_docs = sorted(prepared_docs, key=lambda x: x['meta_data']['index'])
	
	final_docs = []
	buffer = []  # ì—°ì† í…ìŠ¤íŠ¸ ë²„í¼
	current_page = None
	
	def flush_text_buffer():
		"""ë²„í¼ì— ëª¨ì¸ í…ìŠ¤íŠ¸ë¡œ chunked_text ìƒì„± í›„ final_docsì— ì¶”ê°€"""
		nonlocal buffer
		if not buffer:
			return
		# create_single_chunk ë™ì‘ê³¼ ìœ ì‚¬í•˜ê²Œ ë³‘í•©
		content_parts = [t['page_content'] for t in buffer]
		merged_content = '\n\n'.join(content_parts)
		base_meta = buffer[0]['meta_data'].copy()
		chunk_meta = {
			'chunk_id': buffer[0]['meta_data']['chunk_id'],
			'item_label': 'chunked_text',
			'chunked_from': [t['meta_data']['chunk_id'] for t in buffer],
			'chunked_count': len(buffer),
			'index': buffer[0]['meta_data']['index'],
			'page_number': sorted(list(set().union(*[t['meta_data']['page_number'] for t in buffer]))),
			'document_id': base_meta.get('document_id'),
			'hierarchy': None,
			'token_count': count_tokens(merged_content),
		}
		# base_metaì™€ ë³‘í•© (ê¸°ë³¸ í•„ë“œ ìœ ì§€)
		for k, v in base_meta.items():
			if k not in chunk_meta:
				chunk_meta[k] = v
		chunk_doc = {
			"page_content": merged_content,
			"name": extract_title_from_content(buffer[0]['page_content']),
			"meta_data": chunk_meta
		}
		final_docs.append(chunk_doc)
		buffer = []
	
	for doc in ordered_docs:
		label = doc['meta_data']['item_label']
		pages = doc['meta_data'].get('page_number', [])
		page = pages[0] if isinstance(pages, list) and pages else None
		
		if label == 'text' and not doc['meta_data'].get('hierarchy'):
			# ë™ì¼ í˜ì´ì§€ ë‚´ ì—°ì† í…ìŠ¤íŠ¸ëŠ” í•˜ë‚˜ë¡œ
			if buffer and current_page is not None and page != current_page:
				# í˜ì´ì§€ê°€ ë°”ë€Œë©´ ë²„í¼ flush
				flush_text_buffer()
			current_page = page
			buffer.append(doc)
		else:
			# í…ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ìš”ì†Œë¥¼ ë§Œë‚˜ë©´, ë²„í¼ë¥¼ ë¨¼ì € flush í›„ ê·¸ëŒ€ë¡œ ì¶”ê°€
			flush_text_buffer()
			current_page = None
			final_docs.append(doc)
	
	# ë‚¨ì€ ë²„í¼ flush
	flush_text_buffer()
	
	print(f"ì²­í‚¹ ì™„ë£Œ: {len(final_docs)}ê°œ ë¬¸ì„œ")
	text_chunk_count = len([d for d in final_docs if d['meta_data']['item_label'] == 'chunked_text'])
	print(f"  - ìƒì„±ëœ í…ìŠ¤íŠ¸ ì²­í¬: {text_chunk_count}ê°œ")
	return final_docs


def analyze_final_results(final_docs):
	"""ìµœì¢… ê²°ê³¼ ë¶„ì„"""
	
	print(f"\n=== ğŸ“Š ìµœì¢… ì „ì²˜ë¦¬ ê²°ê³¼ ë¶„ì„ ===")
	print(f"ìµœì¢… ë¬¸ì„œ ìˆ˜: {len(final_docs)}")
	
	# item_labelë³„ í†µê³„
	label_counts = Counter(doc['meta_data']['item_label'] for doc in final_docs)
	
	for label, count in label_counts.items():
		print(f"{label}: {count}ê°œ")
	
	# merged ë° chunked ë¬¸ì„œë“¤ì˜ ìƒì„¸ ì •ë³´
	merged_docs = [doc for doc in final_docs if doc['meta_data']['item_label'] == 'merged']
	chunked_docs = [doc for doc in final_docs if doc['meta_data']['item_label'] == 'chunked_text']
	
	if merged_docs:
		print(f"\n=== ğŸ”— í•©ì³ì§„ ë¬¸ì„œ ìƒì„¸ (ìƒìœ„ 10ê°œ) ===")
		
		# í¬ê¸°ë³„ ë¶„í¬
		size_distribution = {'small': 0, 'medium': 0, 'large': 0}
		
		for i, doc in enumerate(merged_docs, 1):
			name = doc.get('name', 'ì œëª© ì—†ìŒ')
			merged_count = doc['meta_data'].get('merged_count', 1)
			content_length = len(doc['page_content'])
			
			# í¬ê¸° ë¶„ë¥˜
			if content_length < 1000:
				size_category = 'small'
			elif content_length < 3000:
				size_category = 'medium'
			else:
				size_category = 'large'
			size_distribution[size_category] += 1
			
			if i <= 10:  # ì²˜ìŒ 10ê°œë§Œ ìƒì„¸ ì¶œë ¥
				print(f"{i}. {name}")
				print(f"   - í•©ì³ì§„ í•­ëª© ìˆ˜: {merged_count}")
				print(f"   - ë‚´ìš© ê¸¸ì´: {content_length}ì ({size_category})")
				
				# ìì‹ ì •ë³´
				children = doc['meta_data'].get('merged_children', [])
				child_types = Counter(child['item_label'] for child in children)
				child_info = ', '.join(f"{k}:{v}" for k, v in child_types.items())
				print(f"   - í¬í•¨ ìš”ì†Œ: {child_info}")
		
		print(f"\n=== ğŸ“ Merged ì²­í¬ í¬ê¸° ë¶„í¬ ===")
		print(f"Small (<1Kì): {size_distribution['small']}ê°œ")
		print(f"Medium (1K-3Kì): {size_distribution['medium']}ê°œ") 
		print(f"Large (>3Kì): {size_distribution['large']}ê°œ")
	
	# chunked ë¬¸ì„œë“¤ì˜ ìƒì„¸ ì •ë³´
	if chunked_docs:
		print(f"\n=== ğŸ”— Chunked ë¬¸ì„œ ìƒì„¸ (ìƒìœ„ 10ê°œ) ===")
		
		# í¬ê¸°ë³„ ë¶„í¬
		chunked_size_distribution = {'small': 0, 'medium': 0, 'large': 0}
		
		for i, doc in enumerate(chunked_docs, 1):
			name = doc.get('name', 'ì œëª© ì—†ìŒ')
			chunked_count = doc['meta_data'].get('chunked_count', 1)
			content_length = len(doc['page_content'])
			token_count = doc['meta_data'].get('token_count', 0)
			
			# í¬ê¸° ë¶„ë¥˜
			if content_length < 1000:
				size_category = 'small'
			elif content_length < 3000:
				size_category = 'medium'
			else:
				size_category = 'large'
			chunked_size_distribution[size_category] += 1
			
			if i <= 10:  # ì²˜ìŒ 10ê°œë§Œ ìƒì„¸ ì¶œë ¥
				print(f"{i}. {name}")
				print(f"   - í•©ì³ì§„ text ìˆ˜: {chunked_count}")
				print(f"   - ë‚´ìš© ê¸¸ì´: {content_length}ì ({size_category})")
				print(f"   - í† í° ìˆ˜: {token_count}")
		
		print(f"\n=== ğŸ“ Chunked í…ìŠ¤íŠ¸ í¬ê¸° ë¶„í¬ ===")
		print(f"Small (<1Kì): {chunked_size_distribution['small']}ê°œ")
		print(f"Medium (1K-3Kì): {chunked_size_distribution['medium']}ê°œ") 
		print(f"Large (>3Kì): {chunked_size_distribution['large']}ê°œ")


def process_single_file(input_file_path, output_file_path, doc_metadata_no_toc):
	"""ë‹¨ì¼ íŒŒì¼ì„ ì „ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ (í˜ì´ì§€ ë‹¨ìœ„ í…ìŠ¤íŠ¸ ì²­í‚¹ ë²„ì „)"""
	print(f"\n=== ğŸ“‚ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {input_file_path} ===")
	
	with open(input_file_path, 'r', encoding='utf-8') as f:
		data = json.load(f)

	documents = data['documents']
	doc_metadata = data.get('metadata', {})
	
	# í˜„ì¬ íŒŒì¼ì˜ document_title ê°€ì ¸ì˜¤ê¸°
	current_document_title = doc_metadata.get('document_title', '')
	print(f"ì›ë³¸ ë¬¸ì„œ: {len(documents)}ê°œ")
	print(f"ë¬¸ì„œ ì œëª©: {current_document_title}")

	# Step 0: picture íƒœê·¸ ë°ì´í„° í•„í„°ë§
	non_picture_docs = []
	picture_count = 0
	for doc in documents:
		item_label = doc.get('meta_data', {}).get('item_label', '')
		if item_label == 'picture':
			picture_count += 1
		else:
			non_picture_docs.append(doc)
	
	print(f"Step 0 ì™„ë£Œ: picture ë°ì´í„° ì œê±° ({picture_count}ê°œ ì œê±°, {len(non_picture_docs)}ê°œ ë‚¨ìŒ)")

	# Step 1: ê¸°ë³¸ ì „ì²˜ë¦¬ (metadata í•©ì¹˜ê¸° + ì›ë³¸ ë³´ì¡´)
	preprocessed = []
	for doc in non_picture_docs:
		merged_meta = doc.get('meta_data', {}).copy()
		merged_meta.update(doc_metadata_no_toc)
		
		# ì›ë³¸ page_content ë³´ì¡´
		original_page_content = doc.get('page_content', '')
		
		# page_contentëŠ” ì „ì²˜ë¦¬ ì ìš©
		processed_page_content = clean_essential_only(original_page_content)
		
		# summaryë„ í•„ìˆ˜ ì •ë¦¬ë§Œ
		if 'summary' in merged_meta and merged_meta['summary']:
			merged_meta['summary'] = clean_essential_only(merged_meta['summary'])
		
		# ì›ë³¸ì„ meta_dataì— ì €ì¥
		merged_meta['original_page_content'] = original_page_content
		
		# meta_dataì˜ document_titleì„ í˜„ì¬ íŒŒì¼ì˜ ì˜¬ë°”ë¥¸ ê°’ìœ¼ë¡œ ì—…ë°ì´íŠ¸
		if current_document_title:
			merged_meta['document_title'] = current_document_title
		
		preprocessed.append({
			'page_content': processed_page_content,  # ì „ì²˜ë¦¬ëœ ë‚´ìš©
			'embedding': None,
			'meta_data': merged_meta
		})

	print(f"Step 1 ì™„ë£Œ: ê¸°ë³¸ ì „ì²˜ë¦¬ + ì›ë³¸ ë³´ì¡´ ({len(preprocessed)}ê°œ)")

	# Step 2: Hierarchy ë³‘í•© (ì œëª©-í‘œ ë§¤ì¹­ í¬í•¨)
	merged_docs = merge_hierarchy_chunks(preprocessed)
	print(f"Step 2 ì™„ë£Œ: Hierarchy ë³‘í•© ({len(merged_docs)}ê°œ)")

	# Step 3: ì˜ë¯¸ì—†ëŠ” ë°ì´í„° í•„í„°ë§ (ì§§ì€ text í¬í•¨)
	filtered_docs = [doc for doc in merged_docs if is_meaningful_data(doc)]
	print(f"Step 3 ì™„ë£Œ: í•„í„°ë§ ({len(filtered_docs)}ê°œ, ì œê±°: {len(merged_docs) - len(filtered_docs)}ê°œ)")

	# Step 4: í˜ì´ì§€ ë‹¨ìœ„ í…ìŠ¤íŠ¸ ì²­í‚¹
	chunked_docs = chunk_texts_by_page(filtered_docs)
	print(f"Step 4 ì™„ë£Œ: í˜ì´ì§€ ë‹¨ìœ„ í…ìŠ¤íŠ¸ ì²­í‚¹ ({len(chunked_docs)}ê°œ)")

	# Step 5: ì œëª©ì„ page_contentì— ì¶”ê°€
	final_docs = []
	for doc in chunked_docs:
		doc_copy = doc.copy()
		current_document_title = doc_copy['meta_data'].get('document_title', '')
		
		if current_document_title:
			# document_titleì„ page_content ë§¨ ì•ì— ì¶”ê°€
			doc_copy['page_content'] = f"{current_document_title}\n\n{doc_copy['page_content']}"
		
		final_docs.append(doc_copy)
	
	print(f"Step 5 ì™„ë£Œ: ì œëª© ì¶”ê°€ ({len(final_docs)}ê°œ)")

	# ìµœì¢… ê²°ê³¼ ì €ì¥
	with open(output_file_path, 'w', encoding='utf-8') as f:
		json.dump(final_docs, f, ensure_ascii=False, indent=2)

	print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {output_file_path}")
	print(f"ğŸ“ picture ë°ì´í„° ì œê±°ë¨, text/table ë°ì´í„°ë§Œ ë³´ì¡´")
	
	# ìµœì¢… ê²°ê³¼ ë¶„ì„
	analyze_final_results(final_docs)
	
	return len(final_docs)


def main():
	import os
	import glob
	
	print(f"=== ğŸš€ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸(í˜ì´ì§€ í…ìŠ¤íŠ¸ ì²­í‚¹) ì‹œì‘ ===")
	
	# ì…ë ¥ ë””ë ‰í† ë¦¬ í™•ì¸
	if not os.path.exists(INPUT_DIR):
		print(f"âŒ ì…ë ¥ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {INPUT_DIR}")
		return
	
	# ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„± (ë¹„ìš°ì§€ ì•ŠìŒ)
	if not os.path.exists(OUTPUT_DIR):
		print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±: {OUTPUT_DIR}")
		os.makedirs(OUTPUT_DIR)
	
	# datas í´ë”ì˜ ëª¨ë“  JSON íŒŒì¼ ì°¾ê¸° (ì´ë¯¸ 'å‡¸'ë¡œ ì‹œì‘í•˜ëŠ” íŒŒì¼ì€ ì œì™¸)
	input_files_all = glob.glob(os.path.join(INPUT_DIR, "*.json"))
	input_files = [f for f in input_files_all if not os.path.basename(f).startswith('å‡¸')]
	
	if not input_files:
		print(f"âŒ {INPUT_DIR} í´ë”ì— ì²˜ë¦¬í•  JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. (ì´ë¯¸ 'å‡¸' ì²˜ë¦¬ëœ íŒŒì¼ë§Œ ì¡´ì¬)")
		return
	
	print(f"ğŸ“‚ ì²˜ë¦¬í•  íŒŒì¼ ìˆ˜: {len(input_files)}ê°œ")
	
	total_processed = 0
	doc_metadata_no_toc = None
	
	for idx, input_file_path in enumerate(input_files):
		# íŒŒì¼ëª…ì—ì„œ í™•ì¥ì ì œê±°
		base_name = os.path.splitext(os.path.basename(input_file_path))[0]
		output_file_name = f"{base_name}_preprocessed.json"
		output_file_path = os.path.join(OUTPUT_DIR, output_file_name)
		
		# ì²« ë²ˆì§¸ ì²˜ë¦¬ íŒŒì¼ì—ì„œ metadata ì¶”ì¶œ
		if doc_metadata_no_toc is None:
			with open(input_file_path, 'r', encoding='utf-8') as f:
				data = json.load(f)
			doc_metadata = data.get('metadata', {})
			doc_metadata_no_toc = {k: v for k, v in doc_metadata.items() if k != 'toc'}
		
		# íŒŒì¼ ì „ì²˜ë¦¬
		processed_count = process_single_file(input_file_path, output_file_path, doc_metadata_no_toc)
		total_processed += processed_count
		
		# ì›ë³¸ ì…ë ¥ íŒŒì¼ì— 'å‡¸' ì ‘ë‘ì‚¬ ë¶€ì—¬í•˜ì—¬ ì¬ì²˜ë¦¬ ë°©ì§€
		try:
			src_dir = os.path.dirname(input_file_path)
			src_base = os.path.basename(input_file_path)
			if not src_base.startswith('å‡¸'):
				new_src_path = os.path.join(src_dir, f"å‡¸{src_base}")
				os.rename(input_file_path, new_src_path)
				print(f"ğŸ”’ ì¬ì²˜ë¦¬ ë°©ì§€: {src_base} â†’ å‡¸{src_base}")
		except Exception as e:
			print(f"âš ï¸ ì…ë ¥ íŒŒì¼ ì´ë¦„ ë³€ê²½ ì‹¤íŒ¨: {input_file_path} ({e})")
	
	print(f"\nğŸ‰ ëª¨ë“  íŒŒì¼ ì „ì²˜ë¦¬ ì™„ë£Œ!")
	print(f"ğŸ“Š ì´ {len(input_files)}ê°œ íŒŒì¼ ì²˜ë¦¬ë¨")
	print(f"ğŸ“ˆ ì´ {total_processed}ê°œ ë¬¸ì„œ ìƒì„±ë¨")
	print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {OUTPUT_DIR}/")


if __name__ == "__main__":
	main() 