import json
import glob
import os
import time
from concurrent.futures import ThreadPoolExecutor, as_completed

from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk

# Elasticsearch ì„¤ì •
es = Elasticsearch(
    "http://localhost:9200",
    basic_auth=("elastic", "changeme")
)

TEXT_INDEX = "page_text"
TABLE_INDEX = "page_table"

def is_text_only_merged(doc):
    """mergedê°€ text-onlyì¸ì§€ íŒë³„"""
    if doc['meta_data']['item_label'] != 'merged':
        return False
    children = doc['meta_data'].get('merged_children', [])
    return all(child['item_label'] != 'table' for child in children)

def is_table_merged(doc):
    """mergedê°€ tableì„ í¬í•¨í•˜ëŠ”ì§€ íŒë³„"""
    if doc['meta_data']['item_label'] != 'merged':
        return False
    return any(child['item_label'] == 'table' for child in doc['meta_data'].get('merged_children', []))

def classify_docs(docs):
    """ë¬¸ì„œë“¤ì„ textì™€ tableë¡œ ë¶„ë¥˜"""
    text_docs = []
    table_docs = []
    for doc in docs:
        label = doc['meta_data']['item_label']
        if label in ['text', 'chunked_text'] or is_text_only_merged(doc):
            text_docs.append(doc)
        elif label == 'table' or is_table_merged(doc):
            table_docs.append(doc)
    return text_docs, table_docs

def bulk_index_documents(documents, es_index):
    """ë¬¸ì„œë“¤ì„ Elasticsearchì— bulk insert"""
    actions = []
    for doc_index, doc in documents:
        actions.append({
            "_index": es_index,
            "_id": doc_index,
            "_source": doc
        })
    
    if not actions:
        return 0, []
    
    try:
        success_count, failed = bulk(es, actions, chunk_size=len(actions))
        return success_count, failed
    except Exception as e:
        print(f"âŒ Bulk insert ì‹¤íŒ¨: {e}")
        return 0, [{"error": str(e)}]

def process_file(file_path, start_index=0):
    """ë‹¨ì¼ íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ Elasticsearchì— ì‚½ì…"""
    try:
        with open(file_path, encoding="utf-8") as f:
            docs = json.load(f)
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ {file_path}: {e}")
        return 0, 0
    
    print(f"\nğŸ“‚ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {file_path} (ì´ {len(docs)}ê°œ ë¬¸ì„œ)")
    text_docs, table_docs = classify_docs(docs)
    
    total_success = 0
    total_errors = 0
    
    for index_type, doc_list, es_index in [
        ("text", text_docs, TEXT_INDEX),
        ("table", table_docs, TABLE_INDEX)
    ]:
        if not doc_list:
            continue
            
        print(f"  â–¶ï¸ {index_type} ì¸ë±ìŠ¤ ëŒ€ìƒ ë¬¸ì„œ: {len(doc_list)}ê°œ")
        
        # ë¬¸ì„œì— ì¸ë±ìŠ¤ ë¶€ì—¬
        documents = []
        for i, doc in enumerate(doc_list):
            doc_index = start_index + total_success + i
            documents.append((doc_index, doc))
        
        print(f"    ğŸ“¦ ì „ì²´ {len(doc_list)}ê°œ ë¬¸ì„œ Elasticsearch bulk insert ì¤‘...")
        start_time = time.time()
        
        success_count, failed = bulk_index_documents(documents, es_index)
        errors_count = len(failed) if failed else 0
        
        total_success += success_count
        total_errors += errors_count
        
        process_time = time.time() - start_time
        print(f"    âœ… {index_type} ì¸ë±ìŠ¤ ì™„ë£Œ: "
              f"{success_count}ê°œ ì„±ê³µ, {errors_count}ê°œ ì‹¤íŒ¨, {process_time:.1f}ì´ˆ")
    
    print(f"ğŸ“ íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ: {file_path}")
    return total_success, total_errors

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # embedding_datas í´ë” í™•ì¸
    if not os.path.exists("embedding_datas"):
        print("âŒ embedding_datas í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return
    
    # JSON íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ì´ë¯¸ 'å‡¸'ë¡œ ì‹œì‘í•˜ëŠ” íŒŒì¼ì€ ì œì™¸)
    file_pattern = os.path.join("embedding_datas", "*.json")
    # file_list = sorted(glob.glob(file_pattern))
    file_list_all = sorted(glob.glob(file_pattern))
    file_list = [p for p in file_list_all if not os.path.basename(p).startswith('å‡¸')]
    
    if not file_list:
        print("âŒ embedding_datas í´ë”ì— ì²˜ë¦¬í•  JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸš€ embedding_datas í´ë” íŒŒì¼ ì‚½ì… ì‹œì‘ (ì´ {len(file_list)}ê°œ íŒŒì¼)")
    
    total_success = 0
    total_errors = 0
    start_index = 0
    
    for file_path in file_list:
        success, errors = process_file(file_path, start_index)
        total_success += success
        total_errors += errors
        start_index += success + errors
        
        # ì…ë ¥ íŒŒì¼ì— 'å‡¸' ì ‘ë‘ì‚¬ ë¶€ì—¬í•˜ì—¬ ì¬ì‚½ì… ë°©ì§€
        try:
            src_dir = os.path.dirname(file_path)
            src_base = os.path.basename(file_path)
            if not src_base.startswith('å‡¸'):
                new_src_path = os.path.join(src_dir, f"å‡¸{src_base}")
                os.rename(file_path, new_src_path)
                print(f"ğŸ”’ ì¬ì‚½ì… ë°©ì§€: {src_base} â†’ å‡¸{src_base}")
        except Exception as e:
            print(f"âš ï¸ íŒŒì¼ ì´ë¦„ ë³€ê²½ ì‹¤íŒ¨: {file_path} ({e})")
    
    print(f"\nğŸ‰ ëª¨ë“  íŒŒì¼ ì‚½ì… ì™„ë£Œ!")
    print(f"ğŸ“Š ìµœì¢… ê²°ê³¼: {total_success}ê°œ ì„±ê³µ, {total_errors}ê°œ ì‹¤íŒ¨")
    print(f"ğŸ“Š Elasticsearch ì¸ë±ìŠ¤: {TEXT_INDEX}, {TABLE_INDEX}")

if __name__ == "__main__":
    main() 