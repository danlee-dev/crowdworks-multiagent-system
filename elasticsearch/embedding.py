import json
import glob
import re
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from sentence_transformers import SentenceTransformer

# Hugging Face ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
hf_model = SentenceTransformer("dragonkue/bge-m3-ko")

BATCH_SIZE = 20
MAX_WORKERS = 20
DELAY_BETWEEN_BATCHES = 1.5

# mergedê°€ text-onlyì¸ì§€ tableí¬í•¨ì¸ì§€ íŒë³„
def is_text_only_merged(doc):
    if doc['meta_data']['item_label'] != 'merged':
        return False
    children = doc['meta_data'].get('merged_children', [])
    return all(child['item_label'] != 'table' for child in children)

def is_table_merged(doc):
    if doc['meta_data']['item_label'] != 'merged':
        return False
    return any(child['item_label'] == 'table' for child in doc['meta_data'].get('merged_children', []))

# Hugging Face ëª¨ë¸ë¡œ ì„ë² ë”© ìƒì„±
def embed_text(text: str) -> list:
    # model.encodeëŠ” numpy arrayë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ .tolist()ë¡œ ë³€í™˜
    return hf_model.encode(text).tolist()

def embed_text_with_retry(text_data, max_retries=3):
    text, index = text_data
    for attempt in range(max_retries):
        try:
            embedding = embed_text(text)
            return index, embedding, None
        except Exception as e:
            if attempt == max_retries - 1:
                print(f"âŒ ì„ë² ë”© ì‹¤íŒ¨ (ì¸ë±ìŠ¤ {index}): {str(e)}")
                return index, None, str(e)
            else:
                print(f"âš ï¸ ì„ë² ë”© ì¬ì‹œë„ {attempt + 1}/{max_retries} (ì¸ë±ìŠ¤ {index})")
                time.sleep(2 ** attempt)

def parallel_embed_batch(text_data_list):
    results = {}
    errors = {}
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_index = {
            executor.submit(embed_text_with_retry, text_data): text_data[1]
            for text_data in text_data_list
        }
        for future in as_completed(future_to_index):
            index, embedding, error = future.result()
            if error:
                errors[index] = error
            else:
                results[index] = embedding
    return results, errors

def chunks(lst, n):
    for i in range(0, len(lst), n):
        yield lst[i:i + n]

def get_embedding_text(item):
    return item.get("page_content", "")

def prepare_documents_batch(data_batch, start_index):
    documents = []
    text_data_list = []
    for i, item in enumerate(data_batch):
        doc_index = start_index + i
        embedding_text = get_embedding_text(item)
        doc = item.copy()
        documents.append((doc_index, doc))
        text_data_list.append((embedding_text, doc_index))
    return documents, text_data_list

def classify_docs(docs):
    text_docs = []
    table_docs = []
    for doc in docs:
        label = doc['meta_data']['item_label']
        if label in ['text', 'chunked_text'] or is_text_only_merged(doc):
            text_docs.append(doc)
        elif label == 'table' or is_table_merged(doc):
            table_docs.append(doc)
    return text_docs, table_docs

def process_file(file_path):
    with open(file_path, encoding="utf-8") as f:
        docs = json.load(f)
    print(f"\nğŸ“‚ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {file_path} (ì´ {len(docs)}ê°œ ë¬¸ì„œ)")
    
    # ëª¨ë“  ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬ (text/table êµ¬ë¶„ ì—†ì´)
    all_docs = docs.copy()
    print(f"  â–¶ï¸ ì „ì²´ ë¬¸ì„œ ì„ë² ë”© ëŒ€ìƒ: {len(all_docs)}ê°œ")
    
    total_processed = 0
    total_errors = 0
    batch_count = 0
    processed_docs = []
    
    for batch_data in chunks(all_docs, BATCH_SIZE):
        batch_count += 1
        batch_start_time = time.time()
        start_index = total_processed
        documents, text_data_list = prepare_documents_batch(batch_data, start_index)
        
        print(f"    ğŸ“¦ ë°°ì¹˜ {batch_count} ({len(batch_data)}ê°œ) ì„ë² ë”© ì¤‘...")
        embeddings, errors = parallel_embed_batch(text_data_list)
        
        # ì„ë² ë”© ê²°ê³¼ë¥¼ ë¬¸ì„œì— ì¶”ê°€
        for doc_index, doc in documents:
            if doc_index in embeddings:
                doc['embedding'] = embeddings[doc_index]
                processed_docs.append(doc)
            else:
                total_errors += 1
                print(f"    âš ï¸ ì„ë² ë”© ì‹¤íŒ¨ë¡œ ì¸í•œ ë¬¸ì„œ ì œì™¸: ì¸ë±ìŠ¤ {doc_index}")
        
        total_processed += len(batch_data)
        batch_time = time.time() - batch_start_time
        print(f"    âœ… ë°°ì¹˜ {batch_count} ì™„ë£Œ: "
              f"{len(embeddings)}ê°œ ì„±ê³µ, {len(errors)}ê°œ ì‹¤íŒ¨, {batch_time:.1f}ì´ˆ")
        
        if batch_count * BATCH_SIZE < len(all_docs):
            print(f"    â³ {DELAY_BETWEEN_BATCHES}ì´ˆ ëŒ€ê¸° ì¤‘... (rate limit ë°©ì§€)")
            time.sleep(DELAY_BETWEEN_BATCHES)
    
    print(f"  â–¶ï¸ ìµœì¢… ê²°ê³¼: {len(processed_docs)}ê°œ ì„±ê³µ, {total_errors}ê°œ ì‹¤íŒ¨")
    print(f"ğŸ“ íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ: {file_path}")
    
    return processed_docs

def main():
    import os
    
    # ì…ë ¥/ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    INPUT_DIR = "preprocessed_datas"
    OUTPUT_DIR = "embedding_datas"
    
    # ì…ë ¥ ë””ë ‰í† ë¦¬ í™•ì¸
    if not os.path.exists(INPUT_DIR):
        print(f"âŒ ì…ë ¥ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {INPUT_DIR}")
        return
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„± (ë¹„ìš°ì§€ ì•ŠìŒ)
    if not os.path.exists(OUTPUT_DIR):
        print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±: {OUTPUT_DIR}")
        os.makedirs(OUTPUT_DIR)
    
    # preprocessed_datas í´ë”ì˜ ëª¨ë“  JSON íŒŒì¼ ì°¾ê¸° (ì´ë¯¸ 'å‡¸'ë¡œ ì‹œì‘í•˜ëŠ” íŒŒì¼ì€ ì œì™¸)
    file_list_all = sorted(glob.glob(os.path.join(INPUT_DIR, "*_preprocessed.json")))
    file_list = [f for f in file_list_all if not os.path.basename(f).startswith('å‡¸')]
    if not file_list:
        print(f"âŒ {INPUT_DIR} í´ë”ì— ì²˜ë¦¬í•  *_preprocessed.json íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. (ì´ë¯¸ 'å‡¸' ì²˜ë¦¬ëœ íŒŒì¼ë§Œ ì¡´ì¬)")
        return
    
    print(f"ğŸš€ ì„ë² ë”© ìƒì„± ì‹œì‘ (ì´ {len(file_list)}ê°œ íŒŒì¼)")
    
    for file_path in file_list:
        # íŒŒì¼ëª…ì—ì„œ í™•ì¥ì ì œê±°
        base_name = os.path.splitext(os.path.basename(file_path))[0]
        # preprocessedë¥¼ embeddedë¡œ ë³€ê²½
        output_file_name = base_name.replace("_preprocessed", "_embedded") + ".json"
        output_file_path = os.path.join(OUTPUT_DIR, output_file_name)
        
        print(f"\nğŸ“‚ ì²˜ë¦¬ ì¤‘: {os.path.basename(file_path)} â†’ {output_file_name}")
        
        # íŒŒì¼ ì„ë² ë”© ì²˜ë¦¬
        processed_docs = process_file(file_path)
        
        # ê²°ê³¼ë¥¼ embedding_datas í´ë”ì— ì €ì¥
        with open(output_file_path, 'w', encoding='utf-8') as f:
            json.dump(processed_docs, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ì €ì¥ ì™„ë£Œ: {output_file_path}")
        
        # ì…ë ¥ íŒŒì¼ì— 'å‡¸' ì ‘ë‘ì‚¬ ë¶€ì—¬í•˜ì—¬ ì¬ì²˜ë¦¬ ë°©ì§€
        try:
            src_dir = os.path.dirname(file_path)
            src_base = os.path.basename(file_path)
            if not src_base.startswith('å‡¸'):
                new_src_path = os.path.join(src_dir, f"å‡¸{src_base}")
                os.rename(file_path, new_src_path)
                print(f"ğŸ”’ ì¬ì²˜ë¦¬ ë°©ì§€: {src_base} â†’ å‡¸{src_base}")
        except Exception as e:
            print(f"âš ï¸ ì…ë ¥ íŒŒì¼ ì´ë¦„ ë³€ê²½ ì‹¤íŒ¨: {file_path} ({e})")
    
    print(f"\nğŸ‰ ëª¨ë“  íŒŒì¼ ì„ë² ë”© ì™„ë£Œ!")
    print(f"ğŸ“Š ì²˜ë¦¬ëœ íŒŒì¼ ìˆ˜: {len(file_list)}ê°œ")
    print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {OUTPUT_DIR}/")

if __name__ == "__main__":
    main() 