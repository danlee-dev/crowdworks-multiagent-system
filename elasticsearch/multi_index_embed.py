import json
import glob
import re
import time
from concurrent.futures import ThreadPoolExecutor, as_completed

from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk
from sentence_transformers import SentenceTransformer

# Elasticsearch ì„¤ì •
es = Elasticsearch(
    "http://localhost:9200",
    basic_auth=(os.getenv("ELASTICSEARCH_USER"), os.getenv("ELASTICSEARCH_PASSWORD"))
)

TEXT_INDEX = "bge_text"
TABLE_INDEX = "bge_table"
BATCH_SIZE = 20
MAX_WORKERS = 20
DELAY_BETWEEN_BATCHES = 1.5

# Hugging Face ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
hf_model = SentenceTransformer("dragonkue/bge-m3-ko")

# mergedê°€ text-onlyì¸ì§€ tableí¬í•¨ì¸ì§€ íŒë³„
def is_text_only_merged(doc):
    if doc['meta_data']['item_label'] != 'merged':
        return False
    children = doc['meta_data'].get('merged_children', [])
    return all(child['item_label'] != 'table' for child in children)

def is_table_merged(doc):
    if doc['meta_data']['item_label'] != 'merged':
        return False
    return any(child['item_label'] == 'table' for child in doc['meta_data'].get('merged_children', []))

# Hugging Face ëª¨ë¸ë¡œ ì„ë² ë”© ìƒì„±
def embed_text(text: str) -> list:
    # model.encodeëŠ” numpy arrayë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ .tolist()ë¡œ ë³€í™˜
    return hf_model.encode(text).tolist()

def embed_text_with_retry(text_data, max_retries=3):
    text, index = text_data
    for attempt in range(max_retries):
        try:
            embedding = embed_text(text)
            return index, embedding, None
        except Exception as e:
            if attempt == max_retries - 1:
                print(f"âŒ ì„ë² ë”© ì‹¤íŒ¨ (ì¸ë±ìŠ¤ {index}): {str(e)}")
                return index, None, str(e)
            else:
                print(f"âš ï¸ ì„ë² ë”© ì¬ì‹œë„ {attempt + 1}/{max_retries} (ì¸ë±ìŠ¤ {index})")
                time.sleep(2 ** attempt)

def parallel_embed_batch(text_data_list):
    results = {}
    errors = {}
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_index = {
            executor.submit(embed_text_with_retry, text_data): text_data[1]
            for text_data in text_data_list
        }
        for future in as_completed(future_to_index):
            index, embedding, error = future.result()
            if error:
                errors[index] = error
            else:
                results[index] = embedding
    return results, errors

def chunks(lst, n):
    for i in range(0, len(lst), n):
        yield lst[i:i + n]

def get_embedding_text(item):
    return item.get("page_content", "")

def prepare_documents_batch(data_batch, start_index, index_type):
    documents = []
    text_data_list = []
    for i, item in enumerate(data_batch):
        doc_index = start_index + i
        embedding_text = get_embedding_text(item)
        doc = item.copy()
        documents.append((doc_index, doc))
        text_data_list.append((embedding_text, doc_index))
    return documents, text_data_list

def bulk_index_documents(documents_with_embeddings, es_index):
    actions = []
    for doc_index, doc, embedding in documents_with_embeddings:
        doc['embedding'] = embedding
        actions.append({
            "_index": es_index,
            "_id": doc_index,
            "_source": doc
        })
    if not actions:
        return 0, []
    try:
        success_count, failed = bulk(es, actions, chunk_size=len(actions))
        return success_count, failed
    except Exception as e:
        print(f"âŒ Bulk insert ì‹¤íŒ¨: {e}")
        return 0, [{"error": str(e)}]

def classify_docs(docs):
    text_docs = []
    table_docs = []
    for doc in docs:
        label = doc['meta_data']['item_label']
        if label in ['text', 'chunked_text'] or is_text_only_merged(doc):
            text_docs.append(doc)
        elif label == 'table' or is_table_merged(doc):
            table_docs.append(doc)
    return text_docs, table_docs

def process_file(file_path):
    with open(file_path, encoding="utf-8") as f:
        docs = json.load(f)
    print(f"\nğŸ“‚ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {file_path} (ì´ {len(docs)}ê°œ ë¬¸ì„œ)")
    text_docs, table_docs = classify_docs(docs)

    for index_type, doc_list, es_index in [
        ("text", text_docs, TEXT_INDEX),
        ("table", table_docs, TABLE_INDEX)
    ]:
        if not doc_list:
            continue
        print(f"  â–¶ï¸ {index_type} ì¸ë±ìŠ¤ ëŒ€ìƒ ë¬¸ì„œ: {len(doc_list)}ê°œ")
        total_processed = total_errors = batch_count = 0

        for batch_data in chunks(doc_list, BATCH_SIZE):
            batch_count += 1
            batch_start_time = time.time()
            start_index = total_processed
            documents, text_data_list = prepare_documents_batch(batch_data, start_index, index_type)

            print(f"    ğŸ“¦ ë°°ì¹˜ {batch_count} ({len(batch_data)}ê°œ) ì„ë² ë”© ì¤‘...")
            embeddings, errors = parallel_embed_batch(text_data_list)

            documents_with_embeddings = []
            for doc_index, doc in documents:
                if doc_index in embeddings:
                    documents_with_embeddings.append((doc_index, doc, embeddings[doc_index]))
                else:
                    total_errors += 1

            if documents_with_embeddings:
                print(f"    ğŸ’¾ Elasticsearch bulk insert ì¤‘... ({len(documents_with_embeddings)}ê°œ)")
                success_count, failed = bulk_index_documents(documents_with_embeddings, es_index)
                total_errors += len(failed) if failed else 0

            total_processed += len(batch_data)
            batch_time = time.time() - batch_start_time
            print(f"    âœ… ë°°ì¹˜ {batch_count} ì™„ë£Œ: "
                  f"{len(documents_with_embeddings)}ê°œ ì„±ê³µ, {len(errors)}ê°œ ì‹¤íŒ¨, {batch_time:.1f}ì´ˆ")

            if batch_count * BATCH_SIZE < len(doc_list):
                print(f"    â³ {DELAY_BETWEEN_BATCHES}ì´ˆ ëŒ€ê¸° ì¤‘... (rate limit ë°©ì§€)")
                time.sleep(DELAY_BETWEEN_BATCHES)

        print(f"  â–¶ï¸ {index_type} ì¸ë±ìŠ¤ ìµœì¢… ê²°ê³¼: "
              f"{total_processed - total_errors}ê°œ ì„±ê³µ, {total_errors}ê°œ ì‹¤íŒ¨")

    print(f"ğŸ“ íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ: {file_path}")

def main():
    file_list = sorted(glob.glob("preprocessed_data*.json"))
    if not file_list:
        print("âŒ ì²˜ë¦¬í•  preprocessed_data*.json íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"ğŸš€ ë©€í‹° ì¸ë±ìŠ¤ ë³‘ë ¬ ì„ë² ë”© ì‹œì‘ (ì´ {len(file_list)}ê°œ íŒŒì¼)")
    for file_path in file_list:
        process_file(file_path)

    print("\nğŸ‰ ëª¨ë“  íŒŒì¼ ì„ë² ë”© ë° ìƒ‰ì¸ ì™„ë£Œ!")
    print(f"ğŸ“Š Elasticsearch ì¸ë±ìŠ¤: {TEXT_INDEX}, {TABLE_INDEX}")

if __name__ == "__main__":
    main()
