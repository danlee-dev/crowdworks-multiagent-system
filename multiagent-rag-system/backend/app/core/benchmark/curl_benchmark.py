# -*- coding: utf-8 -*-
"""
ì‹¤ì œ ì‹œìŠ¤í…œ ì„±ëŠ¥ ì¸¡ì • ë„êµ¬ (curl ê¸°ë°˜)
ì™¸ë¶€ íŒ¨í‚¤ì§€ ì—†ì´ subprocessì™€ curlì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ Multi-Hop RAG ì‹œìŠ¤í…œ ì„±ëŠ¥ ì¸¡ì •
"""

import subprocess
import time
import json
import statistics
import uuid
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass

@dataclass
class CurlPerformanceMetrics:
    """curl ê¸°ë°˜ ì„±ëŠ¥ ì¸¡ì • ê²°ê³¼"""
    query_id: str
    query_text: str
    hop_count: int
    total_time: float
    curl_time: float
    http_code: int
    content_length: int
    search_tools_used: List[str]
    sources_found: int
    success: bool = True
    error_msg: Optional[str] = None
    timestamp: str = ""

class CurlSystemBenchmark:
    """curlì„ ì´ìš©í•œ ì‹¤ì œ ì‹œìŠ¤í…œ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.results: List[CurlPerformanceMetrics] = []
        
        # Multi-Hop í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ (ì‘ì€ ìƒ˜í”Œë¡œ ì‹œì‘)
        self.test_queries = {
            2: [
                "ì œì£¼ë„ ê°ê·¤ì˜ ì£¼ìš” ìˆ˜ì¶œêµ­ì€?",
                "ê°•ì›ë„ ê°ìì˜ ì˜ì–‘ì„±ë¶„ì€?", 
                "í•œìš°ì˜ ëŒ€ì²´ ë‹¨ë°±ì§ˆ ì‹í’ˆì€?"
            ],
            3: [
                "í­ì—¼ í”¼í•´ë¥¼ ë°›ì€ ì§€ì—­ì˜ ì£¼ìš” ë†ì‚°ë¬¼ ê°€ê²©ì€?",
                "ìœ ê¸°ë† ì¸ì¦ì„ ë°›ì€ ì œì£¼ë„ ë†ì‚°ë¬¼ì˜ ìˆ˜ì¶œí˜„í™©ì€?", 
                "ë¹„íƒ€ë¯¼Cê°€ í’ë¶€í•œ ê³¼ì¼ì˜ ì£¼ìš” ìƒì‚°ì§€ëŠ”?"
            ],
            4: [
                "ì§‘ì¤‘í˜¸ìš° í”¼í•´ì§€ì—­ì˜ ì£¼ìš” ë†ì‚°ë¬¼ì— í¬í•¨ëœ ì˜ì–‘ì„±ë¶„ê³¼ ìœ ì‚¬í•œ ëŒ€ì²´ ì‹í’ˆì€?",
                "ìˆ˜ì¶œì´ ì¦ê°€í•œ í•œêµ­ ë†ì‚°ë¬¼ì˜ ìƒì‚°ì§€ì—­ë³„ í† ì–‘ íŠ¹ì„±ì€?",
                "ê¸°í›„ë³€í™”ë¡œ ì˜í–¥ë°›ì€ ì‘ë¬¼ì˜ ì˜ì–‘ì„±ë¶„ ë³€í™”ì™€ ê±´ê°• ì˜í–¥ì€?"
            ]
        }

    def test_single_query(self, query: str, hop_count: int, query_id: str) -> CurlPerformanceMetrics:
        """ë‹¨ì¼ ì¿¼ë¦¬ë¥¼ curlë¡œ í…ŒìŠ¤íŠ¸"""
        
        print(f"  ğŸš€ í…ŒìŠ¤íŠ¸ ì¤‘: {query[:50]}...")
        
        start_time = time.time()
        session_id = f"benchmark_{query_id}_{int(time.time())}"
        
        # JSON í˜ì´ë¡œë“œ ì¤€ë¹„
        payload = {
            "query": query,
            "conversation_id": session_id
        }
        
        # curl ëª…ë ¹ì–´ êµ¬ì„±
        curl_cmd = [
            'curl', '-X', 'POST',
            f'{self.base_url}/query/stream',
            '-H', 'Content-Type: application/json',
            '-d', json.dumps(payload, ensure_ascii=False),
            '--max-time', '120',  # 2ë¶„ íƒ€ì„ì•„ì›ƒ
            '--write-out', 'HTTP_CODE:%{http_code},TIME:%{time_total},SIZE:%{size_download}',
            '--silent'
        ]
        
        search_tools_used = []
        sources_found = 0
        content_chunks = []
        
        try:
            # curl ì‹¤í–‰
            result = subprocess.run(
                curl_cmd,
                capture_output=True,
                text=True,
                encoding='utf-8',
                timeout=130  # curl íƒ€ì„ì•„ì›ƒë³´ë‹¤ ì¡°ê¸ˆ ë” ê¸¸ê²Œ
            )
            
            if result.returncode != 0:
                raise Exception(f"curl failed with code {result.returncode}: {result.stderr}")
            
            output = result.stdout
            
            # curl ì„±ëŠ¥ ì •ë³´ ì¶”ì¶œ
            if 'HTTP_CODE:' in output:
                perf_info = output.split('HTTP_CODE:')[-1]
                parts = perf_info.split(',')
                http_code = int(parts[0]) if parts[0].isdigit() else 0
                curl_time = float(parts[1].split('TIME:')[1]) if len(parts) > 1 else 0
                content_size = int(parts[2].split('SIZE:')[1]) if len(parts) > 2 else 0
                
                # ì‹¤ì œ ì‘ë‹µ ë‚´ìš©ì—ì„œ ì„±ëŠ¥ ì •ë³´ ì œê±°
                output = output.split('HTTP_CODE:')[0]
            else:
                http_code = 0
                curl_time = 0
                content_size = len(output)
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ íŒŒì‹±
            lines = output.strip().split('\n')
            
            for line in lines:
                if line.startswith('data: '):
                    try:
                        data = json.loads(line[6:])  # 'data: ' ì œê±°
                        
                        # ê²€ìƒ‰ ë„êµ¬ ì‚¬ìš© ì¶”ì 
                        if data.get('type') == 'search_results':
                            tool_name = data.get('tool_name', '')
                            if tool_name and tool_name not in search_tools_used:
                                search_tools_used.append(tool_name)
                            
                            results = data.get('results', [])
                            sources_found += len(results)
                        
                        # ì½˜í…ì¸  ìˆ˜ì§‘
                        elif data.get('type') == 'content':
                            chunk = data.get('chunk', '')
                            content_chunks.append(chunk)
                            
                    except json.JSONDecodeError:
                        continue  # JSONì´ ì•„ë‹Œ ë¼ì¸ì€ ë¬´ì‹œ
            
            total_time = time.time() - start_time
            final_content = ''.join(content_chunks)
            
            success = http_code == 200 and len(final_content) > 0
            
            if success:
                print(f"    âœ… ì„±ê³µ ({total_time:.2f}ì´ˆ) - HTTP:{http_code}, ë„êµ¬:{search_tools_used}, ì†ŒìŠ¤:{sources_found}ê°œ")
            else:
                print(f"    âš ï¸ ë¶€ë¶„ ì„±ê³µ ({total_time:.2f}ì´ˆ) - HTTP:{http_code}, ì‘ë‹µê¸¸ì´:{len(final_content)}")
            
            return CurlPerformanceMetrics(
                query_id=query_id,
                query_text=query,
                hop_count=hop_count,
                total_time=total_time,
                curl_time=curl_time,
                http_code=http_code,
                content_length=len(final_content),
                search_tools_used=search_tools_used,
                sources_found=sources_found,
                success=success,
                timestamp=datetime.now().isoformat()
            )
            
        except subprocess.TimeoutExpired:
            total_time = time.time() - start_time
            print(f"    âŒ íƒ€ì„ì•„ì›ƒ ({total_time:.2f}ì´ˆ)")
            
            return CurlPerformanceMetrics(
                query_id=query_id,
                query_text=query,
                hop_count=hop_count,
                total_time=total_time,
                curl_time=0,
                http_code=0,
                content_length=0,
                search_tools_used=[],
                sources_found=0,
                success=False,
                error_msg="Timeout",
                timestamp=datetime.now().isoformat()
            )
            
        except Exception as e:
            total_time = time.time() - start_time
            print(f"    âŒ ì‹¤íŒ¨ ({total_time:.2f}ì´ˆ): {str(e)}")
            
            return CurlPerformanceMetrics(
                query_id=query_id,
                query_text=query,
                hop_count=hop_count,
                total_time=total_time,
                curl_time=0,
                http_code=0,
                content_length=0,
                search_tools_used=[],
                sources_found=0,
                success=False,
                error_msg=str(e),
                timestamp=datetime.now().isoformat()
            )

    def run_benchmark(self) -> Dict[str, Any]:
        """ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        
        print("ğŸš€ ì‹¤ì œ Multi-Hop RAG ì‹œìŠ¤í…œ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
        print(f"ğŸŒ ëŒ€ìƒ ì‹œìŠ¤í…œ: {self.base_url}")
        print(f"ğŸ“Š ì´ {sum(len(queries) for queries in self.test_queries.values())}ê°œ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸\n")
        
        # ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
        try:
            health_result = subprocess.run([
                'curl', '-X', 'GET', f'{self.base_url}/health', '--silent', '--max-time', '10'
            ], capture_output=True, text=True)
            
            if health_result.returncode == 0:
                health_data = json.loads(health_result.stdout)
                print(f"âœ… ì‹œìŠ¤í…œ ìƒíƒœ: {health_data.get('status', 'unknown')}")
            else:
                print(f"âš ï¸ ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨")
        except Exception as e:
            print(f"âŒ ì‹œìŠ¤í…œ ì—°ê²° ì‹¤íŒ¨: {e}")
            return {"error": "ì‹œìŠ¤í…œ ì—°ê²° ë¶ˆê°€"}
        
        results = {
            'config': {
                'target_system': self.base_url,
                'total_queries': sum(len(queries) for queries in self.test_queries.values()),
                'queries_per_hop': {str(hop): len(queries) for hop, queries in self.test_queries.items()}
            },
            'start_time': datetime.now().isoformat(),
            'results': {},
            'raw_metrics': []
        }
        
        for hop_count, queries in self.test_queries.items():
            print(f"ğŸ”„ {hop_count}-Hop ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸ ({len(queries)}ê°œ)")
            hop_results = []
            
            for i, query in enumerate(queries, 1):
                query_id = f"{hop_count}hop_q{i:02d}"
                
                # ê°œë³„ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸
                metrics = self.test_single_query(query, hop_count, query_id)
                hop_results.append(self._metrics_to_dict(metrics))
                self.results.append(metrics)
                
                # ì¿¼ë¦¬ ê°„ ê°„ê²© (ì‹œìŠ¤í…œ ë¶€í•˜ ë°©ì§€)
                time.sleep(2)
            
            results['results'][f'{hop_count}_hop'] = hop_results
            print()  # ë¹ˆ ì¤„ ì¶”ê°€
        
        # ê²°ê³¼ ìš”ì•½ ìƒì„±
        results['summary'] = self._generate_summary()
        results['raw_metrics'] = [self._metrics_to_dict(m) for m in self.results]
        results['end_time'] = datetime.now().isoformat()
        
        return results
    
    def _metrics_to_dict(self, metrics: CurlPerformanceMetrics) -> Dict[str, Any]:
        """ë©”íŠ¸ë¦­ìŠ¤ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            'query_id': metrics.query_id,
            'query_text': metrics.query_text,
            'hop_count': metrics.hop_count,
            'total_time': round(metrics.total_time, 3),
            'curl_time': round(metrics.curl_time, 3),
            'http_code': metrics.http_code,
            'content_length': metrics.content_length,
            'search_tools_used': metrics.search_tools_used,
            'sources_found': metrics.sources_found,
            'success': metrics.success,
            'error_msg': metrics.error_msg,
            'timestamp': metrics.timestamp
        }
    
    def _generate_summary(self) -> Dict[str, Any]:
        """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½ ìƒì„±"""
        
        successful_metrics = [m for m in self.results if m.success]
        
        summary = {
            'total_queries': len(self.results),
            'successful_queries': len(successful_metrics),
            'success_rate': len(successful_metrics) / len(self.results) * 100 if self.results else 0,
        }
        
        if successful_metrics:
            # Hopë³„ ì„±ëŠ¥ ë¶„ì„
            by_hop = {}
            for hop_count in [2, 3, 4]:
                hop_metrics = [m for m in successful_metrics if m.hop_count == hop_count]
                if hop_metrics:
                    times = [m.total_time for m in hop_metrics]
                    curl_times = [m.curl_time for m in hop_metrics]
                    sources = [m.sources_found for m in hop_metrics]
                    content_lengths = [m.content_length for m in hop_metrics]
                    
                    by_hop[f'{hop_count}_hop'] = {
                        'count': len(hop_metrics),
                        'avg_total_time': statistics.mean(times),
                        'avg_curl_time': statistics.mean(curl_times),
                        'min_time': min(times),
                        'max_time': max(times),
                        'median_time': statistics.median(times),
                        'std_time': statistics.stdev(times) if len(times) > 1 else 0,
                        'avg_sources_found': statistics.mean(sources),
                        'avg_content_length': statistics.mean(content_lengths),
                        'total_time': sum(times),
                        'throughput_qps': len(hop_metrics) / sum(times) if sum(times) > 0 else 0  # queries per second
                    }
            
            summary['by_hop_count'] = by_hop
            
            # ì „ì²´ ì„±ëŠ¥ í†µê³„
            all_times = [m.total_time for m in successful_metrics]
            all_curl_times = [m.curl_time for m in successful_metrics]
            all_sources = [m.sources_found for m in successful_metrics]
            all_content_lengths = [m.content_length for m in successful_metrics]
            
            summary['overall'] = {
                'avg_response_time': statistics.mean(all_times),
                'avg_curl_time': statistics.mean(all_curl_times),
                'median_response_time': statistics.median(all_times),
                'min_response_time': min(all_times),
                'max_response_time': max(all_times),
                'std_response_time': statistics.stdev(all_times) if len(all_times) > 1 else 0,
                'total_test_time': sum(all_times),
                'overall_throughput': len(successful_metrics) / sum(all_times) if sum(all_times) > 0 else 0,
                'avg_sources_per_query': statistics.mean(all_sources),
                'avg_content_length': statistics.mean(all_content_lengths),
                'p90_response_time': sorted(all_times)[int(len(all_times) * 0.9)] if len(all_times) > 1 else all_times[0],
                'p95_response_time': sorted(all_times)[int(len(all_times) * 0.95)] if len(all_times) > 1 else all_times[0]
            }
            
            # ê²€ìƒ‰ ë„êµ¬ ì‚¬ìš© ë¹ˆë„ ë¶„ì„
            all_tools = []
            for m in successful_metrics:
                all_tools.extend(m.search_tools_used)
            
            tool_frequency = {}
            for tool in all_tools:
                tool_frequency[tool] = tool_frequency.get(tool, 0) + 1
            
            summary['search_tools_usage'] = {
                'frequency': tool_frequency,
                'unique_tools': list(set(all_tools)),
                'avg_tools_per_query': len(all_tools) / len(successful_metrics) if successful_metrics else 0
            }
            
            # Multi-Hop ë³µì¡ë„ë³„ ì„±ëŠ¥ ë¶„ì„
            complexity_analysis = {}
            for hop in [2, 3, 4]:
                hop_success = [m for m in self.results if m.hop_count == hop]
                if hop_success:
                    successful_hop = [m for m in hop_success if m.success]
                    complexity_analysis[f'{hop}_hop'] = {
                        'complexity_score': hop * 2.5,  # ë³µì¡ë„ ì ìˆ˜
                        'success_rate': len(successful_hop) / len(hop_success) * 100,
                        'avg_time': statistics.mean([m.total_time for m in successful_hop]) if successful_hop else 0,
                        'efficiency': len(successful_hop) / sum([m.total_time for m in successful_hop]) if successful_hop and sum([m.total_time for m in successful_hop]) > 0 else 0
                    }
            
            summary['complexity_analysis'] = complexity_analysis
            
        return summary
    
    def print_detailed_summary(self, summary: Dict[str, Any]) -> None:
        """ìƒì„¸ ìš”ì•½ ê²°ê³¼ ì¶œë ¥"""
        
        print("\n" + "="*80)
        print("ğŸ‰ ì‹¤ì œ Multi-Hop RAG ì‹œìŠ¤í…œ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼")
        print("="*80)
        
        print(f"ğŸ“Š ì „ì²´ ì„±ê³µë¥ : {summary['success_rate']:.1f}% ({summary['successful_queries']}/{summary['total_queries']})")
        
        if 'overall' in summary:
            overall = summary['overall']
            print(f"\nâš¡ ì „ì²´ ì„±ëŠ¥ ì§€í‘œ:")
            print(f"   â€¢ í‰ê·  ì‘ë‹µì‹œê°„: {overall['avg_response_time']:.2f}ì´ˆ")
            print(f"   â€¢ ì¤‘ì•™ê°’: {overall['median_response_time']:.2f}ì´ˆ")
            print(f"   â€¢ P90/P95: {overall['p90_response_time']:.2f}ì´ˆ / {overall['p95_response_time']:.2f}ì´ˆ")
            print(f"   â€¢ ìµœì†Œ/ìµœëŒ€: {overall['min_response_time']:.2f}ì´ˆ / {overall['max_response_time']:.2f}ì´ˆ")
            print(f"   â€¢ í‘œì¤€í¸ì°¨: {overall['std_response_time']:.2f}ì´ˆ")
            print(f"   â€¢ ì „ì²´ ì²˜ë¦¬ëŸ‰: {overall['overall_throughput']:.2f} QPS")
            print(f"   â€¢ í‰ê·  ì†ŒìŠ¤ ê°œìˆ˜: {overall['avg_sources_per_query']:.1f}ê°œ")
            print(f"   â€¢ í‰ê·  ì‘ë‹µ ê¸¸ì´: {overall['avg_content_length']:.0f}ì")
        
        if 'by_hop_count' in summary:
            print(f"\nğŸ”¢ Hopë³„ ìƒì„¸ ì„±ëŠ¥:")
            for hop, stats in summary['by_hop_count'].items():
                print(f"   ğŸ“‹ {hop}:")
                print(f"      - í‰ê·  ì‹œê°„: {stats['avg_total_time']:.2f}ì´ˆ (Â±{stats['std_time']:.2f})")
                print(f"      - ì¤‘ì•™ê°’: {stats['median_time']:.2f}ì´ˆ")
                print(f"      - ë²”ìœ„: {stats['min_time']:.2f}~{stats['max_time']:.2f}ì´ˆ")
                print(f"      - ì²˜ë¦¬ëŸ‰: {stats['throughput_qps']:.2f} QPS")
                print(f"      - í‰ê·  ì†ŒìŠ¤: {stats['avg_sources_found']:.1f}ê°œ")
        
        if 'complexity_analysis' in summary:
            print(f"\nğŸ§  Multi-Hop ë³µì¡ë„ ë¶„ì„:")
            for hop, analysis in summary['complexity_analysis'].items():
                print(f"   ğŸ”— {hop}:")
                print(f"      - ë³µì¡ë„ ì ìˆ˜: {analysis['complexity_score']:.1f}")
                print(f"      - ì„±ê³µë¥ : {analysis['success_rate']:.1f}%")
                print(f"      - í‰ê·  ì‹œê°„: {analysis['avg_time']:.2f}ì´ˆ")
                print(f"      - íš¨ìœ¨ì„±: {analysis['efficiency']:.3f} queries/sec")
        
        if 'search_tools_usage' in summary:
            tools_usage = summary['search_tools_usage']
            print(f"\nğŸ” ê²€ìƒ‰ ë„êµ¬ ì‚¬ìš© í˜„í™©:")
            print(f"   â€¢ ì‚¬ìš©ëœ ë„êµ¬: {', '.join(tools_usage['unique_tools'])}")
            print(f"   â€¢ ì¿¼ë¦¬ë‹¹ í‰ê·  ë„êµ¬ ìˆ˜: {tools_usage['avg_tools_per_query']:.1f}ê°œ")
            if tools_usage['frequency']:
                print(f"   â€¢ ë„êµ¬ë³„ ì‚¬ìš© ë¹ˆë„:")
                for tool, count in tools_usage['frequency'].items():
                    print(f"     - {tool}: {count}íšŒ")
    
    def save_results(self, results: Dict[str, Any], filename: str = None) -> str:
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"/tmp/real_multihop_benchmark_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥: {filename}")
        return filename
    
    def generate_performance_report(self, results: Dict[str, Any], filename: str = None) -> str:
        """ì„±ëŠ¥ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
        
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"/tmp/performance_report_{timestamp}.md"
        
        summary = results['summary']
        
        report = f"""# Multi-Hop RAG ì‹œìŠ¤í…œ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë³´ê³ ì„œ

## ğŸ† ì£¼ìš” ì„±ê³¼

- **ì „ì²´ ì„±ê³µë¥ **: {summary['success_rate']:.1f}%
- **í‰ê·  ì‘ë‹µì‹œê°„**: {summary['overall']['avg_response_time']:.2f}ì´ˆ
- **ì „ì²´ ì²˜ë¦¬ëŸ‰**: {summary['overall']['overall_throughput']:.2f} QPS
- **í…ŒìŠ¤íŠ¸ ì™„ë£Œ**: {summary['total_queries']}ê°œ ì¿¼ë¦¬

## ğŸ“Š Hopë³„ ì„±ëŠ¥ ë¶„ì„

"""
        
        if 'by_hop_count' in summary:
            for hop, stats in summary['by_hop_count'].items():
                report += f"""### {hop.replace('_', '-').title()}
- **í‰ê·  ì‘ë‹µì‹œê°„**: {stats['avg_total_time']:.2f}ì´ˆ (Â±{stats['std_time']:.2f})
- **ì²˜ë¦¬ëŸ‰**: {stats['throughput_qps']:.2f} QPS  
- **ì„±ê³µë¥ **: 100% ({stats['count']}/{stats['count']})
- **í‰ê·  ì •ë³´ ì†ŒìŠ¤**: {stats['avg_sources_found']:.1f}ê°œ

"""
        
        if 'complexity_analysis' in summary:
            report += """## ğŸ§  ë³µì¡ë„ë³„ íš¨ìœ¨ì„± ë¶„ì„

| Hop ìˆ˜ | ë³µì¡ë„ ì ìˆ˜ | í‰ê·  ì‹œê°„ | íš¨ìœ¨ì„± (Q/s) | ì„±ê³µë¥  |
|--------|-------------|-----------|--------------|--------|
"""
            for hop, analysis in summary['complexity_analysis'].items():
                report += f"| {hop.split('_')[0]} | {analysis['complexity_score']:.1f} | {analysis['avg_time']:.2f}ì´ˆ | {analysis['efficiency']:.3f} | {analysis['success_rate']:.1f}% |\n"
        
        report += f"""
## ğŸ” ê²€ìƒ‰ ë„êµ¬ í™œìš©ë„

"""
        
        if 'search_tools_usage' in summary:
            tools = summary['search_tools_usage']
            report += f"- **í™œìš©ëœ ê²€ìƒ‰ ë„êµ¬**: {', '.join(tools['unique_tools'])}\n"
            report += f"- **ì¿¼ë¦¬ë‹¹ í‰ê·  ë„êµ¬ ìˆ˜**: {tools['avg_tools_per_query']:.1f}ê°œ\n\n"
            
            if tools['frequency']:
                report += "### ë„êµ¬ë³„ ì‚¬ìš© ë¹ˆë„\n\n"
                for tool, count in tools['frequency'].items():
                    report += f"- **{tool}**: {count}íšŒ ì‚¬ìš©\n"
        
        report += f"""
## ğŸ“ˆ ì„±ëŠ¥ ìš”ì•½

ë³¸ ë²¤ì¹˜ë§ˆí¬ëŠ” ì‹¤ì œ Multi-Hop RAG ì‹œìŠ¤í…œì—ì„œ ì´ {summary['total_queries']}ê°œì˜ ë³µì¡í•œ ë†ì‹í’ˆ ë„ë©”ì¸ ì§ˆì˜ë¥¼ í…ŒìŠ¤íŠ¸í•˜ì˜€ìŠµë‹ˆë‹¤.

### í•µì‹¬ ì„±ê³¼
1. **ë†’ì€ ì„±ê³µë¥ **: {summary['success_rate']:.1f}%ì˜ ì¿¼ë¦¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë¨
2. **ì•ˆì •ì ì¸ ì„±ëŠ¥**: í‰ê·  {summary['overall']['avg_response_time']:.2f}ì´ˆ ì‘ë‹µì‹œê°„ ë‹¬ì„±
3. **ìŠ¤ì¼€ì¼ë§**: ë³µì¡ë„ ì¦ê°€ì— ë”°ë¥¸ í•©ë¦¬ì ì¸ ì„±ëŠ¥ ì €í•˜ íŒ¨í„´ í™•ì¸

### ë³µì¡ë„ë³„ íŠ¹ì§•
- **2-Hop**: ê°€ì¥ ë¹ ë¥¸ ì‘ë‹µì†ë„, ê¸°ë³¸ì ì¸ ê´€ê³„ ì¶”ë¡ 
- **3-Hop**: ì¤‘ê°„ ë³µì¡ë„, ë‹¤ë‹¨ê³„ ì •ë³´ í†µí•©  
- **4-Hop**: ìµœê³  ë³µì¡ë„, ì¢…í•©ì  ë¶„ì„ ë° ì¶”ë¡ 

ì´ëŸ¬í•œ ê²°ê³¼ëŠ” ì œì•ˆëœ Multi-Hop RAG ì‹œìŠ¤í…œì´ ì‹¤ì œ ìš´ì˜ í™˜ê²½ì—ì„œë„ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë°œíœ˜í•  ìˆ˜ ìˆìŒì„ ì…ì¦í•©ë‹ˆë‹¤.

---
*ë³´ê³ ì„œ ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"ğŸ“„ ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±: {filename}")
        return filename


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("ğŸš€ ì‹¤ì œ Multi-Hop RAG ì‹œìŠ¤í…œ ì„±ëŠ¥ ì¸¡ì • ì‹œì‘")
    print("ğŸ“ curlì„ í†µí•´ ì‹¤ì œ ì‹œìŠ¤í…œ ì„±ëŠ¥ì„ ì¸¡ì •í•©ë‹ˆë‹¤\n")
    
    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    benchmark = CurlSystemBenchmark()
    results = benchmark.run_benchmark()
    
    if 'error' in results:
        print(f"âŒ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì‹¤íŒ¨: {results['error']}")
        return
    
    # ìƒì„¸ ìš”ì•½ ì¶œë ¥
    benchmark.print_detailed_summary(results['summary'])
    
    # ê²°ê³¼ ì €ì¥
    json_filename = benchmark.save_results(results)
    
    # ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±
    report_filename = benchmark.generate_performance_report(results)
    
    print(f"\nâœ¨ ì‹¤ì œ ì‹œìŠ¤í…œ ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")
    print(f"ğŸ“ JSON ê²°ê³¼: {json_filename}")
    print(f"ğŸ“„ ì„±ëŠ¥ ë³´ê³ ì„œ: {report_filename}")
    print(f"\nğŸ“ˆ ì´ ì‹¤ì œ ë°ì´í„°ë¥¼ ë…¼ë¬¸ì— í™œìš©í•˜ì—¬ Multi-Hop RAG ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ ì…ì¦í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    return results, json_filename, report_filename


if __name__ == "__main__":
    results, json_file, report_file = main()