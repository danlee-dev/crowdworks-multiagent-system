# -*- coding: utf-8 -*-
"""
ì‹¤ì œ ì‹œìŠ¤í…œì— ì—°ê²°ëœ Multi-Hop RAG ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ê¸°
"""

import asyncio
import sys
import os
import json
import time
from datetime import datetime
from typing import Dict, List, Any

# Docker ì»¨í…Œì´ë„ˆ í™˜ê²½ì—ì„œì˜ ê²½ë¡œ ì„¤ì •
sys.path.append('/app')
sys.path.append('/app/app')

try:
    from core.agents.orchestrator import OrchestratorAgent, TriageAgent
    from core.models.models import StreamingAgentState
    from core.benchmark.performance_evaluator import (
        PerformanceEvaluator, 
        BenchmarkConfig, 
        PerformanceMetrics,
        BENCHMARK_QUERIES
    )
except ImportError as e:
    print(f"Import error: {e}")
    print("ì‹œìŠ¤í…œ ê²½ë¡œ í™•ì¸ ì¤‘...")
    sys.exit(1)

class SystemIntegratedBenchmark:
    """ì‹¤ì œ ì‹œìŠ¤í…œê³¼ í†µí•©ëœ ë²¤ì¹˜ë§ˆí¬"""
    
    def __init__(self):
        self.orchestrator = None
        self.triage = None
        self.evaluator = None
        
    async def initialize_system(self):
        """ì‹œìŠ¤í…œ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        print("ğŸ”§ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        try:
            self.triage = TriageAgent()
            self.orchestrator = OrchestratorAgent()
            
            config = BenchmarkConfig(
                runs_per_query=3,
                warmup_runs=1,
                timeout=120  # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ 120ì´ˆë¡œ ë‹¨ì¶•
            )
            self.evaluator = PerformanceEvaluator(config)
            print("âœ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            print(f"âŒ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    async def execute_single_query_with_timing(self, query: str, hop_count: int, query_id: str) -> PerformanceMetrics:
        """ë‹¨ì¼ ì¿¼ë¦¬ë¥¼ ì‹¤í–‰í•˜ê³  ìƒì„¸í•œ íƒ€ì´ë° ì¸¡ì •"""
        
        print(f"ğŸš€ ì¿¼ë¦¬ ì‹¤í–‰: {query[:50]}...")
        
        start_time = time.time()
        step_times = []
        search_engine_times = {
            'vector_rag': 0.0,
            'graph_rag': 0.0, 
            'rdb': 0.0,
            'web_search': 0.0
        }
        
        try:
            # State ìƒì„±
            state = StreamingAgentState(
                original_query=query,
                session_id=f"benchmark_{query_id}",
                metadata={}
            )
            
            # 1ë‹¨ê³„: Triage (ë¶„ë¥˜) - ì‹œê°„ ì¸¡ì •
            step_start = time.time()
            classified_state = await self.triage.classify_request(query, state)
            step_times.append(time.time() - step_start)
            
            # 2ë‹¨ê³„: ì‹¤ì œ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ - ì‹œê°„ ì¸¡ì •
            step_start = time.time()
            
            # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰í•˜ë©° ê²°ê³¼ ìˆ˜ì§‘
            result_chunks = []
            async for chunk in self.orchestrator.execute_report_workflow(classified_state):
                result_chunks.append(chunk)
                
            # ê²€ìƒ‰ ì—”ì§„ ì‚¬ìš© ì‹œê°„ ì¶”ì • (ì‹¤ì œë¡œëŠ” orchestratorì—ì„œ ì¸¡ì •í•´ì•¼ í•¨)
            # ì„ì‹œë¡œ ì „ì²´ ì‹œê°„ì˜ 60%ë¥¼ ê²€ìƒ‰ ì‹œê°„ìœ¼ë¡œ ê°€ì •
            workflow_time = time.time() - step_start
            step_times.append(workflow_time)
            
            # ê²€ìƒ‰ ì—”ì§„ ì‹œê°„ ë¶„ë°° (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ê°ê° ì¸¡ì •)
            search_engine_times['graph_rag'] = workflow_time * 0.3
            search_engine_times['vector_rag'] = workflow_time * 0.4
            search_engine_times['web_search'] = workflow_time * 0.2
            search_engine_times['rdb'] = workflow_time * 0.1
            
            total_time = time.time() - start_time
            final_result = ''.join(result_chunks)
            
            print(f"  âœ… ì„±ê³µ ({total_time:.2f}ì´ˆ) - ê²°ê³¼ ê¸¸ì´: {len(final_result)} ë¬¸ì")
            
            return PerformanceMetrics(
                query_id=query_id,
                query_text=query,
                hop_count=hop_count,
                total_time=total_time,
                step_times=step_times,
                search_engine_times=search_engine_times,
                success=True,
                timestamp=datetime.now().isoformat()
            )
            
        except Exception as e:
            total_time = time.time() - start_time
            print(f"  âŒ ì‹¤íŒ¨ ({total_time:.2f}ì´ˆ): {str(e)}")
            
            return PerformanceMetrics(
                query_id=query_id,
                query_text=query,
                hop_count=hop_count,
                total_time=total_time,
                step_times=step_times,
                search_engine_times=search_engine_times,
                success=False,
                error_msg=str(e),
                timestamp=datetime.now().isoformat()
            )
    
    async def run_lightweight_benchmark(self) -> Dict[str, Any]:
        """ê°€ë²¼ìš´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ (ê° hopë‹¹ 3ê°œ ì¿¼ë¦¬)"""
        
        print("ğŸš€ ê²½ëŸ‰ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘!")
        
        # í…ŒìŠ¤íŠ¸ìš© ì¿¼ë¦¬ (ê° hopë‹¹ 3ê°œì”©)
        test_queries = {
            2: BENCHMARK_QUERIES[2][:3],  # 2-hop 3ê°œ
            3: BENCHMARK_QUERIES[3][:3],  # 3-hop 3ê°œ  
            4: BENCHMARK_QUERIES[4][:3],  # 4-hop 3ê°œ
        }
        
        results = {
            'config': {
                'runs_per_query': 1,  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ 1íšŒë§Œ
                'timeout': 120,
                'test_queries_count': sum(len(queries) for queries in test_queries.values())
            },
            'start_time': datetime.now().isoformat(),
            'results': {},
            'metrics': []
        }
        
        total_queries = sum(len(queries) for queries in test_queries.values())
        processed_count = 0
        
        for hop_count, queries in test_queries.items():
            print(f"\nğŸ”„ {hop_count}-Hop ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸ ({len(queries)}ê°œ)")
            hop_results = []
            
            for i, query in enumerate(queries, 1):
                query_id = f"{hop_count}hop_q{i}"
                
                metrics = await self.execute_single_query_with_timing(
                    query, hop_count, query_id
                )
                
                hop_results.append({
                    'query_id': metrics.query_id,
                    'query_text': metrics.query_text,
                    'hop_count': metrics.hop_count,
                    'total_time': metrics.total_time,
                    'step_times': metrics.step_times,
                    'search_engine_times': metrics.search_engine_times,
                    'success': metrics.success,
                    'error_msg': metrics.error_msg,
                    'timestamp': metrics.timestamp
                })
                
                results['metrics'].append(metrics)
                processed_count += 1
                progress = (processed_count / total_queries) * 100
                print(f"    ğŸ“Š ì§„í–‰ë¥ : {progress:.1f}%")
                
            results['results'][f'{hop_count}_hop'] = hop_results
        
        # ê²°ê³¼ ìš”ì•½ ìƒì„±
        results['summary'] = self._generate_summary(results['metrics'])
        results['end_time'] = datetime.now().isoformat()
        
        return results
    
    def _generate_summary(self, metrics: List[PerformanceMetrics]) -> Dict[str, Any]:
        """ê²°ê³¼ ìš”ì•½ ìƒì„±"""
        
        successful_metrics = [m for m in metrics if m.success]
        
        if not successful_metrics:
            return {"error": "ëª¨ë“  ì¿¼ë¦¬ ì‹¤í–‰ ì‹¤íŒ¨"}
        
        summary = {
            'total_queries': len(metrics),
            'successful_queries': len(successful_metrics),
            'success_rate': len(successful_metrics) / len(metrics) * 100,
        }
        
        # Hopë³„ ì„±ëŠ¥ ë¶„ì„
        by_hop = {}
        for hop_count in [2, 3, 4]:
            hop_metrics = [m for m in successful_metrics if m.hop_count == hop_count]
            if hop_metrics:
                times = [m.total_time for m in hop_metrics]
                by_hop[f'{hop_count}_hop'] = {
                    'count': len(hop_metrics),
                    'avg_total_time': sum(times) / len(times),
                    'min_total_time': min(times),
                    'max_total_time': max(times),
                }
        
        summary['by_hop_count'] = by_hop
        
        # ì „ì²´ ì„±ëŠ¥ í†µê³„
        all_times = [m.total_time for m in successful_metrics]
        summary['overall'] = {
            'avg_response_time': sum(all_times) / len(all_times),
            'min_response_time': min(all_times),
            'max_response_time': max(all_times)
        }
        
        # ê²€ìƒ‰ ì—”ì§„ë³„ ì„±ëŠ¥
        engine_summary = {}
        for engine in ['vector_rag', 'graph_rag', 'rdb', 'web_search']:
            engine_times = [m.search_engine_times.get(engine, 0.0) for m in successful_metrics]
            non_zero_times = [t for t in engine_times if t > 0]
            if non_zero_times:
                engine_summary[engine] = {
                    'avg_time': sum(non_zero_times) / len(non_zero_times),
                    'usage_rate': len(non_zero_times) / len(successful_metrics) * 100
                }
        summary['by_search_engine'] = engine_summary
        
        return summary
    
    def save_results(self, results: Dict[str, Any], filename: str = None) -> str:
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"/tmp/benchmark_results_{timestamp}.json"
        
        # PerformanceMetrics ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        serializable_results = {
            'config': results['config'],
            'start_time': results['start_time'],
            'end_time': results['end_time'],
            'results': results['results'],
            'summary': results['summary'],
            'raw_metrics': []
        }
        
        for metric in results['metrics']:
            serializable_results['raw_metrics'].append({
                'query_id': metric.query_id,
                'query_text': metric.query_text,
                'hop_count': metric.hop_count,
                'total_time': metric.total_time,
                'step_times': metric.step_times,
                'search_engine_times': metric.search_engine_times,
                'success': metric.success,
                'error_msg': metric.error_msg,
                'timestamp': metric.timestamp
            })
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ë¨: {filename}")
        return filename
    
    def print_summary(self, summary: Dict[str, Any]) -> None:
        """ìš”ì•½ ê²°ê³¼ ì¶œë ¥"""
        
        print(f"\nğŸ‰ ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ! ğŸ“Š")
        print(f"ğŸ“ˆ ì„±ê³µë¥ : {summary['success_rate']:.1f}% ({summary['successful_queries']}/{summary['total_queries']})")
        
        if 'overall' in summary:
            overall = summary['overall']
            print(f"â±ï¸  í‰ê·  ì‘ë‹µì‹œê°„: {overall['avg_response_time']:.2f}ì´ˆ")
            print(f"ğŸ“Š ìµœì†Œ: {overall['min_response_time']:.2f}ì´ˆ, ìµœëŒ€: {overall['max_response_time']:.2f}ì´ˆ")
        
        if 'by_hop_count' in summary:
            print(f"\nğŸ”¢ Hopë³„ ì„±ëŠ¥:")
            for hop, stats in summary['by_hop_count'].items():
                print(f"  {hop}: {stats['avg_total_time']:.2f}ì´ˆ (í‰ê· )")
        
        if 'by_search_engine' in summary:
            print(f"\nğŸ” ê²€ìƒ‰ ì—”ì§„ë³„ ì„±ëŠ¥:")
            for engine, stats in summary['by_search_engine'].items():
                print(f"  {engine}: {stats['avg_time']:.2f}ì´ˆ (ì‚¬ìš©ë¥ : {stats['usage_rate']:.1f}%)")


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("=" * 60)
    print("ğŸš€ Multi-Hop RAG ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
    print("=" * 60)
    
    benchmark = SystemIntegratedBenchmark()
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    if not await benchmark.initialize_system():
        print("âŒ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨ë¡œ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return
    
    try:
        # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        results = await benchmark.run_lightweight_benchmark()
        
        # ê²°ê³¼ ì¶œë ¥
        benchmark.print_summary(results['summary'])
        
        # ê²°ê³¼ ì €ì¥
        filename = benchmark.save_results(results)
        
        print(f"\nâœ¨ ë²¤ì¹˜ë§ˆí¬ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ!")
        print(f"ğŸ“ ê²°ê³¼ íŒŒì¼: {filename}")
        
        return results
        
    except Exception as e:
        print(f"âŒ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # ë¹„ë™ê¸° ì‹¤í–‰
    results = asyncio.run(main())