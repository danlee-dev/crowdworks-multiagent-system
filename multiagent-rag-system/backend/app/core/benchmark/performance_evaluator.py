# -*- coding: utf-8 -*-
"""
Multi-Hop RAG ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€ ë„êµ¬
- ì²˜ë¦¬ ì†ë„ ì¸¡ì •
- ë³‘ë ¬ vs ìˆœì°¨ ì²˜ë¦¬ ë¹„êµ  
- Baseline ì‹œìŠ¤í…œê³¼ì˜ ì„±ëŠ¥ ë¹„êµ
"""

import time
import asyncio
import json
import statistics
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor
import pandas as pd

@dataclass
class PerformanceMetrics:
    """ì„±ëŠ¥ ì¸¡ì • ì§€í‘œ"""
    query_id: str
    query_text: str
    hop_count: int
    total_time: float
    step_times: List[float]
    search_engine_times: Dict[str, float]  # vector_rag, graph_rag, rdb, web_search
    parallel_time: Optional[float] = None
    sequential_time: Optional[float] = None
    precheck_time: Optional[float] = None
    success: bool = True
    error_msg: Optional[str] = None
    timestamp: str = ""

@dataclass
class BenchmarkConfig:
    """ë²¤ì¹˜ë§ˆí¬ ì„¤ì •"""
    runs_per_query: int = 5  # ê° ì¿¼ë¦¬ë‹¹ ì‹¤í–‰ íšŸìˆ˜
    warmup_runs: int = 2     # ì›Œë°ì—… ì‹¤í–‰ íšŸìˆ˜
    timeout: int = 300       # íƒ€ì„ì•„ì›ƒ (ì´ˆ)
    parallel_enabled: bool = True
    precheck_enabled: bool = True

class PerformanceEvaluator:
    """Multi-Hop RAG ì„±ëŠ¥ í‰ê°€ê¸°"""
    
    def __init__(self, config: BenchmarkConfig = None):
        self.config = config or BenchmarkConfig()
        self.metrics: List[PerformanceMetrics] = []
        
    async def evaluate_query_performance(self, 
                                       query_text: str,
                                       hop_count: int,
                                       orchestrator_agent,
                                       query_id: str = None) -> PerformanceMetrics:
        """ë‹¨ì¼ ì¿¼ë¦¬ì˜ ì„±ëŠ¥ì„ í‰ê°€"""
        
        if query_id is None:
            query_id = f"q_{int(time.time() * 1000)}"
            
        # ì›Œë°ì—… ì‹¤í–‰
        for _ in range(self.config.warmup_runs):
            try:
                await self._execute_query_with_timeout(query_text, orchestrator_agent)
            except Exception:
                pass  # ì›Œë°ì—… ì—ëŸ¬ëŠ” ë¬´ì‹œ
                
        # ì‹¤ì œ ì¸¡ì •
        run_results = []
        
        for run_idx in range(self.config.runs_per_query):
            try:
                result = await self._measure_single_run(
                    query_text, hop_count, orchestrator_agent, f"{query_id}_run_{run_idx}"
                )
                run_results.append(result)
            except Exception as e:
                # ì‹¤íŒ¨í•œ ì‹¤í–‰ë„ ê¸°ë¡
                failed_result = PerformanceMetrics(
                    query_id=f"{query_id}_run_{run_idx}",
                    query_text=query_text,
                    hop_count=hop_count,
                    total_time=0.0,
                    step_times=[],
                    search_engine_times={},
                    success=False,
                    error_msg=str(e),
                    timestamp=datetime.now().isoformat()
                )
                run_results.append(failed_result)
        
        # ì„±ê³µí•œ ì‹¤í–‰ë“¤ì˜ í‰ê·  ê³„ì‚°
        successful_runs = [r for r in run_results if r.success]
        
        if not successful_runs:
            return run_results[0]  # ëª¨ë‘ ì‹¤íŒ¨í–ˆë‹¤ë©´ ì²« ë²ˆì§¸ ì‹¤íŒ¨ ê²°ê³¼ ë°˜í™˜
            
        # í‰ê·  ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
        avg_metrics = self._calculate_average_metrics(successful_runs, query_id, query_text, hop_count)
        self.metrics.append(avg_metrics)
        
        return avg_metrics
    
    async def _measure_single_run(self, query_text: str, hop_count: int, 
                                orchestrator_agent, query_id: str) -> PerformanceMetrics:
        """ë‹¨ì¼ ì‹¤í–‰ì˜ ì„±ëŠ¥ ì¸¡ì •"""
        
        start_time = time.time()
        step_times = []
        search_engine_times = {
            'vector_rag': 0.0,
            'graph_rag': 0.0, 
            'rdb': 0.0,
            'web_search': 0.0
        }
        
        # í”„ë¦¬ì²´í¬ ì‹œê°„ ì¸¡ì •
        precheck_start = time.time()
        precheck_time = time.time() - precheck_start
        
        try:
            # ì‹¤ì œ ì¿¼ë¦¬ ì‹¤í–‰ (ì‹œê°„ ì¸¡ì •ì´ í¬í•¨ëœ ë²„ì „ í•„ìš”)
            result = await self._execute_query_with_profiling(
                query_text, orchestrator_agent, step_times, search_engine_times
            )
            
            total_time = time.time() - start_time
            
            return PerformanceMetrics(
                query_id=query_id,
                query_text=query_text,
                hop_count=hop_count,
                total_time=total_time,
                step_times=step_times,
                search_engine_times=search_engine_times,
                precheck_time=precheck_time,
                success=True,
                timestamp=datetime.now().isoformat()
            )
            
        except Exception as e:
            total_time = time.time() - start_time
            return PerformanceMetrics(
                query_id=query_id,
                query_text=query_text,
                hop_count=hop_count,
                total_time=total_time,
                step_times=step_times,
                search_engine_times=search_engine_times,
                success=False,
                error_msg=str(e),
                timestamp=datetime.now().isoformat()
            )
    
    async def _execute_query_with_profiling(self, query_text: str, orchestrator_agent,
                                          step_times: List[float], 
                                          search_engine_times: Dict[str, float]) -> Any:
        """í”„ë¡œíŒŒì¼ë§ì´ í¬í•¨ëœ ì¿¼ë¦¬ ì‹¤í–‰"""
        
        # ì—¬ê¸°ì„œ ì‹¤ì œ orchestrator_agent í˜¸ì¶œ
        # ê° ê²€ìƒ‰ ì—”ì§„ë³„ ì‹œê°„ì„ ì¸¡ì •í•˜ëŠ” ë¡œì§ í•„ìš”
        
        # ì„ì‹œ êµ¬í˜„ - ì‹¤ì œë¡œëŠ” orchestratorì˜ ê° ë‹¨ê³„ë³„ ì‹œê°„ì„ ì¸¡ì •í•´ì•¼ í•¨
        step_start = time.time()
        
        # ì‹¤ì œ ì¿¼ë¦¬ ì‹¤í–‰ ë¡œì§ì€ orchestrator_agentì— ë”°ë¼ êµ¬í˜„
        # ì˜ˆì‹œ: await orchestrator_agent.execute_query(query_text)
        
        step_time = time.time() - step_start
        step_times.append(step_time)
        
        return "query_result"  # ì‹¤ì œ ê²°ê³¼ ë°˜í™˜
    
    async def _execute_query_with_timeout(self, query_text: str, orchestrator_agent) -> Any:
        """íƒ€ì„ì•„ì›ƒì´ ì ìš©ëœ ì¿¼ë¦¬ ì‹¤í–‰"""
        try:
            return await asyncio.wait_for(
                orchestrator_agent.execute_query(query_text),
                timeout=self.config.timeout
            )
        except asyncio.TimeoutError:
            raise Exception(f"Query timeout after {self.config.timeout} seconds")
    
    def _calculate_average_metrics(self, successful_runs: List[PerformanceMetrics],
                                 query_id: str, query_text: str, hop_count: int) -> PerformanceMetrics:
        """ì„±ê³µí•œ ì‹¤í–‰ë“¤ì˜ í‰ê·  ì§€í‘œ ê³„ì‚°"""
        
        avg_total_time = statistics.mean([r.total_time for r in successful_runs])
        
        # ë‹¨ê³„ë³„ ì‹œê°„ í‰ê· 
        max_steps = max(len(r.step_times) for r in successful_runs) if successful_runs else 0
        avg_step_times = []
        
        for step_idx in range(max_steps):
            step_times_for_idx = [
                r.step_times[step_idx] for r in successful_runs 
                if len(r.step_times) > step_idx
            ]
            if step_times_for_idx:
                avg_step_times.append(statistics.mean(step_times_for_idx))
        
        # ê²€ìƒ‰ ì—”ì§„ë³„ ì‹œê°„ í‰ê· 
        avg_search_times = {}
        for engine in ['vector_rag', 'graph_rag', 'rdb', 'web_search']:
            engine_times = [r.search_engine_times.get(engine, 0.0) for r in successful_runs]
            avg_search_times[engine] = statistics.mean(engine_times) if engine_times else 0.0
        
        # í”„ë¦¬ì²´í¬ ì‹œê°„ í‰ê· 
        precheck_times = [r.precheck_time for r in successful_runs if r.precheck_time is not None]
        avg_precheck_time = statistics.mean(precheck_times) if precheck_times else None
        
        return PerformanceMetrics(
            query_id=query_id,
            query_text=query_text,
            hop_count=hop_count,
            total_time=avg_total_time,
            step_times=avg_step_times,
            search_engine_times=avg_search_times,
            precheck_time=avg_precheck_time,
            success=True,
            timestamp=datetime.now().isoformat()
        )
    
    async def run_benchmark_suite(self, test_queries: Dict[int, List[str]], 
                                orchestrator_agent) -> Dict[str, Any]:
        """ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ìŠ¤ìœ„íŠ¸ ì‹¤í–‰"""
        
        print(f"ğŸš€ Multi-Hop RAG ë²¤ì¹˜ë§ˆí¬ ì‹œì‘ ({datetime.now().strftime('%Y-%m-%d %H:%M:%S')})")
        print(f"ğŸ“Š ì„¤ì •: ì¿¼ë¦¬ë‹¹ {self.config.runs_per_query}íšŒ ì‹¤í–‰, ì›Œë°ì—… {self.config.warmup_runs}íšŒ")
        
        benchmark_results = {
            'config': asdict(self.config),
            'start_time': datetime.now().isoformat(),
            'results': {},
            'summary': {}
        }
        
        total_queries = sum(len(queries) for queries in test_queries.values())
        processed_count = 0
        
        for hop_count, queries in test_queries.items():
            print(f"\nğŸ”„ {hop_count}-Hop ì¿¼ë¦¬ í‰ê°€ ì‹œì‘ ({len(queries)}ê°œ)")
            hop_results = []
            
            for i, query in enumerate(queries, 1):
                print(f"  [{i}/{len(queries)}] ì²˜ë¦¬ ì¤‘: {query[:50]}...")
                
                try:
                    metrics = await self.evaluate_query_performance(
                        query_text=query,
                        hop_count=hop_count,
                        orchestrator_agent=orchestrator_agent,
                        query_id=f"{hop_count}hop_q{i}"
                    )
                    hop_results.append(asdict(metrics))
                    
                    processed_count += 1
                    progress = (processed_count / total_queries) * 100
                    print(f"    âœ… ì™„ë£Œ ({progress:.1f}%) - {metrics.total_time:.2f}ì´ˆ")
                    
                except Exception as e:
                    print(f"    âŒ ì‹¤íŒ¨: {str(e)}")
                    processed_count += 1
            
            benchmark_results['results'][f'{hop_count}_hop'] = hop_results
        
        # ê²°ê³¼ ìš”ì•½ ê³„ì‚°
        benchmark_results['summary'] = self._generate_summary()
        benchmark_results['end_time'] = datetime.now().isoformat()
        
        print(f"\nğŸ‰ ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ! ê²°ê³¼ ìš”ì•½:")
        self._print_summary(benchmark_results['summary'])
        
        return benchmark_results
    
    def _generate_summary(self) -> Dict[str, Any]:
        """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½ ìƒì„±"""
        
        if not self.metrics:
            return {"error": "No successful measurements"}
        
        successful_metrics = [m for m in self.metrics if m.success]
        
        summary = {
            'total_queries': len(self.metrics),
            'successful_queries': len(successful_metrics),
            'success_rate': len(successful_metrics) / len(self.metrics) * 100,
        }
        
        if successful_metrics:
            # Hopë³„ ì„±ëŠ¥ ë¶„ì„
            by_hop = {}
            for hop_count in [2, 3, 4]:
                hop_metrics = [m for m in successful_metrics if m.hop_count == hop_count]
                if hop_metrics:
                    by_hop[f'{hop_count}_hop'] = {
                        'count': len(hop_metrics),
                        'avg_total_time': statistics.mean([m.total_time for m in hop_metrics]),
                        'min_total_time': min([m.total_time for m in hop_metrics]),
                        'max_total_time': max([m.total_time for m in hop_metrics]),
                        'std_total_time': statistics.stdev([m.total_time for m in hop_metrics]) if len(hop_metrics) > 1 else 0
                    }
            
            summary['by_hop_count'] = by_hop
            
            # ì „ì²´ ì„±ëŠ¥ í†µê³„
            all_times = [m.total_time for m in successful_metrics]
            summary['overall'] = {
                'avg_response_time': statistics.mean(all_times),
                'median_response_time': statistics.median(all_times),
                'p95_response_time': sorted(all_times)[int(len(all_times) * 0.95)] if len(all_times) > 1 else all_times[0],
                'min_response_time': min(all_times),
                'max_response_time': max(all_times)
            }
            
            # ê²€ìƒ‰ ì—”ì§„ë³„ ì„±ëŠ¥
            engine_summary = {}
            for engine in ['vector_rag', 'graph_rag', 'rdb', 'web_search']:
                engine_times = [m.search_engine_times.get(engine, 0.0) for m in successful_metrics]
                non_zero_times = [t for t in engine_times if t > 0]
                if non_zero_times:
                    engine_summary[engine] = {
                        'avg_time': statistics.mean(non_zero_times),
                        'usage_rate': len(non_zero_times) / len(successful_metrics) * 100
                    }
            summary['by_search_engine'] = engine_summary
            
        return summary
    
    def _print_summary(self, summary: Dict[str, Any]) -> None:
        """ìš”ì•½ ê²°ê³¼ë¥¼ ì½˜ì†”ì— ì¶œë ¥"""
        
        print(f"ğŸ“ˆ ì„±ê³µë¥ : {summary['success_rate']:.1f}% ({summary['successful_queries']}/{summary['total_queries']})")
        
        if 'overall' in summary:
            overall = summary['overall']
            print(f"â±ï¸  í‰ê·  ì‘ë‹µì‹œê°„: {overall['avg_response_time']:.2f}ì´ˆ")
            print(f"ğŸ“Š ì¤‘ì•™ê°’: {overall['median_response_time']:.2f}ì´ˆ, P95: {overall['p95_response_time']:.2f}ì´ˆ")
        
        if 'by_hop_count' in summary:
            print("\nğŸ”¢ Hopë³„ ì„±ëŠ¥:")
            for hop, stats in summary['by_hop_count'].items():
                print(f"  {hop}: {stats['avg_total_time']:.2f}ì´ˆ (Â±{stats['std_total_time']:.2f})")
    
    def save_results(self, results: Dict[str, Any], filename: str = None) -> str:
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"benchmark_results_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ë¨: {filename}")
        return filename
    
    def export_to_csv(self, filename: str = None) -> str:
        """ê²°ê³¼ë¥¼ CSVë¡œ ë‚´ë³´ë‚´ê¸°"""
        
        if not self.metrics:
            raise ValueError("No metrics to export")
        
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")  
            filename = f"benchmark_metrics_{timestamp}.csv"
        
        # DataFrameìœ¼ë¡œ ë³€í™˜
        df_data = []
        for m in self.metrics:
            row = {
                'query_id': m.query_id,
                'query_text': m.query_text,
                'hop_count': m.hop_count,
                'total_time': m.total_time,
                'success': m.success,
                'precheck_time': m.precheck_time,
                'timestamp': m.timestamp
            }
            
            # ê²€ìƒ‰ ì—”ì§„ë³„ ì‹œê°„ ì¶”ê°€
            for engine, time_val in m.search_engine_times.items():
                row[f'{engine}_time'] = time_val
            
            # ë‹¨ê³„ë³„ ì‹œê°„ ì¶”ê°€ (ìµœëŒ€ 10ë‹¨ê³„ê¹Œì§€)
            for i, step_time in enumerate(m.step_times[:10]):
                row[f'step_{i+1}_time'] = step_time
                
            df_data.append(row)
        
        df = pd.DataFrame(df_data)
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        
        print(f"ğŸ“Š CSV ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {filename}")
        return filename


# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì •ì˜
BENCHMARK_QUERIES = {
    2: [
        "ì œì£¼ë„ ê°ê·¤ì˜ ì£¼ìš” ìˆ˜ì¶œêµ­ì€?",
        "ê°•ì›ë„ ê°ìì˜ ì˜ì–‘ì„±ë¶„ì€?", 
        "í•œìš°ì˜ ëŒ€ì²´ ë‹¨ë°±ì§ˆ ì‹í’ˆì€?",
        "ìœ ê¸°ë† ìŒ€ì˜ í‰ê·  ê°€ê²©ì€?",
        "ê¹€ì¹˜ì— í¬í•¨ëœ ì£¼ìš” ë¹„íƒ€ë¯¼ì€?"
    ],
    3: [
        "í­ì—¼ í”¼í•´ë¥¼ ë°›ì€ ì§€ì—­ì˜ ì£¼ìš” ë†ì‚°ë¬¼ ê°€ê²©ì€?",
        "ìœ ê¸°ë† ì¸ì¦ì„ ë°›ì€ ì œì£¼ë„ ë†ì‚°ë¬¼ì˜ ìˆ˜ì¶œí˜„í™©ì€?", 
        "ë¹„íƒ€ë¯¼Cê°€ í’ë¶€í•œ ê³¼ì¼ì˜ ì£¼ìš” ìƒì‚°ì§€ëŠ”?",
        "ê°€ë­„ í”¼í•´ì§€ì—­ì˜ ê³¡ë¬¼ ìƒì‚°ëŸ‰ ë³€í™”ëŠ”?",
        "ìˆ˜ì¶œ ì¦ê°€ìœ¨ì´ ë†’ì€ í•œêµ­ ë†ì‚°ë¬¼ì˜ íŠ¹ì§•ì€?"
    ],
    4: [
        "ì§‘ì¤‘í˜¸ìš° í”¼í•´ì§€ì—­ì˜ ì£¼ìš” ë†ì‚°ë¬¼ì— í¬í•¨ëœ ì˜ì–‘ì„±ë¶„ê³¼ ìœ ì‚¬í•œ ëŒ€ì²´ ì‹í’ˆì€?",
        "ìˆ˜ì¶œì´ ì¦ê°€í•œ í•œêµ­ ë†ì‚°ë¬¼ì˜ ìƒì‚°ì§€ì—­ë³„ í† ì–‘ íŠ¹ì„±ì€?",
        "ê¸°í›„ë³€í™”ë¡œ ì˜í–¥ë°›ì€ ì‘ë¬¼ì˜ ì˜ì–‘ì„±ë¶„ ë³€í™”ì™€ ê±´ê°• ì˜í–¥ì€?",
        "ìœ ê¸°ë† ì¸ì¦ ë†ì‚°ë¬¼ì˜ ì§€ì—­ë³„ ìƒì‚°í˜„í™©ê³¼ ì†Œë¹„ì ì„ í˜¸ë„ëŠ”?",
        "í•œêµ­ ì „í†µ ë°œíš¨ì‹í’ˆì˜ í•´ì™¸ ìˆ˜ì¶œ í˜„í™©ê³¼ í˜„ì§€ ì ì‘ ì „ëµì€?"
    ]
}


# ë¹„ë™ê¸° ì‹¤í–‰ì„ ìœ„í•œ ë©”ì¸ í•¨ìˆ˜
async def main():
    """ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì˜ˆì‹œ"""
    
    config = BenchmarkConfig(
        runs_per_query=3,
        warmup_runs=1,
        timeout=180
    )
    
    evaluator = PerformanceEvaluator(config)
    
    # ì‹¤ì œ ì‚¬ìš© ì‹œì—ëŠ” orchestrator_agent ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì „ë‹¬
    # orchestrator_agent = YourOrchestratorAgent()
    
    # results = await evaluator.run_benchmark_suite(BENCHMARK_QUERIES, orchestrator_agent)
    # evaluator.save_results(results)
    # evaluator.export_to_csv()
    
    print("ë²¤ì¹˜ë§ˆí¬ ë„êµ¬ ì¤€ë¹„ ì™„ë£Œ!")


if __name__ == "__main__":
    asyncio.run(main())