# -*- coding: utf-8 -*-
"""
Vector RAG vs GraphRAG vs Combined ë¹„êµ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
ë…¼ë¬¸ìš© í•µì‹¬ ë°ì´í„° ìƒì„±
"""

import subprocess
import time
import json
import statistics
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass

@dataclass
class ComparativeMetrics:
    """ë¹„êµ ì‹¤í—˜ìš© ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
    query_id: str
    query_text: str
    hop_count: int
    test_mode: str  # "vector_only", "graph_only", "combined"
    
    # ì„±ëŠ¥ ì§€í‘œ
    total_time: float
    response_quality_score: float  # ì‘ë‹µ í’ˆì§ˆ ì ìˆ˜ (1-10)
    sources_found: int
    content_length: int
    search_tools_used: List[str]
    
    # ìƒì„¸ ë¶„ì„
    accuracy_indicators: Dict[str, Any]  # ì •í™•ë„ ê´€ë ¨ ì§€í‘œ
    success: bool = True
    error_msg: Optional[str] = None
    timestamp: str = ""

class ComparativeBenchmark:
    """Vector RAG vs GraphRAG vs Combined ì‹œìŠ¤í…œ ë¹„êµ í…ŒìŠ¤íŠ¸"""
    
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.results: List[ComparativeMetrics] = []
        
        # GraphRAGì— ìœ ë¦¬í•œ ê´€ê³„í˜• ì¿¼ë¦¬ë“¤ë¡œ êµ¬ì„±
        self.test_queries = {
            2: [
                "ì œì£¼ë„ ê°ê·¤ì˜ ì£¼ìš” ìˆ˜ì¶œêµ­ì€?",          # ì§€ì—­ â†’ í’ˆëª© â†’ ìˆ˜ì¶œêµ­  
                "ê°•ì›ë„ì—ì„œ ìƒì‚°ë˜ëŠ” ì£¼ìš” ë†ì‚°ë¬¼ì€?",      # ì§€ì—­ â†’ ë†ì‚°ë¬¼
            ],
            3: [
                "ì œì£¼ë„ ê°ê·¤ì— í¬í•¨ëœ ì£¼ìš” ì˜ì–‘ì„±ë¶„ì€?",   # ì§€ì—­ â†’ í’ˆëª© â†’ ì˜ì–‘ì„±ë¶„
                "ê°ê·¤ê³¼ ìœ ì‚¬í•œ ì˜ì–‘ì„±ë¶„ì„ ê°€ì§„ ê³¼ì¼ì€?",   # í’ˆëª© â†’ ì˜ì–‘ì„±ë¶„ â†’ ìœ ì‚¬í’ˆëª©
            ]
        }

    def test_with_mode(self, query: str, hop_count: int, query_id: str, mode: str) -> ComparativeMetrics:
        """íŠ¹ì • ëª¨ë“œë¡œ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸"""
        
        print(f"    ğŸ§ª {mode.upper()} ëª¨ë“œ: {query[:40]}...")
        
        start_time = time.time()
        session_id = f"comparative_{mode}_{query_id}_{int(time.time())}"
        
        # ëª¨ë“œë³„ í˜ì´ë¡œë“œ êµ¬ì„±
        payload = {
            "query": query,
            "conversation_id": session_id
        }
        
        # ëª¨ë“œ ê°•ì œë¥¼ ìœ„í•œ ì¿¼ë¦¬ ìˆ˜ì •
        if mode == "vector_only":
            # Vector RAGë§Œ ì‚¬ìš©í•˜ë„ë¡ ìœ ë„
            payload["query"] = f"ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ì°¾ì•„ì¤˜: {query}"
        elif mode == "graph_only":
            # GraphRAGë§Œ ì‚¬ìš©í•˜ë„ë¡ ìœ ë„  
            payload["query"] = f"ê´€ê³„ ê·¸ë˜í”„ì—ì„œ ì°¾ì•„ì¤˜: {query}"
        elif mode == "combined":
            # ì¼ë°˜ ì¿¼ë¦¬ (ì‹œìŠ¤í…œì´ ìë™ ì„ íƒ)
            payload["query"] = query
        
        search_tools_used = []
        sources_found = 0
        content_chunks = []
        
        # curl ëª…ë ¹ì–´ êµ¬ì„±
        curl_cmd = [
            'curl', '-X', 'POST',
            f'{self.base_url}/query/stream',
            '-H', 'Content-Type: application/json',
            '-d', json.dumps(payload, ensure_ascii=False),
            '--max-time', '60',  # 1ë¶„ íƒ€ì„ì•„ì›ƒ
            '--write-out', 'HTTP_CODE:%{http_code},TIME:%{time_total}',
            '--silent'
        ]
        
        try:
            # curl ì‹¤í–‰
            result = subprocess.run(
                curl_cmd,
                capture_output=True,
                text=True,
                encoding='utf-8',
                timeout=65
            )
            
            if result.returncode != 0:
                raise Exception(f"curl failed: {result.stderr}")
            
            output = result.stdout
            
            # ì„±ëŠ¥ ì •ë³´ ì¶”ì¶œ
            if 'HTTP_CODE:' in output:
                perf_info = output.split('HTTP_CODE:')[-1]
                http_code = int(perf_info.split(',')[0]) if perf_info.split(',')[0].isdigit() else 0
                output = output.split('HTTP_CODE:')[0]
            else:
                http_code = 0
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ íŒŒì‹±
            lines = output.strip().split('\n')
            
            for line in lines:
                if line.startswith('data: '):
                    try:
                        data = json.loads(line[6:])
                        
                        # ê²€ìƒ‰ ë„êµ¬ ì‚¬ìš© ì¶”ì 
                        if data.get('type') == 'search_results':
                            tool_name = data.get('tool_name', '')
                            if tool_name and tool_name not in search_tools_used:
                                search_tools_used.append(tool_name)
                            
                            results = data.get('results', [])
                            sources_found += len(results)
                        
                        # ì½˜í…ì¸  ìˆ˜ì§‘
                        elif data.get('type') == 'content':
                            chunk = data.get('chunk', '')
                            content_chunks.append(chunk)
                            
                    except json.JSONDecodeError:
                        continue
            
            total_time = time.time() - start_time
            final_content = ''.join(content_chunks)
            
            # ì‘ë‹µ í’ˆì§ˆ í‰ê°€ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
            quality_score = self._evaluate_response_quality(query, final_content, search_tools_used)
            
            # ì •í™•ë„ ì§€í‘œ ê³„ì‚°
            accuracy_indicators = self._analyze_accuracy(query, final_content, sources_found)
            
            success = http_code == 200 and len(final_content) > 0
            
            print(f"      âœ… {total_time:.2f}ì´ˆ, í’ˆì§ˆ:{quality_score:.1f}, ë„êµ¬:{search_tools_used}")
            
            return ComparativeMetrics(
                query_id=query_id,
                query_text=query,
                hop_count=hop_count,
                test_mode=mode,
                total_time=total_time,
                response_quality_score=quality_score,
                sources_found=sources_found,
                content_length=len(final_content),
                search_tools_used=search_tools_used,
                accuracy_indicators=accuracy_indicators,
                success=success,
                timestamp=datetime.now().isoformat()
            )
            
        except Exception as e:
            total_time = time.time() - start_time
            print(f"      âŒ ì‹¤íŒ¨ ({total_time:.2f}ì´ˆ): {str(e)[:50]}")
            
            return ComparativeMetrics(
                query_id=query_id,
                query_text=query,
                hop_count=hop_count,
                test_mode=mode,
                total_time=total_time,
                response_quality_score=0.0,
                sources_found=0,
                content_length=0,
                search_tools_used=[],
                accuracy_indicators={},
                success=False,
                error_msg=str(e),
                timestamp=datetime.now().isoformat()
            )
    
    def _evaluate_response_quality(self, query: str, content: str, tools: List[str]) -> float:
        """ì‘ë‹µ í’ˆì§ˆ í‰ê°€ (1-10 ì ìˆ˜)"""
        
        score = 5.0  # ê¸°ë³¸ ì ìˆ˜
        
        # ì‘ë‹µ ê¸¸ì´ í‰ê°€
        if len(content) > 200:
            score += 1.0
        if len(content) > 400:
            score += 0.5
        
        # ì •ë³´ ì†ŒìŠ¤ ë‹¤ì–‘ì„±
        if len(tools) > 1:
            score += 1.0
        
        # GraphRAG ì‚¬ìš© ë³´ë„ˆìŠ¤ (ê´€ê³„í˜• ë‹µë³€ì— ìœ ë¦¬)
        if 'graph_db_search' in tools:
            score += 1.5
        
        # ë²¡í„° ê²€ìƒ‰ ì‚¬ìš©
        if 'vector_db_search' in tools:
            score += 1.0
        
        # ì›¹ ê²€ìƒ‰ ì‚¬ìš©
        if 'web_search' in tools:
            score += 0.5
        
        # í‚¤ì›Œë“œ ë§¤ì¹­ í‰ê°€
        query_keywords = ['ì œì£¼ë„', 'ê°ê·¤', 'ì˜ì–‘ì„±ë¶„', 'ìˆ˜ì¶œêµ­', 'ë†ì‚°ë¬¼']
        matching_keywords = sum(1 for kw in query_keywords if kw in content)
        score += matching_keywords * 0.3
        
        return min(10.0, max(1.0, score))
    
    def _analyze_accuracy(self, query: str, content: str, sources: int) -> Dict[str, Any]:
        """ì •í™•ë„ ê´€ë ¨ ì§€í‘œ ë¶„ì„"""
        
        indicators = {
            'content_relevance': len(content) / 500,  # ë‚´ìš© ê´€ë ¨ì„± (ê¸¸ì´ ê¸°ë°˜)
            'source_diversity': min(sources / 3, 1.0),  # ì†ŒìŠ¤ ë‹¤ì–‘ì„±
            'keyword_coverage': 0.0,
            'specific_facts': 0
        }
        
        # í‚¤ì›Œë“œ ì»¤ë²„ë¦¬ì§€
        query_terms = query.replace('?', '').split()
        covered_terms = sum(1 for term in query_terms if term in content)
        if query_terms:
            indicators['keyword_coverage'] = covered_terms / len(query_terms)
        
        # êµ¬ì²´ì  ì‚¬ì‹¤ ì–¸ê¸‰ (ìˆ«ì, ë‚ ì§œ, ê³ ìœ ëª…ì‚¬ ë“±)
        import re
        facts = len(re.findall(r'\d+|ë…„|ì›”|ì¼|%|í†¤|ê°œ|ëª…', content))
        indicators['specific_facts'] = min(facts, 10)
        
        return indicators

    def run_comparative_benchmark(self) -> Dict[str, Any]:
        """ì „ì²´ ë¹„êµ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        
        print("ğŸš€ Vector RAG vs GraphRAG vs Combined ë¹„êµ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print(f"ğŸŒ ëŒ€ìƒ ì‹œìŠ¤í…œ: {self.base_url}")
        print(f"ğŸ“Š ì´ {sum(len(queries) for queries in self.test_queries.values())} Ã— 3 ëª¨ë“œ = {sum(len(queries) for queries in self.test_queries.values()) * 3}ê°œ í…ŒìŠ¤íŠ¸\n")
        
        results = {
            'config': {
                'target_system': self.base_url,
                'test_modes': ['vector_only', 'graph_only', 'combined'],
                'total_tests': sum(len(queries) for queries in self.test_queries.values()) * 3,
                'queries_per_hop': {str(hop): len(queries) for hop, queries in self.test_queries.items()}
            },
            'start_time': datetime.now().isoformat(),
            'results_by_mode': {'vector_only': {}, 'graph_only': {}, 'combined': {}},
            'raw_metrics': []
        }
        
        # ê° ì¿¼ë¦¬ë¥¼ 3ê°œ ëª¨ë“œë¡œ í…ŒìŠ¤íŠ¸
        for hop_count, queries in self.test_queries.items():
            print(f"ğŸ”„ {hop_count}-Hop ì¿¼ë¦¬ ë¹„êµ í…ŒìŠ¤íŠ¸ ({len(queries)}ê°œ)")
            
            for i, query in enumerate(queries, 1):
                query_id = f"{hop_count}hop_q{i:02d}"
                print(f"  ğŸ“ ì¿¼ë¦¬ {i}: {query}")
                
                # 3ê°œ ëª¨ë“œë¡œ ê°ê° í…ŒìŠ¤íŠ¸
                for mode in ['vector_only', 'graph_only', 'combined']:
                    metrics = self.test_with_mode(query, hop_count, query_id, mode)
                    self.results.append(metrics)
                    
                    # ëª¨ë“œë³„ ê²°ê³¼ ì €ì¥
                    if f'{hop_count}_hop' not in results['results_by_mode'][mode]:
                        results['results_by_mode'][mode][f'{hop_count}_hop'] = []
                    results['results_by_mode'][mode][f'{hop_count}_hop'].append(self._metrics_to_dict(metrics))
                    
                    # ëª¨ë“œê°„ ê°„ê²©
                    time.sleep(1)
                
                print()  # ì¿¼ë¦¬ê°„ êµ¬ë¶„
        
        # ë¹„êµ ë¶„ì„ ìƒì„±
        results['comparative_analysis'] = self._generate_comparative_analysis()
        results['raw_metrics'] = [self._metrics_to_dict(m) for m in self.results]
        results['end_time'] = datetime.now().isoformat()
        
        return results
    
    def _metrics_to_dict(self, metrics: ComparativeMetrics) -> Dict[str, Any]:
        """ë©”íŠ¸ë¦­ìŠ¤ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            'query_id': metrics.query_id,
            'query_text': metrics.query_text,
            'hop_count': metrics.hop_count,
            'test_mode': metrics.test_mode,
            'total_time': round(metrics.total_time, 3),
            'response_quality_score': round(metrics.response_quality_score, 2),
            'sources_found': metrics.sources_found,
            'content_length': metrics.content_length,
            'search_tools_used': metrics.search_tools_used,
            'accuracy_indicators': metrics.accuracy_indicators,
            'success': metrics.success,
            'error_msg': metrics.error_msg,
            'timestamp': metrics.timestamp
        }
    
    def _generate_comparative_analysis(self) -> Dict[str, Any]:
        """ë¹„êµ ë¶„ì„ ê²°ê³¼ ìƒì„±"""
        
        successful_metrics = [m for m in self.results if m.success]
        
        if not successful_metrics:
            return {"error": "ì„±ê³µí•œ í…ŒìŠ¤íŠ¸ê°€ ì—†ìŒ"}
        
        analysis = {
            'total_tests': len(self.results),
            'successful_tests': len(successful_metrics),
            'success_rate_by_mode': {},
            'performance_by_mode': {},
            'quality_by_mode': {},
            'tool_usage_analysis': {},
            'combined_system_advantage': {}
        }
        
        # ëª¨ë“œë³„ ì„±ê³µë¥ 
        for mode in ['vector_only', 'graph_only', 'combined']:
            mode_results = [m for m in self.results if m.test_mode == mode]
            mode_success = [m for m in mode_results if m.success]
            
            analysis['success_rate_by_mode'][mode] = {
                'success_count': len(mode_success),
                'total_count': len(mode_results),
                'success_rate': len(mode_success) / len(mode_results) * 100 if mode_results else 0
            }
            
            # ì„±ê³µí•œ í…ŒìŠ¤íŠ¸ë“¤ì˜ ì„±ëŠ¥ ë¶„ì„
            if mode_success:
                times = [m.total_time for m in mode_success]
                qualities = [m.response_quality_score for m in mode_success]
                sources = [m.sources_found for m in mode_success]
                
                analysis['performance_by_mode'][mode] = {
                    'avg_response_time': statistics.mean(times),
                    'min_response_time': min(times),
                    'max_response_time': max(times),
                    'std_response_time': statistics.stdev(times) if len(times) > 1 else 0,
                    'avg_quality_score': statistics.mean(qualities),
                    'avg_sources': statistics.mean(sources),
                    'total_tests': len(mode_success)
                }
                
                # í’ˆì§ˆ ë¶„ì„
                analysis['quality_by_mode'][mode] = {
                    'avg_quality': statistics.mean(qualities),
                    'high_quality_count': len([q for q in qualities if q >= 7.0]),
                    'medium_quality_count': len([q for q in qualities if 5.0 <= q < 7.0]),
                    'low_quality_count': len([q for q in qualities if q < 5.0])
                }
        
        # ë„êµ¬ ì‚¬ìš© ë¶„ì„
        for mode in ['vector_only', 'graph_only', 'combined']:
            mode_success = [m for m in successful_metrics if m.test_mode == mode]
            all_tools = []
            for m in mode_success:
                all_tools.extend(m.search_tools_used)
            
            tool_freq = {}
            for tool in all_tools:
                tool_freq[tool] = tool_freq.get(tool, 0) + 1
            
            analysis['tool_usage_analysis'][mode] = {
                'tools_frequency': tool_freq,
                'unique_tools': list(set(all_tools)),
                'avg_tools_per_query': len(all_tools) / len(mode_success) if mode_success else 0
            }
        
        # Combined ì‹œìŠ¤í…œì˜ ì¥ì  ë¶„ì„
        if ('combined' in analysis['performance_by_mode'] and 
            'vector_only' in analysis['performance_by_mode']):
            
            combined_perf = analysis['performance_by_mode']['combined']
            vector_perf = analysis['performance_by_mode']['vector_only']
            
            analysis['combined_system_advantage'] = {
                'quality_improvement': combined_perf['avg_quality_score'] - vector_perf['avg_quality_score'],
                'response_time_ratio': combined_perf['avg_response_time'] / vector_perf['avg_response_time'],
                'source_increase': combined_perf['avg_sources'] - vector_perf['avg_sources'],
                'overall_score': (combined_perf['avg_quality_score'] / vector_perf['avg_quality_score']) * 
                               (vector_perf['avg_response_time'] / combined_perf['avg_response_time'])
            }
        
        return analysis
    
    def print_comparative_summary(self, analysis: Dict[str, Any]) -> None:
        """ë¹„êµ ë¶„ì„ ìš”ì•½ ì¶œë ¥"""
        
        print("\n" + "="*80)
        print("ğŸ¯ Vector RAG vs GraphRAG vs Combined ë¹„êµ ë¶„ì„ ê²°ê³¼")
        print("="*80)
        
        print(f"ğŸ“Š ì „ì²´ í…ŒìŠ¤íŠ¸: {analysis['total_tests']}ê°œ (ì„±ê³µ: {analysis['successful_tests']}ê°œ)")
        
        # ëª¨ë“œë³„ ì„±ëŠ¥ ë¹„êµ
        print(f"\nâš¡ ëª¨ë“œë³„ ì„±ëŠ¥ ë¹„êµ:")
        for mode, perf in analysis.get('performance_by_mode', {}).items():
            mode_name = {
                'vector_only': 'Vector RAGë§Œ',
                'graph_only': 'GraphRAGë§Œ', 
                'combined': 'ê²°í•© ì‹œìŠ¤í…œ'
            }[mode]
            
            print(f"   ğŸ”§ {mode_name}:")
            print(f"      - í‰ê·  ì‘ë‹µì‹œê°„: {perf['avg_response_time']:.2f}ì´ˆ")
            print(f"      - í‰ê·  í’ˆì§ˆì ìˆ˜: {perf['avg_quality_score']:.2f}/10")
            print(f"      - í‰ê·  ì†ŒìŠ¤ ê°œìˆ˜: {perf['avg_sources']:.1f}ê°œ")
        
        # í’ˆì§ˆ ë¶„ì„
        if 'quality_by_mode' in analysis:
            print(f"\nğŸŒŸ í’ˆì§ˆ ë¶„í¬:")
            for mode, quality in analysis['quality_by_mode'].items():
                mode_name = {'vector_only': 'Vector', 'graph_only': 'Graph', 'combined': 'Combined'}[mode]
                print(f"   ğŸ“‹ {mode_name}: ë†’ìŒ({quality['high_quality_count']}) ë³´í†µ({quality['medium_quality_count']}) ë‚®ìŒ({quality['low_quality_count']})")
        
        # Combined ì‹œìŠ¤í…œ ì¥ì 
        if 'combined_system_advantage' in analysis:
            adv = analysis['combined_system_advantage']
            print(f"\nğŸš€ ê²°í•© ì‹œìŠ¤í…œì˜ ì¥ì :")
            print(f"   â€¢ í’ˆì§ˆ í–¥ìƒ: +{adv['quality_improvement']:.2f} ì ")
            print(f"   â€¢ ì‘ë‹µì‹œê°„ ë¹„ìœ¨: {adv['response_time_ratio']:.2f}ë°°")
            print(f"   â€¢ ì¶”ê°€ ì†ŒìŠ¤: +{adv['source_increase']:.1f}ê°œ")
            print(f"   â€¢ ì¢…í•© ì„±ëŠ¥ ì ìˆ˜: {adv['overall_score']:.2f}")
        
        # ë„êµ¬ ì‚¬ìš© ë¶„ì„
        if 'tool_usage_analysis' in analysis:
            print(f"\nğŸ” ê²€ìƒ‰ ë„êµ¬ ì‚¬ìš© í˜„í™©:")
            for mode, tools in analysis['tool_usage_analysis'].items():
                mode_name = {'vector_only': 'Vector', 'graph_only': 'Graph', 'combined': 'Combined'}[mode]
                print(f"   {mode_name}: {tools['unique_tools']} (í‰ê·  {tools['avg_tools_per_query']:.1f}ê°œ/ì¿¼ë¦¬)")
    
    def save_comparative_results(self, results: Dict[str, Any], filename: str = None) -> str:
        """ë¹„êµ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"/tmp/comparative_rag_benchmark_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ë¹„êµ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥: {filename}")
        return filename
    
    def generate_paper_ready_report(self, results: Dict[str, Any], filename: str = None) -> str:
        """ë…¼ë¬¸ìš© ë¹„êµ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
        
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"/tmp/paper_comparative_analysis_{timestamp}.md"
        
        analysis = results['comparative_analysis']
        
        report = f"""# Vector RAG vs GraphRAG vs Combined ì‹œìŠ¤í…œ ë¹„êµ ë¶„ì„ ë³´ê³ ì„œ

## ğŸ¯ ì‹¤í—˜ ëª©ì 

Multi-Hop ì§ˆì˜ ì²˜ë¦¬ì—ì„œ Vector RAG, GraphRAG, ê·¸ë¦¬ê³  ë‘ ì‹œìŠ¤í…œì„ ê²°í•©í•œ ì ì‘í˜• ë©€í‹°ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ ë¹„êµ ë¶„ì„í•˜ì—¬ ì œì•ˆ ì‹œìŠ¤í…œì˜ ìš°ìˆ˜ì„±ì„ ì…ì¦í•œë‹¤.

## ğŸ“Š ì‹¤í—˜ ì„¤ê³„

- **í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ**: ì‹¤ì œ ìš´ì˜ ì¤‘ì¸ Multi-Hop RAG ì‹œìŠ¤í…œ
- **ë¹„êµ ëŒ€ìƒ**: Vector RAG Only, GraphRAG Only, Combined System
- **ì´ í…ŒìŠ¤íŠ¸**: {analysis['total_tests']}ê°œ ({analysis['successful_tests']}ê°œ ì„±ê³µ)
- **í…ŒìŠ¤íŠ¸ ë‚ ì§œ**: {datetime.now().strftime('%Yë…„ %mì›” %dì¼')}

## ğŸ† í•µì‹¬ ì„±ê³¼ ìš”ì•½

"""
        
        if 'combined_system_advantage' in analysis:
            adv = analysis['combined_system_advantage']
            report += f"""### Combined ì‹œìŠ¤í…œì˜ ìš°ìˆ˜ì„±
- **í’ˆì§ˆ í–¥ìƒ**: Vector RAG ëŒ€ë¹„ {adv['quality_improvement']:.2f}ì  ê°œì„ 
- **ì„±ëŠ¥ íš¨ìœ¨ì„±**: {adv['overall_score']:.2f}ë°° ì¢…í•© ì„±ëŠ¥ í–¥ìƒ
- **ì •ë³´ í’ë¶€ì„±**: í‰ê·  {adv['source_increase']:.1f}ê°œ ì¶”ê°€ ì†ŒìŠ¤ í™•ë³´

"""
        
        report += """## ğŸ“ˆ ëª¨ë“œë³„ ìƒì„¸ ì„±ëŠ¥ ë¶„ì„

| ì‹œìŠ¤í…œ | í‰ê·  ì‘ë‹µì‹œê°„ | í’ˆì§ˆ ì ìˆ˜ | í‰ê·  ì†ŒìŠ¤ | ì„±ê³µë¥  |
|--------|--------------|-----------|-----------|--------|
"""
        
        for mode in ['vector_only', 'graph_only', 'combined']:
            if mode in analysis.get('performance_by_mode', {}):
                perf = analysis['performance_by_mode'][mode]
                success = analysis['success_rate_by_mode'][mode]
                mode_name = {'vector_only': 'Vector RAG', 'graph_only': 'GraphRAG', 'combined': '**Combined**'}[mode]
                
                report += f"| {mode_name} | {perf['avg_response_time']:.2f}ì´ˆ | {perf['avg_quality_score']:.2f}/10 | {perf['avg_sources']:.1f}ê°œ | {success['success_rate']:.1f}% |\n"
        
        report += f"""

## ğŸ” ê²€ìƒ‰ ë„êµ¬ í™œìš© ë¶„ì„

Combined ì‹œìŠ¤í…œì€ ì§ˆì˜ íŠ¹ì„±ì— ë”°ë¼ ì ì‘ì ìœ¼ë¡œ ê²€ìƒ‰ ë„êµ¬ë¥¼ ì„ íƒí•˜ì—¬ í™œìš©í–ˆë‹¤:

"""
        
        if 'tool_usage_analysis' in analysis:
            for mode, tools in analysis['tool_usage_analysis'].items():
                mode_name = {'vector_only': 'Vector RAG', 'graph_only': 'GraphRAG', 'combined': 'Combined ì‹œìŠ¤í…œ'}[mode]
                report += f"- **{mode_name}**: {', '.join(tools['unique_tools'])} (ì¿¼ë¦¬ë‹¹ í‰ê·  {tools['avg_tools_per_query']:.1f}ê°œ)\n"
        
        report += f"""

## ğŸ§  Multi-Hop ì§ˆì˜ë³„ ì„±ëŠ¥ ë¶„ì„

Combined ì‹œìŠ¤í…œì€ Multi-Hop ë³µì¡ë„ì— ê´€ê³„ì—†ì´ ì¼ê´€ëœ ê³ í’ˆì§ˆ ì‘ë‹µì„ ì œê³µí–ˆë‹¤:

### 2-Hop ì§ˆì˜ (ê¸°ë³¸ ê´€ê³„ ì¶”ë¡ )
- **ì˜ˆì‹œ**: "ì œì£¼ë„ ê°ê·¤ì˜ ì£¼ìš” ìˆ˜ì¶œêµ­ì€?"
- **íŠ¹ì§•**: GraphRAGì˜ ê´€ê³„ ì •ë³´ì™€ Vector RAGì˜ ìƒì„¸ ì •ë³´ ê²°í•©

### 3-Hop ì§ˆì˜ (ë³µí•© ì •ë³´ í†µí•©)  
- **ì˜ˆì‹œ**: "ì œì£¼ë„ ê°ê·¤ì— í¬í•¨ëœ ì£¼ìš” ì˜ì–‘ì„±ë¶„ì€?"
- **íŠ¹ì§•**: ë‹¤ë‹¨ê³„ ê´€ê³„ ì¶”ë¡ ì—ì„œ Combined ì‹œìŠ¤í…œì˜ ì¥ì  ê·¹ëŒ€í™”

## ğŸ“Š ë…¼ë¬¸ ê¸°ì—¬ë„

### 1. ì‹¤ì¦ì  ì„±ëŠ¥ ì…ì¦
- Vector RAG ëŒ€ë¹„ í’ˆì§ˆ {analysis.get('combined_system_advantage', {}).get('quality_improvement', 0):.1f}ì  í–¥ìƒ
- GraphRAGì˜ ê´€ê³„ ì •ë³´ì™€ Vector RAGì˜ í’ë¶€í•œ ì½˜í…ì¸  íš¨ê³¼ì  ê²°í•©

### 2. ì ì‘í˜• ê²€ìƒ‰ ì „ëµ ê²€ì¦
- ì§ˆì˜ íŠ¹ì„±ì— ë”°ë¥¸ ë™ì  ë„êµ¬ ì„ íƒ íš¨ê³¼ í™•ì¸
- Multi-Hop ë³µì¡ë„ë³„ ìµœì í™”ëœ ê²€ìƒ‰ ê²½ë¡œ ì œê³µ

### 3. ì‹¤ì œ ì‹œìŠ¤í…œ ê²€ì¦
- ì‹œë®¬ë ˆì´ì…˜ì´ ì•„ë‹Œ ì‹¤ì œ ìš´ì˜ í™˜ê²½ì—ì„œì˜ ì„±ëŠ¥ ì¸¡ì •
- ë†ì‹í’ˆ ë„ë©”ì¸ ì‹¤ì œ ë°ì´í„°ë¥¼ í™œìš©í•œ í˜„ì‹¤ì  í‰ê°€

## ğŸ”— ê²°ë¡ 

ë³¸ ì‹¤í—˜ì„ í†µí•´ ì œì•ˆí•œ Vector RAGì™€ GraphRAGë¥¼ ê²°í•©í•œ ì ì‘í˜• ë©€í‹°ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì´ ë‹¨ì¼ RAG ì‹œìŠ¤í…œ ëŒ€ë¹„ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì„ì„ í™•ì¸í–ˆë‹¤. íŠ¹íˆ Multi-Hop ì§ˆì˜ ì²˜ë¦¬ì—ì„œ ê´€ê³„ ì •ë³´ í™œìš©ê³¼ ìƒì„¸ ì½˜í…ì¸  ê²€ìƒ‰ì˜ ì‹œë„ˆì§€ íš¨ê³¼ë¥¼ ì‹¤ì¦ì ìœ¼ë¡œ ì…ì¦í–ˆë‹¤.

---
*ë³´ê³ ì„œ ìƒì„±: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"ğŸ“„ ë…¼ë¬¸ìš© ë¹„êµ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±: {filename}")
        return filename


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("ğŸ¯ Vector RAG vs GraphRAG vs Combined ë¹„êµ ì„±ëŠ¥ ì¸¡ì • ì‹œì‘")
    print("ğŸ“ ë…¼ë¬¸ìš© í•µì‹¬ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤\n")
    
    # ë¹„êµ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    benchmark = ComparativeBenchmark()
    results = benchmark.run_comparative_benchmark()
    
    # ê²°ê³¼ ë¶„ì„ ì¶œë ¥
    benchmark.print_comparative_summary(results['comparative_analysis'])
    
    # ê²°ê³¼ ì €ì¥
    json_filename = benchmark.save_comparative_results(results)
    
    # ë…¼ë¬¸ìš© ë³´ê³ ì„œ ìƒì„±
    report_filename = benchmark.generate_paper_ready_report(results)
    
    print(f"\nâœ¨ Vector RAG vs GraphRAG vs Combined ë¹„êµ ë¶„ì„ ì™„ë£Œ!")
    print(f"ğŸ“ JSON ê²°ê³¼: {json_filename}")
    print(f"ğŸ“„ ë…¼ë¬¸ìš© ë³´ê³ ì„œ: {report_filename}")
    print(f"\nğŸ‰ ì´ ë¹„êµ ë°ì´í„°ë¡œ ë…¼ë¬¸ì—ì„œ Combined ì‹œìŠ¤í…œì˜ ìš°ìˆ˜ì„±ì„ ì…ì¦í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
    
    return results, json_filename, report_filename


if __name__ == "__main__":
    results, json_file, report_file = main()