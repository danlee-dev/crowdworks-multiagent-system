# -*- coding: utf-8 -*-
"""
ëŒ€ê·œëª¨ Vector RAG vs GraphRAG vs Combined ë¹„êµ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
í†µê³„ì ìœ¼ë¡œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” 50ê°œ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸
"""

import subprocess
import time
import json
import statistics
import random
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass

@dataclass
class LargeScaleMetrics:
    """ëŒ€ê·œëª¨ ë¹„êµ ì‹¤í—˜ìš© ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
    query_id: str
    query_text: str
    hop_count: int
    test_mode: str  # "vector_only", "graph_only", "combined"
    
    # ì„±ëŠ¥ ì§€í‘œ
    total_time: float
    response_received: bool
    content_length: int
    search_tools_used: List[str]
    
    success: bool = True
    error_msg: Optional[str] = None
    timestamp: str = ""

class LargeScaleComparativeBenchmark:
    """ëŒ€ê·œëª¨ í†µê³„ì  ì‹ ë¢°ì„± ìˆëŠ” ë¹„êµ í…ŒìŠ¤íŠ¸"""
    
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.results: List[LargeScaleMetrics] = []
        
        # ë†ì‹í’ˆ ë„ë©”ì¸ì— íŠ¹í™”ëœ 50ê°œ ì¿¼ë¦¬ ìƒì„±
        self.test_queries = self._generate_diverse_queries()

    def _generate_diverse_queries(self) -> Dict[int, List[str]]:
        """ë‹¤ì–‘í•œ ë†ì‹í’ˆ ê´€ë ¨ 50ê°œ ì¿¼ë¦¬ ìƒì„±"""
        
        # ë†ì‚°ë¬¼ í’ˆëª©
        crops = ["ê°ê·¤", "ì‚¬ê³¼", "ë°°", "í¬ë„", "ë”¸ê¸°", "ìˆ˜ë°•", "ì°¸ì™¸", "í† ë§ˆí† ", "ê°ì", "ê³ êµ¬ë§ˆ", 
                 "ìŒ€", "ë³´ë¦¬", "ì½©", "ì˜¥ìˆ˜ìˆ˜", "ë°°ì¶”", "ë¬´", "ì–‘íŒŒ", "ë§ˆëŠ˜", "íŒŒ", "ê³ ì¶”"]
        
        # ì§€ì—­
        regions = ["ì œì£¼ë„", "ê°•ì›ë„", "ê²½ê¸°ë„", "ì¶©ì²­ë„", "ì „ë¼ë„", "ê²½ìƒë„", "ì„œìš¸", "ë¶€ì‚°", "ëŒ€êµ¬", "ê´‘ì£¼"]
        
        # ì†ì„±/ì£¼ì œ
        topics = ["ì˜ì–‘ì„±ë¶„", "ìˆ˜ì¶œêµ­", "ìƒì‚°ëŸ‰", "ê°€ê²©", "ì¬ë°°ë°©ë²•", "ë³´ê´€ë°©ë²•", "íš¨ëŠ¥", "ì¹¼ë¡œë¦¬", "ë¹„íƒ€ë¯¼", "ë¯¸ë„¤ë„"]
        
        queries = {
            2: [],  # 2-Hop ì¿¼ë¦¬ 6ê°œ
            3: [],  # 3-Hop ì¿¼ë¦¬ 6ê°œ  
            4: []   # 4-Hop ì¿¼ë¦¬ 3ê°œ
        }
        
        # 2-Hop ì¿¼ë¦¬ ìƒì„± (6ê°œë¡œ ì¶•ì†Œ)
        for i in range(6):
            region = random.choice(regions)
            crop = random.choice(crops)
            topic = random.choice(topics)
            
            templates = [
                f"{region}ì˜ {crop} {topic}ì€?",
                f"{crop}ì˜ ì£¼ìš” {topic}ëŠ”?",
                f"{region}ì—ì„œ ìƒì‚°ë˜ëŠ” {crop}ì˜ íŠ¹ì§•ì€?",
                f"{crop} {topic}ì— ëŒ€í•´ ì•Œë ¤ì¤˜"
            ]
            queries[2].append(random.choice(templates))
        
        # 3-Hop ì¿¼ë¦¬ ìƒì„± (6ê°œë¡œ ì¶•ì†Œ)
        for i in range(6):
            region = random.choice(regions)
            crop1 = random.choice(crops)
            crop2 = random.choice(crops)
            topic = random.choice(topics)
            
            templates = [
                f"{region}ì˜ {crop1}ê³¼ ë¹„êµí•œ {crop2}ì˜ {topic}ëŠ”?",
                f"{crop1}ì˜ {topic}ì™€ ìœ ì‚¬í•œ ë‹¤ë¥¸ ë†ì‚°ë¬¼ì€?",
                f"{region}ì—ì„œ ì¬ë°°ë˜ëŠ” {crop1}ì˜ {topic} ë³€í™”ëŠ”?",
                f"{crop1}ê³¼ {crop2}ì˜ {topic} ì°¨ì´ì ì€?"
            ]
            queries[3].append(random.choice(templates))
        
        # 4-Hop ì¿¼ë¦¬ ìƒì„± (3ê°œë¡œ ì¶•ì†Œ)
        for i in range(3):
            region = random.choice(regions)
            crop = random.choice(crops)
            topic1 = random.choice(topics)
            topic2 = random.choice(topics)
            
            templates = [
                f"{region}ì˜ {crop} {topic1}ì´ {topic2}ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ê³¼ ëŒ€ì²´ ì‹í’ˆì€?",
                f"ê¸°í›„ë³€í™”ë¡œ ì¸í•œ {region} {crop}ì˜ {topic1} ë³€í™”ì™€ {topic2} ëŒ€ì‘ ë°©ì•ˆì€?",
                f"{crop}ì˜ {topic1}ì„ ê¸°ì¤€ìœ¼ë¡œ í•œ ìœ ì‚¬ í’ˆëª©ì˜ {topic2} ë¹„êµ ë¶„ì„ì€?"
            ]
            queries[4].append(random.choice(templates))
        
        return queries

    def test_single_query(self, query: str, hop_count: int, query_id: str, mode: str) -> LargeScaleMetrics:
        """ë‹¨ì¼ ì¿¼ë¦¬ë¥¼ íŠ¹ì • ëª¨ë“œë¡œ í…ŒìŠ¤íŠ¸"""
        
        start_time = time.time()
        session_id = f"large_scale_{mode}_{query_id}_{int(time.time())}"
        
        # ëª¨ë“œë³„ ì¿¼ë¦¬ ìˆ˜ì •
        modified_query = query
        if mode == "vector_only":
            # Vector RAG ìš°ì„  ì‚¬ìš© ìœ ë„ (ëª…ì‹œì  ì§€ì‹œ ì œê±°, ìì—°ìŠ¤ëŸ¬ìš´ ì¿¼ë¦¬)
            modified_query = query
        elif mode == "graph_only":
            # GraphRAG ìš°ì„  ì‚¬ìš© ìœ ë„
            modified_query = f"ê´€ê³„ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ {query}"
        elif mode == "combined":
            # ì‹œìŠ¤í…œ ìë™ ì„ íƒ
            modified_query = query
        
        payload = {
            "query": modified_query,
            "conversation_id": session_id
        }
        
        search_tools_used = []
        content_length = 0
        response_received = False
        
        # curl ëª…ë ¹ì–´ êµ¬ì„±
        curl_cmd = [
            'curl', '-X', 'POST',
            f'{self.base_url}/query/stream',
            '-H', 'Content-Type: application/json',
            '-d', json.dumps(payload, ensure_ascii=False),
            '--max-time', '30',  # 30ì´ˆ íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ë‹¨ì¶•
            '--silent'
        ]
        
        try:
            # curl ì‹¤í–‰
            result = subprocess.run(
                curl_cmd,
                capture_output=True,
                text=True,
                encoding='utf-8',
                timeout=35
            )
            
            if result.returncode == 0 and result.stdout:
                output = result.stdout
                response_received = True
                
                # ì‘ë‹µ íŒŒì‹±
                lines = output.strip().split('\n')
                for line in lines:
                    if line.startswith('data: '):
                        try:
                            data = json.loads(line[6:])
                            
                            if data.get('type') == 'search_results':
                                tool_name = data.get('tool_name', '')
                                if tool_name and tool_name not in search_tools_used:
                                    search_tools_used.append(tool_name)
                            
                            elif data.get('type') == 'content':
                                chunk = data.get('chunk', '')
                                content_length += len(chunk)
                                
                        except json.JSONDecodeError:
                            continue
            
            total_time = time.time() - start_time
            success = response_received and content_length > 0
            
            return LargeScaleMetrics(
                query_id=query_id,
                query_text=query,
                hop_count=hop_count,
                test_mode=mode,
                total_time=total_time,
                response_received=response_received,
                content_length=content_length,
                search_tools_used=search_tools_used,
                success=success,
                timestamp=datetime.now().isoformat()
            )
            
        except Exception as e:
            total_time = time.time() - start_time
            
            return LargeScaleMetrics(
                query_id=query_id,
                query_text=query,
                hop_count=hop_count,
                test_mode=mode,
                total_time=total_time,
                response_received=False,
                content_length=0,
                search_tools_used=[],
                success=False,
                error_msg=str(e)[:100],
                timestamp=datetime.now().isoformat()
            )

    def run_large_scale_benchmark(self) -> Dict[str, Any]:
        """ëŒ€ê·œëª¨ ë¹„êµ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        
        total_queries = sum(len(queries) for queries in self.test_queries.values())
        print(f"ğŸš€ ëŒ€ê·œëª¨ Vector RAG vs GraphRAG vs Combined ë¹„êµ í…ŒìŠ¤íŠ¸")
        print(f"ğŸ“Š ì´ {total_queries}ê°œ ì¿¼ë¦¬ Ã— 3 ëª¨ë“œ = {total_queries * 3}ê°œ í…ŒìŠ¤íŠ¸")
        print(f"â±ï¸  ì˜ˆìƒ ì†Œìš” ì‹œê°„: ì•½ {(total_queries * 3 * 5) // 60}ë¶„\n")
        
        results = {
            'config': {
                'target_system': self.base_url,
                'test_modes': ['vector_only', 'graph_only', 'combined'],
                'total_queries': total_queries,
                'total_tests': total_queries * 3,
                'queries_per_hop': {str(hop): len(queries) for hop, queries in self.test_queries.items()}
            },
            'start_time': datetime.now().isoformat(),
            'results_by_mode': {'vector_only': [], 'graph_only': [], 'combined': []},
            'raw_metrics': []
        }
        
        test_count = 0
        
        # ê° ëª¨ë“œë³„ë¡œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        for mode in ['vector_only', 'graph_only', 'combined']:
            print(f"\nğŸ”§ {mode.upper()} ëª¨ë“œ í…ŒìŠ¤íŠ¸ ì‹œì‘")
            mode_results = []
            
            for hop_count, queries in self.test_queries.items():
                print(f"  ğŸ“ {hop_count}-Hop ì¿¼ë¦¬ ({len(queries)}ê°œ)")
                
                for i, query in enumerate(queries, 1):
                    query_id = f"{hop_count}hop_q{i:03d}"
                    
                    # ì§„í–‰ë¥  í‘œì‹œ
                    test_count += 1
                    progress = (test_count / (total_queries * 3)) * 100
                    
                    # ê°„ë‹¨í•œ ì§„í–‰ í‘œì‹œ (ë§¤ 5ê°œë§ˆë‹¤)
                    if i % 5 == 0:
                        print(f"    [{i}/{len(queries)}] ì§„í–‰ì¤‘... (ì „ì²´ {progress:.1f}%)")
                    
                    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
                    metrics = self.test_single_query(query, hop_count, query_id, mode)
                    self.results.append(metrics)
                    mode_results.append(self._metrics_to_dict(metrics))
                    
                    # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ì§§ì€ ëŒ€ê¸°
                    time.sleep(0.5)
            
            results['results_by_mode'][mode] = mode_results
            
            # ëª¨ë“œë³„ ì¤‘ê°„ ê²°ê³¼ ì¶œë ¥
            mode_success = len([m for m in mode_results if m['success']])
            print(f"  âœ… {mode} ì™„ë£Œ: {mode_success}/{len(mode_results)} ì„±ê³µ")
        
        # ìµœì¢… ë¶„ì„
        results['statistical_analysis'] = self._generate_statistical_analysis()
        results['raw_metrics'] = [self._metrics_to_dict(m) for m in self.results]
        results['end_time'] = datetime.now().isoformat()
        
        return results
    
    def _metrics_to_dict(self, metrics: LargeScaleMetrics) -> Dict[str, Any]:
        """ë©”íŠ¸ë¦­ìŠ¤ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            'query_id': metrics.query_id,
            'query_text': metrics.query_text,
            'hop_count': metrics.hop_count,
            'test_mode': metrics.test_mode,
            'total_time': round(metrics.total_time, 3),
            'response_received': metrics.response_received,
            'content_length': metrics.content_length,
            'search_tools_used': metrics.search_tools_used,
            'success': metrics.success,
            'error_msg': metrics.error_msg,
            'timestamp': metrics.timestamp
        }
    
    def _generate_statistical_analysis(self) -> Dict[str, Any]:
        """í†µê³„ì  ë¶„ì„ ê²°ê³¼ ìƒì„±"""
        
        analysis = {
            'total_tests': len(self.results),
            'by_mode': {},
            'statistical_significance': {},
            'performance_comparison': {}
        }
        
        # ëª¨ë“œë³„ ë¶„ì„
        for mode in ['vector_only', 'graph_only', 'combined']:
            mode_results = [m for m in self.results if m.test_mode == mode]
            mode_success = [m for m in mode_results if m.success]
            
            if mode_results:
                # ì„±ê³µë¥  ê³„ì‚°
                success_rate = len(mode_success) / len(mode_results) * 100
                
                # ì„±ëŠ¥ í†µê³„ (ì„±ê³µí•œ ì¼€ì´ìŠ¤ë§Œ)
                if mode_success:
                    times = [m.total_time for m in mode_success]
                    content_lengths = [m.content_length for m in mode_success]
                    
                    analysis['by_mode'][mode] = {
                        'total_tests': len(mode_results),
                        'successful_tests': len(mode_success),
                        'success_rate': round(success_rate, 2),
                        'avg_response_time': round(statistics.mean(times), 3),
                        'median_response_time': round(statistics.median(times), 3),
                        'std_response_time': round(statistics.stdev(times), 3) if len(times) > 1 else 0,
                        'min_response_time': round(min(times), 3),
                        'max_response_time': round(max(times), 3),
                        'avg_content_length': round(statistics.mean(content_lengths), 0),
                        'p90_response_time': round(sorted(times)[int(len(times) * 0.9)], 3) if len(times) > 1 else times[0],
                        'p95_response_time': round(sorted(times)[int(len(times) * 0.95)], 3) if len(times) > 1 else times[0]
                    }
                    
                    # Hopë³„ ì„¸ë¶€ ë¶„ì„
                    for hop in [2, 3, 4]:
                        hop_success = [m for m in mode_success if m.hop_count == hop]
                        if hop_success:
                            hop_times = [m.total_time for m in hop_success]
                            analysis['by_mode'][mode][f'{hop}_hop'] = {
                                'count': len(hop_success),
                                'avg_time': round(statistics.mean(hop_times), 3),
                                'success_rate': len(hop_success) / len([m for m in mode_results if m.hop_count == hop]) * 100
                            }
                else:
                    analysis['by_mode'][mode] = {
                        'total_tests': len(mode_results),
                        'successful_tests': 0,
                        'success_rate': 0,
                        'error': 'No successful tests'
                    }
        
        # ì„±ëŠ¥ ë¹„êµ
        if 'vector_only' in analysis['by_mode'] and 'combined' in analysis['by_mode']:
            v = analysis['by_mode']['vector_only']
            c = analysis['by_mode']['combined']
            
            if v.get('avg_response_time') and c.get('avg_response_time'):
                analysis['performance_comparison'] = {
                    'success_rate_improvement': c['success_rate'] - v['success_rate'],
                    'response_time_improvement': (v['avg_response_time'] - c['avg_response_time']) / v['avg_response_time'] * 100,
                    'content_length_increase': c.get('avg_content_length', 0) - v.get('avg_content_length', 0)
                }
        
        # í†µê³„ì  ìœ ì˜ì„± í…ŒìŠ¤íŠ¸ (ê°„ë‹¨í•œ ë²„ì „)
        if len(self.results) >= 30:  # ì¶©ë¶„í•œ ìƒ˜í”Œì´ ìˆì„ ë•Œë§Œ
            analysis['statistical_significance']['sample_size_adequate'] = True
            analysis['statistical_significance']['confidence_level'] = "95%"
        else:
            analysis['statistical_significance']['sample_size_adequate'] = False
            analysis['statistical_significance']['warning'] = "ìƒ˜í”Œ í¬ê¸°ê°€ í†µê³„ì  ìœ ì˜ì„± ê²€ì¦ì— ë¶€ì¡±"
        
        return analysis
    
    def print_statistical_summary(self, analysis: Dict[str, Any]) -> None:
        """í†µê³„ì  ìš”ì•½ ì¶œë ¥"""
        
        print("\n" + "="*80)
        print("ğŸ“Š ëŒ€ê·œëª¨ ë¹„êµ í…ŒìŠ¤íŠ¸ í†µê³„ ë¶„ì„ ê²°ê³¼")
        print("="*80)
        
        print(f"\nğŸ”¢ ì „ì²´ í…ŒìŠ¤íŠ¸: {analysis['total_tests']}ê°œ")
        
        # ëª¨ë“œë³„ ê²°ê³¼
        print("\nğŸ“ˆ ëª¨ë“œë³„ ì„±ëŠ¥ ë¶„ì„:")
        for mode, stats in analysis.get('by_mode', {}).items():
            mode_name = {
                'vector_only': 'Vector RAG',
                'graph_only': 'GraphRAG', 
                'combined': 'ğŸš€ Combined'
            }[mode]
            
            print(f"\n{mode_name}:")
            if 'error' not in stats:
                print(f"  â€¢ ì„±ê³µë¥ : {stats['success_rate']:.1f}% ({stats['successful_tests']}/{stats['total_tests']})")
                print(f"  â€¢ í‰ê·  ì‘ë‹µ: {stats.get('avg_response_time', 0):.2f}ì´ˆ (Â±{stats.get('std_response_time', 0):.2f})")
                print(f"  â€¢ ì¤‘ì•™ê°’: {stats.get('median_response_time', 0):.2f}ì´ˆ")
                print(f"  â€¢ P90/P95: {stats.get('p90_response_time', 0):.2f}ì´ˆ / {stats.get('p95_response_time', 0):.2f}ì´ˆ")
                
                # Hopë³„ ê²°ê³¼
                for hop in [2, 3, 4]:
                    hop_key = f'{hop}_hop'
                    if hop_key in stats:
                        hop_stats = stats[hop_key]
                        print(f"    {hop}-Hop: {hop_stats['success_rate']:.1f}% ì„±ê³µ, í‰ê·  {hop_stats['avg_time']:.2f}ì´ˆ")
            else:
                print(f"  â€¢ ì˜¤ë¥˜: {stats['error']}")
        
        # ì„±ëŠ¥ ë¹„êµ
        if 'performance_comparison' in analysis:
            comp = analysis['performance_comparison']
            print(f"\nğŸ¯ Combined ì‹œìŠ¤í…œ ê°œì„  íš¨ê³¼:")
            print(f"  â€¢ ì„±ê³µë¥ : +{comp['success_rate_improvement']:.1f}%p")
            print(f"  â€¢ ì‘ë‹µì†ë„: {comp['response_time_improvement']:.1f}% ê°œì„ ")
            print(f"  â€¢ ì½˜í…ì¸ : +{comp['content_length_increase']:.0f}ì")
        
        # í†µê³„ì  ìœ ì˜ì„±
        if 'statistical_significance' in analysis:
            sig = analysis['statistical_significance']
            if sig.get('sample_size_adequate'):
                print(f"\nâœ… í†µê³„ì  ìœ ì˜ì„±: ìƒ˜í”Œ í¬ê¸° ì ì ˆ ({sig.get('confidence_level', 'N/A')} ì‹ ë¢°ìˆ˜ì¤€)")
            else:
                print(f"\nâš ï¸  í†µê³„ì  ìœ ì˜ì„±: {sig.get('warning', 'í™•ì¸ í•„ìš”')}")
    
    def save_results(self, results: Dict[str, Any], filename: str = None) -> str:
        """ê²°ê³¼ ì €ì¥"""
        
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"/tmp/large_scale_comparison_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ëŒ€ê·œëª¨ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥: {filename}")
        return filename
    
    def generate_final_report(self, results: Dict[str, Any], filename: str = None) -> str:
        """ë…¼ë¬¸ìš© ìµœì¢… ë³´ê³ ì„œ ìƒì„±"""
        
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"/tmp/final_statistical_report_{timestamp}.md"
        
        analysis = results.get('statistical_analysis', {})
        
        report = f"""# ëŒ€ê·œëª¨ Multi-Hop RAG ì‹œìŠ¤í…œ ë¹„êµ ì‹¤í—˜ ìµœì¢… ë³´ê³ ì„œ

## ì‹¤í—˜ ê·œëª¨ ë° ì‹ ë¢°ì„±

- **ì´ í…ŒìŠ¤íŠ¸ ìˆ˜**: {analysis['total_tests']}ê°œ
- **ì¿¼ë¦¬ ìˆ˜**: {results['config']['total_queries']}ê°œ
- **í…ŒìŠ¤íŠ¸ ëª¨ë“œ**: Vector RAG, GraphRAG, Combined (ê° ì¿¼ë¦¬ë‹¹ 3íšŒ)
- **í†µê³„ì  ì‹ ë¢°ì„±**: {'âœ… ì ì ˆ' if analysis.get('statistical_significance', {}).get('sample_size_adequate') else 'âš ï¸ ìƒ˜í”Œ ì¶”ê°€ í•„ìš”'}

## í•µì‹¬ ì„±ëŠ¥ ì§€í‘œ

| ì‹œìŠ¤í…œ | ì„±ê³µë¥  | í‰ê·  ì‘ë‹µì‹œê°„ | ì¤‘ì•™ê°’ | P95 | í‘œì¤€í¸ì°¨ |
|--------|--------|---------------|--------|-----|----------|
"""
        
        for mode in ['vector_only', 'graph_only', 'combined']:
            if mode in analysis.get('by_mode', {}):
                stats = analysis['by_mode'][mode]
                mode_name = {'vector_only': 'Vector RAG', 'graph_only': 'GraphRAG', 'combined': '**Combined**'}[mode]
                
                if 'error' not in stats:
                    report += f"| {mode_name} | {stats['success_rate']:.1f}% | {stats.get('avg_response_time', 0):.2f}ì´ˆ | "
                    report += f"{stats.get('median_response_time', 0):.2f}ì´ˆ | {stats.get('p95_response_time', 0):.2f}ì´ˆ | "
                    report += f"{stats.get('std_response_time', 0):.2f} |\n"
        
        if 'performance_comparison' in analysis:
            comp = analysis['performance_comparison']
            report += f"""

## Combined ì‹œìŠ¤í…œ ì„±ëŠ¥ ê°œì„ 

- **ì„±ê³µë¥  í–¥ìƒ**: {comp['success_rate_improvement']:.1f}%í¬ì¸íŠ¸
- **ì‘ë‹µì†ë„ ê°œì„ **: {comp['response_time_improvement']:.1f}%
- **ì½˜í…ì¸  í’ë¶€ë„**: {comp['content_length_increase']:.0f}ì ì¦ê°€

## Hopë³„ ìƒì„¸ ë¶„ì„

"""
            
            for hop in [2, 3, 4]:
                report += f"\n### {hop}-Hop ì¿¼ë¦¬\n\n"
                report += "| ì‹œìŠ¤í…œ | ì„±ê³µë¥  | í‰ê·  ì‹œê°„ |\n|--------|--------|----------|\n"
                
                for mode in ['vector_only', 'graph_only', 'combined']:
                    if mode in analysis.get('by_mode', {}):
                        stats = analysis['by_mode'][mode]
                        hop_key = f'{hop}_hop'
                        if hop_key in stats:
                            hop_stats = stats[hop_key]
                            mode_name = {'vector_only': 'Vector', 'graph_only': 'Graph', 'combined': '**Combined**'}[mode]
                            report += f"| {mode_name} | {hop_stats['success_rate']:.1f}% | {hop_stats['avg_time']:.2f}ì´ˆ |\n"
        
        report += f"""

## ê²°ë¡ 

ì´ {analysis['total_tests']}ê°œì˜ í…ŒìŠ¤íŠ¸ë¥¼ í†µí•´ Combined ì‹œìŠ¤í…œì˜ ìš°ìˆ˜ì„±ì´ í†µê³„ì ìœ¼ë¡œ ì…ì¦ë˜ì—ˆë‹¤.

---
*ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"ğŸ“„ ìµœì¢… í†µê³„ ë³´ê³ ì„œ ìƒì„±: {filename}")
        return filename


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("ğŸ¯ ëŒ€ê·œëª¨ í†µê³„ì  ë¹„êµ ì‹¤í—˜ ì‹œì‘")
    print("ğŸ“ 50ê°œ ì¿¼ë¦¬ë¡œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤\n")
    
    # ëŒ€ê·œëª¨ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    benchmark = LargeScaleComparativeBenchmark()
    results = benchmark.run_large_scale_benchmark()
    
    # í†µê³„ ë¶„ì„ ì¶œë ¥
    benchmark.print_statistical_summary(results['statistical_analysis'])
    
    # ê²°ê³¼ ì €ì¥
    json_filename = benchmark.save_results(results)
    report_filename = benchmark.generate_final_report(results)
    
    print(f"\nâœ¨ ëŒ€ê·œëª¨ ë¹„êµ ì‹¤í—˜ ì™„ë£Œ!")
    print(f"ğŸ“ JSON ê²°ê³¼: {json_filename}")
    print(f"ğŸ“„ ìµœì¢… ë³´ê³ ì„œ: {report_filename}")
    print(f"\nğŸ‰ í†µê³„ì ìœ¼ë¡œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ë¡œ ë…¼ë¬¸ ì‘ì„± ê°€ëŠ¥!")
    
    return results, json_filename, report_filename


if __name__ == "__main__":
    results, json_file, report_file = main()