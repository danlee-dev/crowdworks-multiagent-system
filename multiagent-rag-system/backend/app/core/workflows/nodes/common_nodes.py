"""
Common Nodes for RAG Workflow

Shared nodes used across both chat and task flows:
- triage_node: Classify request as 'chat' or 'task'
- abort_check_node: Check if execution should be aborted
"""

import json
import re
from datetime import datetime
from typing import Literal

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.runnables import RunnableConfig

from ..state import RAGState, log_state_transition


# ============================================================================
# Triage Node
# ============================================================================

async def triage_node(state: RAGState, config: RunnableConfig) -> RAGState:
    """
    Triage node: Classify request as 'chat' or 'task'.

    This node analyzes the user query and determines the appropriate workflow:
    - 'chat': Simple Q&A that can be answered with basic search + LLM
    - 'task': Complex analysis requiring multi-step data gathering and reporting

    Based on: TriageAgent.classify_request() from orchestrator.py:37-110

    Args:
        state: Current RAGState
        config: LangGraph runtime configuration

    Returns:
        Updated state with flow_type set
    """
    print(f"\n{'='*60}")
    print(f"ğŸ” [Triage Node] Classifying request")
    print(f"   Query: {state['original_query'][:100]}...")
    print(f"{'='*60}\n")

    query = state["original_query"]

    # Initialize LLM for classification
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash-lite",
        temperature=0.1
    )

    classification_prompt = f"""
ì‚¬ìš©ì ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ì²˜ë¦¬ ë°©ì‹ì„ ê²°ì •í•˜ì„¸ìš”:

ì‚¬ìš©ì ìš”ì²­: {query}

ë¶„ë¥˜ ê¸°ì¤€:
1. **chat**: ê°„ë‹¨í•œ ì§ˆë¬¸, ì¼ë°˜ì ì¸ ëŒ€í™”, ì›¹/ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ë‹µë³€ ê°€ëŠ¥í•œ ê²½ìš°
   - ì˜ˆ: "ì•ˆë…•í•˜ì„¸ìš”", "ê°ì‚¬í•©ë‹ˆë‹¤", "ê°„ë‹¨í•œ ì„¤ëª… ìš”ì²­"
   - ì˜ˆ: "ìµœê·¼ ~ ì‹œì„¸ ì•Œë ¤ì¤˜", "ìµœê·¼ ì´ìŠˆ Top 10ì´ ë­ì•¼?"
   - ì˜ˆ: "ì´ ë§í¬ ë‚´ìš©ì´ ë­ì•¼?" (ë‹¨ìˆœ í™•ì¸)

2. **task**: ë³µí•©ì ì¸ ë¶„ì„, ë°ì´í„° ìˆ˜ì§‘, ë¦¬í¬íŠ¸ ìƒì„±ì´ í•„ìš”í•œ ê²½ìš°
   - ì˜ˆ: "~ë¥¼ ë¶„ì„í•´ì¤˜", "~ì— ëŒ€í•œ ìë£Œë¥¼ ì°¾ì•„ì¤˜", "ë³´ê³ ì„œ ì‘ì„±"
   - ì˜ˆ: "ìì„¸í•œ ì˜ì–‘ ì •ë³´" (RDB ì¡°íšŒ í•„ìš”)
   - ì˜ˆ: "ì´ ë§í¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒì„¸í•œ ë³´ê³ ì„œ ì‘ì„±í•´ì¤˜"
   - ì˜ˆ: "Graph DB ê²€ìƒ‰ì´ í•„ìš”í•œ ê²½ìš°", "ë…¼ë¬¸ ê²€ìƒ‰ì´ í•„ìš”í•œ ê²½ìš°"

JSONìœ¼ë¡œ ì‘ë‹µ:
{{
    "flow_type": "chat" ë˜ëŠ” "task",
    "reasoning": "ë¶„ë¥˜ ê·¼ê±° ì„¤ëª…"
}}
"""

    try:
        # LLM í˜¸ì¶œ (LangSmithê°€ ìë™ìœ¼ë¡œ ì¶”ì )
        response = await llm.ainvoke(classification_prompt)
        response_content = response.content.strip()

        print(f"ğŸ“ LLM Response: {response_content[:200]}...")

        # Parse JSON response
        classification = None
        try:
            # Direct JSON parsing
            classification = json.loads(response_content)
        except json.JSONDecodeError:
            # Fallback: Extract JSON from markdown code block
            json_match = re.search(r'\{.*\}', response_content, re.DOTALL)
            if json_match:
                classification = json.loads(json_match.group())
            else:
                raise ValueError("Valid JSON not found in response")

        # Validate required fields
        required_fields = ["flow_type", "reasoning"]
        for field in required_fields:
            if field not in classification:
                raise ValueError(f"Missing required field: {field}")

        flow_type = classification["flow_type"]
        reasoning = classification["reasoning"]

        print(f"âœ… Classification Result:")
        print(f"   Flow Type: {flow_type}")
        print(f"   Reasoning: {reasoning}")

        # Update state
        new_state = dict(state)
        new_state["flow_type"] = flow_type

        # Update metadata
        metadata = dict(state.get("metadata", {}))
        metadata["triage_reasoning"] = reasoning
        metadata["classified_at"] = datetime.now().isoformat()
        new_state["metadata"] = metadata

        # Add execution log
        new_state = log_state_transition(
            new_state,
            "triage_node",
            f"Classified as '{flow_type}': {reasoning}"
        )

        return new_state

    except Exception as e:
        print(f"âš ï¸  Classification failed: {e}")
        print(f"   Defaulting to 'task' flow")

        # Fallback to 'task' on error (safer default)
        new_state = dict(state)
        new_state["flow_type"] = "task"

        metadata = dict(state.get("metadata", {}))
        metadata["triage_error"] = str(e)
        metadata["classified_at"] = datetime.now().isoformat()
        new_state["metadata"] = metadata

        new_state = log_state_transition(
            new_state,
            "triage_node",
            f"Classification failed, defaulted to 'task': {str(e)}"
        )

        return new_state


# ============================================================================
# Routing Function
# ============================================================================

def route_after_triage(state: RAGState) -> Literal["chat_flow", "task_flow"]:
    """
    Conditional routing function after triage.

    Routes to appropriate subgraph based on flow_type:
    - "chat" â†’ chat_flow (SimpleAnswerer)
    - "task" â†’ task_flow (Orchestrator)

    Args:
        state: Current RAGState

    Returns:
        "chat_flow" or "task_flow"
    """
    flow_type = state.get("flow_type", "task")

    if flow_type == "chat":
        print(f"ğŸ”€ Routing â†’ chat_flow")
        return "chat_flow"
    else:
        print(f"ğŸ”€ Routing â†’ task_flow")
        return "task_flow"


# ============================================================================
# Abort Check Node
# ============================================================================

async def abort_check_node(state: RAGState, config: RunnableConfig) -> RAGState:
    """
    Check if execution should be aborted.

    This node checks the run_manager for abort requests.
    If abort is requested, it raises an interrupt to stop the workflow.

    Note: This will be integrated with RunManager in the full implementation.
    For now, it's a placeholder that checks metadata.

    Args:
        state: Current RAGState
        config: LangGraph runtime configuration

    Returns:
        Unchanged state if not aborted

    Raises:
        Exception: If abort is requested
    """
    metadata = state.get("metadata", {})
    run_id = metadata.get("run_id")

    # Check if abort flag is set in metadata
    if metadata.get("abort_requested", False):
        print(f"ğŸ›‘ Abort requested for run_id: {run_id}")
        raise Exception(f"Execution aborted by user request: {run_id}")

    return state


# ============================================================================
# Status Update Node
# ============================================================================

async def status_update_node(
    state: RAGState,
    config: RunnableConfig,
    message: str
) -> RAGState:
    """
    Generic status update node.

    Adds a status message to execution log and can emit custom events.

    Args:
        state: Current RAGState
        config: LangGraph runtime configuration
        message: Status message

    Returns:
        Updated state with log entry
    """
    print(f"ğŸ“Š Status: {message}")

    new_state = log_state_transition(state, "status_update", message)

    return new_state
