"""
Chat Flow Nodes for LangGraph

This module implements the chat flow for simple Q&A, porting logic from
SimpleAnswererAgent (conversational_agent.py) into LangGraph nodes.

Node Structure:
    determine_search_node ‚Üí [web_search_node, vector_search_node, scrape_node]
    ‚Üí memory_context_node ‚Üí generate_answer_node
"""

import json
import asyncio
import os
import sys
from typing import List, Dict, Any
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnableConfig
from langchain_core.messages import AIMessage

# API Fallback system import
sys.path.append('/app/utils')
try:
    from api_fallback import api_manager
except ImportError:
    print("‚ö†Ô∏è api_fallback Î™®ÎìàÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏùå, Í∏∞Î≥∏ Î∞©Ïãù ÏÇ¨Ïö©")
    api_manager = None

from ..state import RAGState
from ...models.models import SearchResult
from ....services.search.search_tools import (
    vector_db_search,
    debug_web_search,
    scrape_and_extract_content
)

# Global thread pool for search operations
_global_executor = ThreadPoolExecutor(max_workers=16, thread_name_prefix="chat_search_worker")

# Load persona prompts
PERSONA_PROMPTS = {}
try:
    # Ïò¨Î∞îÎ•∏ Í≤ΩÎ°ú: /app/app/core/workflows/nodes/chat_nodes.pyÏóêÏÑú
    # /app/app/core/agents/prompts/persona_prompts.jsonÎ°ú Í∞ÄÎ†§Î©¥
    # nodes -> workflows -> core Î°ú 3Îã®Í≥Ñ Ïò¨ÎùºÍ∞Ñ ÌõÑ agents/promptsÎ°ú ÏßÑÏûÖ
    current_dir = os.path.dirname(os.path.abspath(__file__))
    file_path = os.path.join(current_dir, "../../agents/prompts", "persona_prompts.json")
    with open(file_path, "r", encoding="utf-8") as f:
        PERSONA_PROMPTS = json.load(f)
    print(f"Chat Nodes: ÌéòÎ•¥ÏÜåÎÇò ÌîÑÎ°¨ÌîÑÌä∏ Î°úÎìú ÏÑ±Í≥µ ({len(PERSONA_PROMPTS)}Í∞ú)")
except Exception as e:
    print(f"‚ùå Chat Nodes: ÌéòÎ•¥ÏÜåÎÇò ÌîÑÎ°¨ÌîÑÌä∏ Î°úÎìú Ïã§Ìå® - {e}")
    print(f"   ÏãúÎèÑÌïú Í≤ΩÎ°ú: {file_path}")
    raise RuntimeError(f"ÌéòÎ•¥ÏÜåÎÇò ÌîÑÎ°¨ÌîÑÌä∏ ÌååÏùºÏùÑ Î°úÎìúÌï† Ïàò ÏóÜÏäµÎãàÎã§: {e}")


# ============================================================================
# Model Initialization with Fallback
# ============================================================================

def _initialize_models(temperature: float = 0.7):
    """
    Initialize LLM models with API fallback support.

    Returns:
        Tuple of (streaming_chat, llm_lite, llm_gemini_backup, llm_openai_mini)
    """
    # Primary models
    streaming_chat = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash-lite",
        temperature=temperature
    )
    llm_lite = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash-lite",
        temperature=temperature
    )

    # Fallback models
    if api_manager:
        try:
            llm_gemini_backup = api_manager.create_langchain_model(
                "gemini-2.5-flash-lite",
                temperature=temperature
            )
            llm_openai_mini = api_manager.create_langchain_model(
                "gpt-4o-mini",
                temperature=temperature
            )
            print(f"Chat Nodes: Fallback Î™®Îç∏ Ï¥àÍ∏∞Ìôî ÏôÑÎ£å (ÏÇ¨Ïö© API: {api_manager.last_successful_api})")
        except Exception as e:
            print(f"Chat Nodes: Fallback Î™®Îç∏ Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
            llm_gemini_backup = None
            llm_openai_mini = None
    else:
        # Legacy fallback
        openai_api_key = os.getenv("OPENAI_API_KEY")
        if openai_api_key:
            llm_openai_mini = ChatOpenAI(
                model="gpt-4o-mini",
                temperature=temperature,
                api_key=openai_api_key
            )
            print("Chat Nodes: OpenAI fallback Î™®Îç∏ Ï¥àÍ∏∞Ìôî ÏôÑÎ£å (Í∏∞Ï°¥ Î∞©Ïãù)")
        else:
            llm_openai_mini = None
        llm_gemini_backup = None
        print("Chat Nodes: Í≤ΩÍ≥†: ÌÜµÌï© API Í¥ÄÎ¶¨Ïûê ÏóÜÏùå, Ï†úÌïúÎêú fallback ÏÇ¨Ïö©")

    return streaming_chat, llm_lite, llm_gemini_backup, llm_openai_mini


# Initialize models at module level
STREAMING_CHAT, LLM_LITE, LLM_GEMINI_BACKUP, LLM_OPENAI_MINI = _initialize_models()


# ============================================================================
# Fallback Utilities
# ============================================================================

async def _invoke_with_fallback(prompt: str, primary_model, fallback_model):
    """
    Invoke LLM with Gemini ÌÇ§ 2Í∞ú -> OpenAI ÏàúÏ∞® fallback.

    Args:
        prompt: LLM prompt
        primary_model: Primary Gemini model
        fallback_model: OpenAI fallback model

    Returns:
        LLM response
    """
    # 1Ï∞®: Primary Gemini (key 1)
    try:
        result = await primary_model.ainvoke(prompt)
        return result
    except Exception as e:
        error_str = str(e).lower()
        rate_limit_indicators = ['429', 'quota', 'rate limit', 'exceeded', 'resource_exhausted']

        if any(indicator in error_str for indicator in rate_limit_indicators):
            print(f"Chat Nodes: Gemini ÌÇ§ 1 rate limit Í∞êÏßÄ: {e}")

            # 2Ï∞®: Gemini backup (key 2)
            if LLM_GEMINI_BACKUP:
                try:
                    print("Chat Nodes: Gemini ÌÇ§ 2Î°ú fallback ÏãúÎèÑ")
                    result = await LLM_GEMINI_BACKUP.ainvoke(prompt)
                    print("Chat Nodes: Gemini ÌÇ§ 2 fallback ÏÑ±Í≥µ")
                    return result
                except Exception as backup_error:
                    print(f"Chat Nodes: Gemini ÌÇ§ 2ÎèÑ Ïã§Ìå®: {backup_error}")

            # 3Ï∞®: OpenAI fallback
            if fallback_model:
                try:
                    print("Chat Nodes: OpenAI fallback ÏãúÎèÑ")
                    result = await fallback_model.ainvoke(prompt)
                    print("Chat Nodes: OpenAI fallback ÏÑ±Í≥µ")
                    return result
                except Exception as openai_error:
                    print(f"Chat Nodes: OpenAI fallbackÎèÑ Ïã§Ìå®: {openai_error}")
                    raise openai_error
            else:
                print("Chat Nodes: OpenAI Î™®Îç∏Ïù¥ Ï¥àÍ∏∞ÌôîÎêòÏßÄ ÏïäÏùå")
                raise e
        else:
            raise e


async def _astream_with_fallback(prompt: str, primary_model, fallback_model):
    """
    Stream LLM with Gemini ÌÇ§ 2Í∞ú -> OpenAI ÏàúÏ∞® fallback.

    Args:
        prompt: LLM prompt
        primary_model: Primary Gemini model
        fallback_model: OpenAI fallback model

    Yields:
        LLM chunks
    """
    # 1Ï∞®: Primary Gemini (key 1)
    try:
        async for chunk in primary_model.astream(prompt):
            yield chunk
        return
    except Exception as e:
        error_str = str(e).lower()
        rate_limit_indicators = ['429', 'quota', 'rate limit', 'exceeded', 'resource_exhausted']

        if any(indicator in error_str for indicator in rate_limit_indicators):
            print(f"Chat Nodes: Gemini ÌÇ§ 1 rate limit Í∞êÏßÄ: {e}")

            # 2Ï∞®: Gemini backup (key 2)
            if LLM_GEMINI_BACKUP:
                try:
                    print("Chat Nodes: Gemini ÌÇ§ 2Î°ú fallback ÏãúÎèÑ")
                    async for chunk in LLM_GEMINI_BACKUP.astream(prompt):
                        yield chunk
                    return
                except Exception as backup_error:
                    print(f"Chat Nodes: Gemini ÌÇ§ 2ÎèÑ Ïã§Ìå®: {backup_error}")

            # 3Ï∞®: OpenAI fallback
            if fallback_model:
                try:
                    print("Chat Nodes: OpenAI fallbackÏúºÎ°ú Ïä§Ìä∏Î¶¨Î∞ç ÏãúÏûë")
                    async for chunk in fallback_model.astream(prompt):
                        yield chunk
                    return
                except Exception as openai_error:
                    print(f"Chat Nodes: OpenAI fallbackÎèÑ Ïã§Ìå®: {openai_error}")
                    raise openai_error
            else:
                print("Chat Nodes: OpenAI Î™®Îç∏Ïù¥ Ï¥àÍ∏∞ÌôîÎêòÏßÄ ÏïäÏùå")
                raise e
        else:
            raise e


# ============================================================================
# Node 1: Determine Search Requirements
# ============================================================================

async def determine_search_node(state: RAGState, config: RunnableConfig) -> RAGState:
    """
    Determine search requirements for the query.

    Ported from SimpleAnswererAgent._needs_search() and _needs_scraping().

    Updates state with:
        - search_flags: Dict with needs_web_search, needs_vector_search, needs_scraping
        - Corresponding query strings and URLs
    """
    print("\n" + "="*60)
    print("üí¨ [Chat Flow] Step 1: Determine Search Requirements")
    print("="*60 + "\n")

    query = state["original_query"]
    current_date = datetime.now().strftime('%YÎÖÑ %mÏõî %dÏùº')

    # Determine web/vector search needs
    search_prompt = f"""
ÎãπÏã†ÏùÄ AI Ïñ¥ÏãúÏä§ÌÑ¥Ìä∏ÏûÖÎãàÎã§. ÏÇ¨Ïö©ÏûêÏùò ÏßàÎ¨∏Ïóê ÎãµÎ≥ÄÌïòÍ∏∞ ÏúÑÌï¥ Í≤ÄÏÉâÏù¥ ÌïÑÏöîÌïúÏßÄ ÌåêÎã®ÌïòÏÑ∏Ïöî.
ÏßàÎ¨∏: {query}
Ïò§Îäò ÎÇ†Ïßú : {current_date}
Web Í≤ÄÏÉâÏù¥ ÌïÑÏöîÌïòÎ©¥ True, ÏïÑÎãàÎ©¥ FalseÎ•º Î∞òÌôòÌïòÏÑ∏Ïöî.
Vector DB Í≤ÄÏÉâÏù¥ ÌïÑÏöîÌïòÎ©¥ True, ÏïÑÎãàÎ©¥ FalseÎ•º Î∞òÌôòÌïòÏÑ∏Ïöî.
- Web Í≤ÄÏÉâÏùÄ ÏµúÍ∑º Ï†ïÎ≥¥, Ïù¥Ïäà, Í∞ÑÎã®Ìïú Ï†ïÎ≥¥Í∞Ä ÌïÑÏöîÌï† Îïå ÏÇ¨Ïö©
- Vector DB Í≤ÄÏÉâÏùÄ ÌäπÏ†ï Îç∞Ïù¥ÌÑ∞, Î¨∏ÏÑú, ÌòÑÌô©, ÌÜµÍ≥Ñ, ÎÇ¥Î∂Ä Ï†ïÎ≥¥Í∞Ä ÌïÑÏöîÌï† Îïå ÏÇ¨Ïö©

Îã§ÏùåÍ≥º Í∞ôÏùÄ ÏàúÏÑú/ÌòïÏãùÏúºÎ°ú ÏùëÎãµÌïòÏÑ∏Ïöî:
{{
    "needs_web_search": false,
    "web_search_query": "Ïõπ Í≤ÄÏÉâ ÏøºÎ¶¨",
    "needs_vector_search": false,
    "vector_search_query": "Î≤°ÌÑ∞ DB Í≤ÄÏÉâ ÏøºÎ¶¨"
}}

Ïõπ Í≤ÄÏÉâ ÏøºÎ¶¨ ÏòàÏãú
- "2025ÎÖÑ ÏµúÏã† Í±¥Í∞ïÍ∏∞Îä•ÏãùÌíà Ìä∏Î†åÎìú"
Î≤°ÌÑ∞ Í≤ÄÏÉâ ÏøºÎ¶¨ ÏòàÏãú
- "2025ÎÖÑ Ïú†ÌñâÌïòÎäî Í±¥Í∞ïÏãùÌíàÏù¥ Î≠êÍ∞Ä ÏûàÎÇòÏöî?"

Ïõπ Í≤ÄÏÉâ ÏøºÎ¶¨Îäî ÌÇ§ÏõåÎìú Í∏∞Î∞ò Î¨∏Ïû•ÏúºÎ°ú
Î≤°ÌÑ∞ Í≤ÄÏÉâ ÏøºÎ¶¨Îäî ÏßàÎ¨∏ÌòïÏãùÏúºÎ°ú ÏûëÏÑ±ÌïòÏÑ∏Ïöî
"""

    try:
        response = await _invoke_with_fallback(search_prompt, LLM_LITE, LLM_OPENAI_MINI)
        response_content = response.content.strip()

        # Parse JSON response
        try:
            clean_response = response_content
            if "```json" in response_content:
                clean_response = response_content.split("```json")[1].split("```")[0].strip()
            elif "```" in response_content:
                clean_response = response_content.split("```")[1].split("```")[0].strip()

            response_json = json.loads(clean_response)
            needs_web_search = response_json.get("needs_web_search", False)
            web_search_query = response_json.get("web_search_query", "")
            needs_vector_search = response_json.get("needs_vector_search", False)
            vector_search_query = response_json.get("vector_search_query", "")

        except json.JSONDecodeError as e:
            print(f"   ‚ö†Ô∏è JSON ÌååÏã± Ïã§Ìå®, fallback ÌååÏã± ÏÇ¨Ïö©: {e}")
            # Fallback: keyword matching
            needs_web_search = "needs_web_search\": true" in response_content or "needs_web_search\":true" in response_content
            needs_vector_search = "needs_vector_search\": true" in response_content or "needs_vector_search\":true" in response_content
            web_search_query = ""
            vector_search_query = ""

    except Exception as e:
        print(f"   ‚ùå Search determination error: {e}")
        needs_web_search = False
        web_search_query = ""
        needs_vector_search = False
        vector_search_query = ""

    # Determine scraping needs
    scraping_prompt = f"""
ÏÇ¨Ïö©ÏûêÏùò ÏßàÎ¨∏ÏùÑ Î∂ÑÏÑùÌïòÏó¨ ÏõπÌéòÏù¥ÏßÄ Ïä§ÌÅ¨ÎûòÌïëÏù¥ ÌïÑÏöîÌïúÏßÄ ÌåêÎã®ÌïòÏÑ∏Ïöî.

ÏßàÎ¨∏: {query}

Îã§Ïùå Í≤ΩÏö∞Ïóê Ïä§ÌÅ¨ÎûòÌïëÏù¥ ÌïÑÏöîÌï©ÎãàÎã§:
1. ÌäπÏ†ï URL/ÎßÅÌÅ¨Ïùò ÎÇ¥Ïö©ÏùÑ Î∂ÑÏÑùÌïòÎùºÍ≥† ÏöîÏ≤≠ÌïòÎäî Í≤ΩÏö∞
2. "Ïù¥ ÎßÅÌÅ¨", "Ìï¥Îãπ ÏÇ¨Ïù¥Ìä∏", "Ïù¥ ÌéòÏù¥ÏßÄ" Îì±Ïùò ÌëúÌòÑÏù¥ ÏûàÎäî Í≤ΩÏö∞
3. URLÏù¥ ÏßÅÏ†ë Ìè¨Ìï®Îêú Í≤ΩÏö∞
4. ÌäπÏ†ï ÏõπÏÇ¨Ïù¥Ìä∏Ïùò ÏÉÅÏÑ∏Ìïú ÎÇ¥Ïö© Î∂ÑÏÑùÏùÑ ÏöîÏ≤≠ÌïòÎäî Í≤ΩÏö∞
5. "Ï†ÑÏ≤¥ ÎÇ¥Ïö©", "ÏÉÅÏÑ∏ Î∂ÑÏÑù", "Î≥¥Í≥†ÏÑú ÏûëÏÑ±" Îì±Ïùò ÌÇ§ÏõåÎìúÍ∞Ä ÏûàÏúºÎ©¥ÏÑú Í≤ÄÏÉâÏùÑ ÏöîÍµ¨ÌïòÎäî Í≤ΩÏö∞

ÏùëÎãµ ÌòïÏãù:
{{
    "needs_scraping": true/false,
    "urls": ["url1", "url2"]  // Î∞úÍ≤¨Îêú URLÎì§, ÏóÜÏúºÎ©¥ Îπà Î∞∞Ïó¥
}}

URL Ìå®ÌÑ¥: http://, https://Î°ú ÏãúÏûëÌïòÎäî Î¨∏ÏûêÏó¥ÏùÑ Ï∞æÏïÑÏ£ºÏÑ∏Ïöî.
"""

    needs_scraping = False
    scraping_urls = []

    try:
        response = await _invoke_with_fallback(scraping_prompt, LLM_LITE, LLM_OPENAI_MINI)
        response_content = response.content.strip()

        # Parse JSON
        try:
            clean_response = response_content
            if "```json" in response_content:
                clean_response = response_content.split("```json")[1].split("```")[0].strip()
            elif "```" in response_content:
                clean_response = response_content.split("```")[1].split("```")[0].strip()

            response_json = json.loads(clean_response)
            needs_scraping = response_json.get("needs_scraping", False)
            scraping_urls = response_json.get("urls", [])

        except json.JSONDecodeError:
            print("   ‚ö†Ô∏è Scraping JSON ÌååÏã± Ïã§Ìå®, ÏßÅÏ†ë URL Ï∂îÏ∂ú ÏãúÎèÑ")

        # Always try direct URL extraction from query
        import re
        url_pattern = r'https?://[^\s]+'
        found_urls = re.findall(url_pattern, query)

        # Domain/path pattern (e.g., parking.airport.kr/reserve/6130_01)
        domain_pattern = r'[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}/[^\s]*'
        domain_urls = re.findall(domain_pattern, query)
        for domain_url in domain_urls:
            if not domain_url.startswith(('http://', 'https://')):
                found_urls.append(f"https://{domain_url}")

        scraping_urls.extend(found_urls)

        # Deduplicate and validate URLs
        valid_urls = []
        for url in set(scraping_urls):
            if url and (url.startswith(('http://', 'https://')) or ('.' in url and '/' in url)):
                if not url.startswith(('http://', 'https://')):
                    url = f"https://{url}"
                valid_urls.append(url)

        scraping_urls = valid_urls
        needs_scraping = len(scraping_urls) > 0

    except Exception as e:
        print(f"   ‚ùå Scraping determination error: {e}")
        needs_scraping = False
        scraping_urls = []

    # Update state
    new_state = dict(state)
    new_state["search_flags"] = {
        "needs_web_search": needs_web_search,
        "web_search_query": web_search_query,
        "needs_vector_search": needs_vector_search,
        "vector_search_query": vector_search_query,
        "needs_scraping": needs_scraping,
        "scraping_urls": scraping_urls
    }

    # Add to execution log
    execution_log = list(state.get("execution_log", []))
    execution_log.append(f"Search requirements determined: web={needs_web_search}, vector={needs_vector_search}, scrape={needs_scraping}")
    new_state["execution_log"] = execution_log

    print(f"   ‚úì Ïõπ Í≤ÄÏÉâ: {needs_web_search} ({web_search_query})")
    print(f"   ‚úì Î≤°ÌÑ∞ Í≤ÄÏÉâ: {needs_vector_search} ({vector_search_query})")
    print(f"   ‚úì Ïä§ÌÅ¨ÎûòÌïë: {needs_scraping} ({len(scraping_urls)}Í∞ú URL)")

    return new_state


# ============================================================================
# Node 2: Web Search
# ============================================================================

async def web_search_node(state: RAGState, config: RunnableConfig) -> RAGState:
    """
    Perform web search if needed.

    Ported from SimpleAnswererAgent._simple_web_search().

    Updates state with:
        - web_results: List of SearchResult dicts
    """
    print("\n" + "="*60)
    print("üåê [Chat Flow] Step 2: Web Search")
    print("="*60 + "\n")

    search_flags = state.get("search_flags", {})
    needs_web_search = search_flags.get("needs_web_search", False)
    web_search_query = search_flags.get("web_search_query", "")

    if not needs_web_search or not web_search_query:
        print("   ‚è≠Ô∏è  Web search not needed, skipping")
        return state

    print(f"   üîç Performing web search: {web_search_query}")

    try:
        # Run web search in executor
        result_text = await asyncio.get_event_loop().run_in_executor(
            None,
            debug_web_search,
            web_search_query
        )

        # Parse results into SearchResult objects
        search_results = []
        if result_text and isinstance(result_text, str):
            lines = result_text.split('\n')
            current_result = {}

            for line in lines:
                line = line.strip()
                if line.startswith(('1.', '2.', '3.', '4.', '5.')):
                    # Save previous result
                    if current_result:
                        search_result = SearchResult(
                            source="web_search",
                            content=current_result.get("snippet", ""),
                            search_query=web_search_query,
                            title=current_result.get("title", "Ïõπ Í≤ÄÏÉâ Í≤∞Í≥º"),
                            url=current_result.get("link"),
                            relevance_score=0.9,
                            timestamp=datetime.now().isoformat(),
                            document_type="web",
                            metadata={"original_query": web_search_query, **current_result},
                            source_url=current_result.get("link", "Ïõπ Í≤ÄÏÉâ Í≤∞Í≥º")
                        )
                        search_results.append(search_result)

                    # Start new result
                    current_result = {"title": line[3:].strip()}
                elif line.startswith("Ï∂úÏ≤ò ÎßÅÌÅ¨:"):
                    current_result["link"] = line[7:].strip()
                elif line.startswith("ÏöîÏïΩ:"):
                    current_result["snippet"] = line[3:].strip()

            # Save last result
            if current_result:
                search_result = SearchResult(
                    source="web_search",
                    content=current_result.get("snippet", ""),
                    search_query=web_search_query,
                    title=current_result.get("title", "Ïõπ Í≤ÄÏÉâ Í≤∞Í≥º"),
                    url=current_result.get("link"),
                    relevance_score=0.9,
                    timestamp=datetime.now().isoformat(),
                    document_type="web",
                    metadata={"original_query": web_search_query, **current_result},
                    source_url=current_result.get("link", "Ïõπ Í≤ÄÏÉâ Í≤∞Í≥º")
                )
                search_results.append(search_result)

        # Take top 3 results
        search_results = search_results[:3]

        # Convert SearchResult objects to dicts for state
        web_results_dicts = [
            {
                "source": r.source,
                "content": r.content,
                "search_query": r.search_query,
                "title": r.title,
                "url": r.url if hasattr(r, 'url') else None,
                "relevance_score": r.relevance_score if hasattr(r, 'relevance_score') else 0.9,
                "score": getattr(r, 'score', 0.9),
                "timestamp": r.timestamp if hasattr(r, 'timestamp') else datetime.now().isoformat(),
                "document_type": r.document_type if hasattr(r, 'document_type') else "web",
                "metadata": r.metadata if hasattr(r, 'metadata') else {},
                "source_url": r.source_url if hasattr(r, 'source_url') else ""
            }
            for r in search_results
        ]

        # Update state (reducer will accumulate)
        new_state = dict(state)
        new_state["web_results"] = web_results_dicts

        # Add to execution log
        execution_log = list(state.get("execution_log", []))
        execution_log.append(f"Web search completed: {len(search_results)} results")
        new_state["execution_log"] = execution_log

        print(f"   ‚úì Web search completed: {len(search_results)} results")

        return new_state

    except Exception as e:
        print(f"   ‚ùå Web search error: {e}")
        # Return state unchanged
        return state


# ============================================================================
# Node 3: Vector Search
# ============================================================================

async def vector_search_node(state: RAGState, config: RunnableConfig) -> RAGState:
    """
    Perform vector DB search if needed.

    Ported from SimpleAnswererAgent._simple_vector_search().

    Updates state with:
        - vector_results: List of SearchResult dicts
    """
    print("\n" + "="*60)
    print("üìö [Chat Flow] Step 3: Vector Search")
    print("="*60 + "\n")

    search_flags = state.get("search_flags", {})
    needs_vector_search = search_flags.get("needs_vector_search", False)
    vector_search_query = search_flags.get("vector_search_query", "")

    if not needs_vector_search or not vector_search_query:
        print("   ‚è≠Ô∏è  Vector search not needed, skipping")
        return state

    print(f"   üîç Performing vector search: {vector_search_query}")

    try:
        # Run vector search in executor
        loop = asyncio.get_event_loop()
        results = await loop.run_in_executor(
            _global_executor,
            vector_db_search,
            vector_search_query
        )

        # Convert to SearchResult objects
        search_results = []
        for result in results[:3]:  # Top 3 results
            if isinstance(result, dict):
                doc_link = result.get("source_url", "")
                page_number = result.get("page_number", [])
                doc_title = result.get("title", "")
                meta_data = result.get("meta_data", {})

                # Add page numbers to title
                full_title = f"{doc_title}, ({', '.join([f'p.{num}' for num in page_number])})".strip()
                score = result.get("score", 5.2)
                chunk_id = result.get("chunk_id", "")

                search_result = SearchResult(
                    source="vector_db",
                    content=result.get("content", ""),
                    search_query=vector_search_query,
                    title=full_title,
                    document_type="database",
                    score=score,
                    metadata=meta_data,
                    url=doc_link,
                    chunk_id=chunk_id,
                )
                search_results.append(search_result)

        # Convert to dicts for state
        vector_results_dicts = [
            {
                "source": r.source,
                "content": r.content,
                "search_query": r.search_query,
                "title": r.title,
                "url": r.url if hasattr(r, 'url') else None,
                "score": r.score if hasattr(r, 'score') else 0.7,
                "relevance_score": getattr(r, 'relevance_score', 0.7),
                "document_type": r.document_type if hasattr(r, 'document_type') else "database",
                "metadata": r.metadata if hasattr(r, 'metadata') else {},
                "chunk_id": r.chunk_id if hasattr(r, 'chunk_id') else ""
            }
            for r in search_results
        ]

        # Update state
        new_state = dict(state)
        new_state["vector_results"] = vector_results_dicts

        # Add to execution log
        execution_log = list(state.get("execution_log", []))
        execution_log.append(f"Vector search completed: {len(search_results)} results")
        new_state["execution_log"] = execution_log

        print(f"   ‚úì Vector search completed: {len(search_results)} results")

        return new_state

    except Exception as e:
        print(f"   ‚ùå Vector search error: {e}")
        return state


# ============================================================================
# Node 4: Scraping
# ============================================================================

async def scrape_node(state: RAGState, config: RunnableConfig) -> RAGState:
    """
    Scrape content from URLs if needed.

    Ported from SimpleAnswererAgent._scrape_content().

    Updates state with:
        - scraped_content: List of SearchResult dicts
    """
    print("\n" + "="*60)
    print("üîó [Chat Flow] Step 4: Web Scraping")
    print("="*60 + "\n")

    search_flags = state.get("search_flags", {})
    needs_scraping = search_flags.get("needs_scraping", False)
    scraping_urls = search_flags.get("scraping_urls", [])

    if not needs_scraping or not scraping_urls:
        print("   ‚è≠Ô∏è  Scraping not needed, skipping")
        return state

    print(f"   üîç Scraping {len(scraping_urls)} URLs")

    query = state["original_query"]
    scraping_results = []

    # Process up to 3 URLs
    for url in scraping_urls[:3]:
        try:
            print(f"   üìÑ Scraping: {url}")

            # Run scraping in executor
            loop = asyncio.get_event_loop()
            content = await loop.run_in_executor(
                _global_executor,
                scrape_and_extract_content,
                json.dumps({"url": url, "query": query})
            )

            # Extract title from URL
            title = url.split("/")[-1] if "/" in url else url
            if title.endswith('.pdf'):
                title = f"PDF: {title}"

            search_result = SearchResult(
                source="scraper",
                content=content,
                search_query=query,
                title=title,
                url=url,
                document_type="web_scraping",
                score=1.0,
                metadata={
                    "scraping_query": query,
                    "original_url": url,
                    "content_length": len(content)
                },
                chunk_id=f"scrape_{hash(url)}"
            )
            scraping_results.append(search_result)

            print(f"      ‚úì Scraped: {len(content)} characters")

        except Exception as e:
            print(f"      ‚ùå Scraping error for {url}: {e}")
            # Add error result
            error_result = SearchResult(
                source="scraper",
                content=f"Ïä§ÌÅ¨ÎûòÌïë Ïã§Ìå®: {str(e)}",
                search_query=query,
                title="Ïä§ÌÅ¨ÎûòÌïë Ïò§Î•ò",
                url=url,
                document_type="error",
                score=0.0,
                metadata={"error": str(e)},
                chunk_id=f"error_{hash(url)}"
            )
            scraping_results.append(error_result)

    # Convert to dicts
    scraped_dicts = [
        {
            "source": r.source,
            "content": r.content,
            "search_query": r.search_query,
            "title": r.title,
            "url": r.url if hasattr(r, 'url') else None,
            "score": r.score if hasattr(r, 'score') else 1.0,
            "document_type": r.document_type if hasattr(r, 'document_type') else "web_scraping",
            "metadata": r.metadata if hasattr(r, 'metadata') else {},
            "chunk_id": r.chunk_id if hasattr(r, 'chunk_id') else ""
        }
        for r in scraping_results
    ]

    # Update state
    new_state = dict(state)
    new_state["scraped_content"] = scraped_dicts

    # Add to execution log
    execution_log = list(state.get("execution_log", []))
    execution_log.append(f"Scraping completed: {len(scraping_results)} results")
    new_state["execution_log"] = execution_log

    print(f"   ‚úì Scraping completed: {len(scraping_results)} results")

    return new_state


# ============================================================================
# Helper Functions for Answer Generation
# ============================================================================

def _build_memory_context(conversation_history: List[dict]) -> str:
    """
    Build memory context from conversation history.

    Ported from SimpleAnswererAgent._build_memory_context().
    """
    if not conversation_history:
        return ""

    memory_parts = []
    extracted_data = {
        "regions": set(),
        "food_items": set(),
        "numbers": [],
        "dates": set(),
        "key_facts": set()
    }

    for msg in conversation_history:
        msg_type = msg.get("type", "")
        content = msg.get("content", "")

        if not content.strip():
            continue

        # User message
        if msg_type == "user":
            memory_parts.append(f"**ÏÇ¨Ïö©Ïûê**: {content}")
        # Assistant message
        elif msg_type == "assistant":
            # Extract key data
            key_data = _extract_key_data_from_content(content)
            extracted_data["regions"].update(key_data["regions"])
            extracted_data["food_items"].update(key_data["food_items"])
            extracted_data["numbers"].extend(key_data["numbers"])
            extracted_data["dates"].update(key_data["dates"])
            extracted_data["key_facts"].update(key_data["key_facts"])

            # Summarize long responses
            if len(content) > 200:
                summary = content[:200] + "..." + content[-100:] if len(content) > 300 else content[:200] + "..."
                memory_parts.append(f"**AI**: {summary}")
            else:
                memory_parts.append(f"**AI**: {content}")

    if memory_parts:
        # Basic conversation context
        context = "### Ïù¥ Ï±ÑÌåÖÎ∞©Ïùò Ïù¥Ï†Ñ ÎåÄÌôî ÎÇ¥Ïö©\n" + "\n\n".join(memory_parts[-4:]) + "\n"

        # Add extracted key data
        if any([extracted_data["regions"], extracted_data["food_items"], extracted_data["key_facts"]]):
            context += "\n### Ïù¥Ï†Ñ ÎåÄÌôîÏóêÏÑú Ïñ∏Í∏âÎêú ÌïµÏã¨ Ï†ïÎ≥¥\n"

            if extracted_data["regions"]:
                context += f"**Ïñ∏Í∏âÎêú ÏßÄÏó≠**: {', '.join(list(extracted_data['regions'])[:10])}\n"

            if extracted_data["food_items"]:
                context += f"**Ïñ∏Í∏âÎêú ÏãùÏû¨Î£å/ÎÜçÏÇ∞Î¨º**: {', '.join(list(extracted_data['food_items'])[:10])}\n"

            if extracted_data["key_facts"]:
                context += f"**ÌïµÏã¨ ÏÇ¨Ïã§**: {', '.join(list(extracted_data['key_facts'])[:5])}\n"

            if extracted_data["dates"]:
                context += f"**Í¥ÄÎ†® Í∏∞Í∞Ñ**: {', '.join(list(extracted_data['dates'])[:5])}\n"

        print(f"   üß† Memory context generated: {len(memory_parts)} messages ‚Üí {len(context)} chars")
        return context

    return ""


def _extract_key_data_from_content(content: str) -> dict:
    """Extract key data from AI response content."""
    import re

    extracted = {
        "regions": [],
        "food_items": [],
        "numbers": [],
        "dates": [],
        "key_facts": []
    }

    # Extract regions
    region_patterns = [
        r'(Í≤ΩÍ∏∞|Ï∂©ÎÇ®|Ï∂©Î∂Å|Ï†ÑÎÇ®|Ï†ÑÎ∂Å|Í≤ΩÎÇ®|Í≤ΩÎ∂Å|Í∞ïÏõê|Ï†úÏ£º)\s*([Í∞Ä-Ìû£]+[ÏãúÍµ∞Íµ¨]?)',
        r'([Í∞Ä-Ìû£]+[ÏãúÍµ∞Íµ¨])',
        r'([Í∞Ä-Ìû£]+Íµ∞|[Í∞Ä-Ìû£]+Ïãú)'
    ]
    for pattern in region_patterns:
        matches = re.findall(pattern, content)
        for match in matches:
            if isinstance(match, tuple):
                region = ' '.join(match).strip()
            else:
                region = match.strip()
            if region and len(region) > 1 and region not in extracted["regions"]:
                extracted["regions"].append(region)

    # Extract food items
    food_keywords = ["Ìè¨ÎèÑ", "Î∞∞", "ÏÇ¨Í≥º", "ÏåÄ", "Ï±ÑÏÜå", "Í≥ºÏùº", "ÎÜçÏÇ∞Î¨º", "Ï∂ïÏÇ∞Î¨º", "ÏàòÏÇ∞Î¨º", "Í≥°Î¨º", "Îã≠Í≥†Í∏∞", "ÎèºÏßÄÍ≥†Í∏∞", "ÏÜåÍ≥†Í∏∞"]
    for keyword in food_keywords:
        if keyword in content and keyword not in extracted["food_items"]:
            extracted["food_items"].append(keyword)

    # Extract numbers
    number_patterns = [
        r'(\d+(?:\.\d+)?)\s*%',
        r'(\d+(?:,\d+)*)\s*Ïñµ',
        r'(\d+(?:,\d+)*)\s*Îßå',
        r'(\d+(?:\.\d+)?)\s*ÌÜ§',
        r'(\d+(?:,\d+)*)\s*Ïõê'
    ]
    for pattern in number_patterns:
        matches = re.findall(pattern, content)
        for match in matches:
            if match not in extracted["numbers"]:
                extracted["numbers"].append(match)

    # Extract dates
    date_patterns = [
        r'20\d{2}ÎÖÑ\s*\d+Ïõî',
        r'\d+Ïõî\s*\d+Ïùº',
        r'20\d{2}ÎÖÑ'
    ]
    for pattern in date_patterns:
        matches = re.findall(pattern, content)
        for match in matches:
            if match not in extracted["dates"]:
                extracted["dates"].append(match)

    # Extract key facts
    key_fact_patterns = [
        r'(ÌäπÎ≥ÑÏû¨ÎÇúÏßÄÏó≠)',
        r'(ÏßëÏ§ëÌò∏Ïö∞\s*ÌîºÌï¥)',
        r'(ÏÉùÏÇ∞Îüâ\s*[Ï¶ùÍ∞ÄÍ∞êÏÜå])',
        r'(Í∞ÄÍ≤©\s*[ÏÉÅÏäπÌïòÎùΩ])'
    ]
    for pattern in key_fact_patterns:
        matches = re.findall(pattern, content)
        for match in matches:
            if match not in extracted["key_facts"]:
                extracted["key_facts"].append(match)

    return extracted


def _generate_memory_summary(conversation_history: List[dict], current_query: str) -> str:
    """Generate memory summary guidance."""
    if not conversation_history:
        return "ÏÉàÎ°úÏö¥ ÎåÄÌôîÎ•º ÏãúÏûëÌï©ÎãàÎã§."

    # Check for continuation keywords
    continuation_keywords = [
        "Í∑∏", "Í∑∏Í≤É", "Í∑∏Í±∞", "ÏúÑ", "ÏïûÏÑú", "Ïù¥Ï†Ñ", "Î∞©Í∏à", "ÏïÑÍπå", "Ï†ÄÍ∏∞", "Í±∞Í∏∞",
        "Í∑∏ Ï§ë", "Í∑∏Ï§ë", "Í∑∏Îü∞Îç∞", "Í∑∏Îüº", "Í∑∏ÎûòÏÑú", "Îî∞ÎùºÏÑú", "Ïù¥Ïñ¥ÏÑú", "Í≥ÑÏÜçÌï¥ÏÑú",
        "Ï∂îÍ∞ÄÎ°ú", "Îçî", "ÎòêÌïú", "Í∑∏Î¶¨Í≥†", "Îòê", "ÌïúÌé∏", "Î∞òÎ©¥", "ÎåÄÏã†"
    ]

    has_continuation = any(keyword in current_query for keyword in continuation_keywords)

    if has_continuation and len(conversation_history) >= 2:
        # Find recent user question and AI answer
        recent_user = None
        recent_ai = None

        for msg in reversed(conversation_history):
            if msg.get("type") == "user" and not recent_user:
                recent_user = msg.get("content", "")
            elif msg.get("type") == "assistant" and not recent_ai and recent_user:
                recent_ai = msg.get("content", "")
                break

        if recent_user and recent_ai:
            ai_summary = recent_ai[:100] + "..." if len(recent_ai) > 100 else recent_ai

            return f"""Ïù¥Ï†Ñ ÎåÄÌôî Îß•ÎùΩÏùÑ Í≥†Î†§ÌïòÏó¨ ÎãµÎ≥ÄÌïòÏÑ∏Ïöî.
ÎãµÎ≥Ä ÏãúÏûë Ïãú Îã§Ïùå ÌòïÏãùÏúºÎ°ú Ïù¥Ï†Ñ ÎåÄÌôîÎ•º Í∞ÑÎã®Ìûà ÏöîÏïΩÌï¥Ï£ºÏÑ∏Ïöî:
"Ïù¥Ï†ÑÏóê Î¨∏ÏùòÌïòÏã† '{recent_user[:50]}{'...' if len(recent_user) > 50 else ''}'Ïóê ÎåÄÌï¥ {ai_summary}ÎùºÍ≥† ÎãµÎ≥ÄÎìúÎ†∏ÎäîÎç∞, Ïù¥Î•º Î∞îÌÉïÏúºÎ°ú ÎßêÏîÄÎìúÎ¶¨Í≤†ÏäµÎãàÎã§."
Í∑∏ Îã§Ïùå Î≥∏Í≤©Ï†ÅÏù∏ ÎãµÎ≥ÄÏùÑ Ïù¥Ïñ¥ÏÑú Ìï¥Ï£ºÏÑ∏Ïöî."""

    return "Ïù¥Ï†Ñ ÎåÄÌôî ÎÇ¥Ïö©ÏùÑ Ï∞∏Í≥†ÌïòÏó¨ ÎãµÎ≥ÄÌïòÏÑ∏Ïöî."


def _create_enhanced_prompt_with_memory(
    query: str,
    all_search_results: List[Dict],
    state: RAGState
) -> str:
    """
    Create enhanced prompt with persona, memory, and search results.

    Ported from SimpleAnswererAgent._create_enhanced_prompt_with_memory().
    """
    current_date_str = datetime.now().strftime("%YÎÖÑ %mÏõî %dÏùº")

    # Get persona
    persona_name = state.get("persona", "Í∏∞Î≥∏")
    persona_instruction = PERSONA_PROMPTS.get(persona_name, {}).get(
        "prompt",
        "ÎãπÏã†ÏùÄ ÏπúÏ†àÌïòÍ≥† ÎèÑÏõÄÏù¥ ÎêòÎäî AI Ïñ¥ÏãúÏä§ÌÑ¥Ìä∏ÏûÖÎãàÎã§."
    )

    # Get memory context
    memory_context = state.get("memory_context", "")
    memory_info = f"\n{memory_context}\n" if memory_context else ""

    # Create search results summary
    context_summary = ""
    if all_search_results:
        summary_parts = []
        for i, result in enumerate(all_search_results[:3]):
            content = result.get("content", "")
            title = result.get("title", "ÏûêÎ£å")

            # URL info
            url_info = ""
            if result.get("url"):
                url_info = f"\n  **Ï∂úÏ≤ò ÎßÅÌÅ¨**: {result['url']}"
            elif result.get("source_url") and not result["source_url"].startswith(('Ïõπ Í≤ÄÏÉâ', 'Vector DB')):
                url_info = f"\n  **Ï∂úÏ≤ò ÎßÅÌÅ¨**: {result['source_url']}"

            summary_parts.append(f"**[Ï∞∏Í≥†ÏûêÎ£å {i}]** **{title}**: {content[:200]}...{url_info}")
        context_summary = "\n\n".join(summary_parts)

    # Memory summary
    memory_summary = ""
    conversation_history = state.get("metadata", {}).get("conversation_history", [])
    if memory_context and conversation_history:
        memory_summary = _generate_memory_summary(conversation_history, query)

    return f"""{persona_instruction}

ÏúÑÏùò ÎãπÏã†Ïùò Ïó≠Ìï†Í≥º ÏõêÏπôÏùÑ Î∞òÎìúÏãú ÏßÄÌÇ§Î©¥ÏÑú ÎãµÎ≥ÄÌï¥Ï£ºÏÑ∏Ïöî.

ÌòÑÏû¨ ÎÇ†Ïßú: {current_date_str}

{memory_info}

## Ï∞∏Í≥† Ï†ïÎ≥¥
{context_summary if context_summary else "Ï∂îÍ∞Ä Ï∞∏Í≥† Ï†ïÎ≥¥ ÏóÜÏùå"}

## ÏÇ¨Ïö©Ïûê ÏßàÎ¨∏
{query}

## ÏùëÎãµ Í∞ÄÏù¥Îìú
- **Î©îÎ™®Î¶¨ Í∏∞Î∞ò ÎãµÎ≥Ä**: {memory_summary}
- **ÌéòÎ•¥ÏÜåÎÇò Ïú†ÏßÄ**: ÎãπÏã†Ïùò Ïó≠Ìï†Ïóê ÎßûÎäî ÎßêÌà¨ÏôÄ Í¥ÄÏ†êÏùÑ ÏùºÍ¥ÄÎêòÍ≤å Ïú†ÏßÄÌïòÏÑ∏Ïöî.
- ÏûêÏó∞Ïä§ÎüΩÍ≥† ÏπúÍ∑ºÌïú ÌÜ§ÏúºÎ°ú ÎãµÎ≥Ä
- Ï∞∏Í≥† Ï†ïÎ≥¥Í∞Ä ÏûàÏúºÎ©¥ Ïù¥Î•º ÌôúÏö©ÌïòÎêò, Ï†ïÌôïÌïú Ï†ïÎ≥¥Îßå ÏÇ¨Ïö©
- Î∂àÌôïÏã§Ìïú ÎÇ¥Ïö©ÏùÄ Î™ÖÏãúÏ†ÅÏúºÎ°ú ÌëúÌòÑ
- Í∞ÑÍ≤∞ÌïòÎ©¥ÏÑúÎèÑ ÎèÑÏõÄÏù¥ ÎêòÎäî ÎãµÎ≥Ä Ï†úÍ≥µ
- ÌïÑÏöîÏãú Ï∂îÍ∞Ä ÏßàÎ¨∏ÏùÑ Í∂åÏú†
- ÎßàÌÅ¨Îã§Ïö¥ ÌòïÏãùÏúºÎ°ú ÎãµÎ≥Ä ÏûëÏÑ±
- ÎßàÌÅ¨Îã§Ïö¥Ïùò '-', '*', '+', '##', '###' Îì±ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Í∞ÄÎèÖÏÑ± Ï¢ãÏùÄ ÎãµÎ≥Ä ÏûëÏÑ±
- **Ï§ëÏöî**: Ï∞∏Í≥† Ï†ïÎ≥¥Î•º ÏÇ¨Ïö©Ìï† ÎïåÎäî Îã§Ïùå ÌòïÏãùÏúºÎ°ú Ï∂úÏ≤òÎ•º ÌëúÍ∏∞ÌïòÏÑ∏Ïöî:
  * Î¨∏Ïû• ÎÅùÏóê [SOURCE:Ïà´Ïûê1, Ïà´Ïûê2, Ïà´Ïûê3, ...] ÌòïÏãùÏúºÎ°ú Ï∂úÏ≤ò Î≤àÌò∏Î•º ÌëúÍ∏∞ (Ïà´ÏûêÎßå ÏÇ¨Ïö©, "Îç∞Ïù¥ÌÑ∞"ÎÇò "Î¨∏ÏÑú" Îì±Ïùò Îã®Ïñ¥ ÏÇ¨Ïö© Í∏àÏßÄ)
  * ÏòàÏãú: "Í±¥Í∞ïÍ∏∞Îä•ÏãùÌíà ÏãúÏû• Í∑úÎ™®Îäî 6Ï°∞ 440Ïñµ ÏõêÏûÖÎãàÎã§ [SOURCE:0]"
  * ÏòàÏãú: "Í≤ΩÏüÅÏÇ¨Ïùò Í≤ΩÏö∞ Î∞îÏù¥Îü¥ÏùÑ ÌÜµÌïú ÎßàÏºÄÌåÖ Ï†ÑÎûµÏùÑ ÏÇ¨Ïö©Ìï©ÎãàÎã§ [SOURCE:1]"
  * ÏûòÎ™ªÎêú ÏòàÏãú: [SOURCE:Îç∞Ïù¥ÌÑ∞ 1], [SOURCE:Î¨∏ÏÑú 1] (Ïù¥Îü∞ ÌòïÏãù ÏÇ¨Ïö© Í∏àÏßÄ)
  * Ï∞∏Í≥† Ï†ïÎ≥¥Ïùò Ïù∏Îç±Ïä§ ÏàúÏÑúÎåÄÎ°ú 0, 1, 2... Î≤àÌò∏Î•º ÏÇ¨Ïö©ÌïòÏÑ∏Ïöî

ÎãµÎ≥Ä:"""


# ============================================================================
# Node 5: Memory Context
# ============================================================================

async def memory_context_node(state: RAGState, config: RunnableConfig) -> RAGState:
    """
    Build memory context from conversation history.

    Updates state with:
        - memory_context: String with formatted conversation context
    """
    print("\n" + "="*60)
    print("üß† [Chat Flow] Step 5: Memory Context")
    print("="*60 + "\n")

    conversation_history = state.get("metadata", {}).get("conversation_history", [])
    conversation_id = state.get("conversation_id", "unknown")

    memory_context = _build_memory_context(conversation_history)

    if memory_context:
        print(f"   ‚úì Memory context built: {len(conversation_history)} messages, {len(memory_context)} chars")
    else:
        print(f"   ‚ÑπÔ∏è  No memory context (new conversation)")

    # Update state
    new_state = dict(state)
    new_state["memory_context"] = memory_context

    # Add to execution log
    execution_log = list(state.get("execution_log", []))
    execution_log.append(f"Memory context built: {len(conversation_history)} messages")
    new_state["execution_log"] = execution_log

    return new_state


# ============================================================================
# Node 6: Generate Answer (Streaming)
# ============================================================================

async def generate_answer_node(state: RAGState, config: RunnableConfig) -> RAGState:
    """
    Generate final answer with streaming LLM.

    Ported from SimpleAnswererAgent.answer_streaming().

    Updates state with:
        - final_answer: Generated answer string
        - sources: List of source data for frontend
        - messages: AIMessage with generated content
    """
    print("\n" + "="*60)
    print("‚ú® [Chat Flow] Step 6: Generate Answer")
    print("="*60 + "\n")

    query = state["original_query"]

    # Validate and set persona
    selected_persona = state.get("persona", "Í∏∞Î≥∏")
    if selected_persona not in PERSONA_PROMPTS:
        print(f"   ‚ö†Ô∏è  Unknown persona '{selected_persona}', using 'Í∏∞Î≥∏'")
        selected_persona = "Í∏∞Î≥∏"

    print(f"   üé≠ Using persona: '{selected_persona}'")

    # Gather all search results
    web_results = state.get("web_results", [])
    vector_results = state.get("vector_results", [])
    scraped_content = state.get("scraped_content", [])

    all_search_results = []
    all_search_results.extend(web_results)
    all_search_results.extend(vector_results)
    all_search_results.extend(scraped_content)

    print(f"   üìö Total search results: {len(all_search_results)} (web={len(web_results)}, vector={len(vector_results)}, scraped={len(scraped_content)})")

    # Create prompt
    prompt = _create_enhanced_prompt_with_memory(query, all_search_results, state)

    # Generate answer with streaming
    full_response = ""

    try:
        chunk_count = 0
        content_generated = False

        async for chunk in _astream_with_fallback(prompt, STREAMING_CHAT, LLM_OPENAI_MINI):
            chunk_count += 1
            if hasattr(chunk, 'content') and chunk.content:
                content_generated = True
                full_response += chunk.content
                print(f"   üìù Chunk {chunk_count}: {len(chunk.content)} chars", end='\r')

        print(f"\n   ‚úì Answer generated: {chunk_count} chunks, {len(full_response)} chars")

        # Fallback if no content generated
        if not content_generated or not full_response.strip():
            print("   ‚ö†Ô∏è  No content generated, using fallback response")
            full_response = f"""Ï£ÑÏÜ°Ìï©ÎãàÎã§. ÌòÑÏû¨ ÏãúÏä§ÌÖúÏóê ÏùºÏãúÏ†ÅÏù∏ Î¨∏Ï†úÍ∞Ä ÏûàÏñ¥ ÎãµÎ≥ÄÏùÑ ÏÉùÏÑ±Ìï† Ïàò ÏóÜÏäµÎãàÎã§.

**ÏÇ¨Ïö©Ïûê ÏßàÎ¨∏**: {query}

Îã§Ïãú ÏãúÎèÑÌï¥ Ï£ºÏãúÍ±∞ÎÇò, Ïû†Ïãú ÌõÑÏóê Îã§Ïãú Î¨∏ÏùòÌï¥ Ï£ºÏÑ∏Ïöî."""

    except Exception as e:
        print(f"   ‚ùå LLM error: {e}")
        full_response = f"""Ï£ÑÏÜ°Ìï©ÎãàÎã§. ÌòÑÏû¨ ÏãúÏä§ÌÖúÏóê ÏùºÏãúÏ†ÅÏù∏ Î¨∏Ï†úÍ∞Ä ÏûàÏñ¥ ÎãµÎ≥ÄÏùÑ ÏÉùÏÑ±Ìï† Ïàò ÏóÜÏäµÎãàÎã§.

**ÏÇ¨Ïö©Ïûê ÏßàÎ¨∏**: {query}

Îã§Ïãú ÏãúÎèÑÌï¥ Ï£ºÏãúÍ±∞ÎÇò, Ïû†Ïãú ÌõÑÏóê Îã§Ïãú Î¨∏ÏùòÌï¥ Ï£ºÏÑ∏Ïöî."""

    # Update state
    new_state = dict(state)
    new_state["final_answer"] = full_response

    # Add AIMessage to messages
    messages = list(state.get("messages", []))
    messages.append(AIMessage(content=full_response))
    new_state["messages"] = messages

    # Prepare sources for frontend
    if all_search_results:
        sources_data = []
        full_data_dict = {}

        for idx, result in enumerate(all_search_results[:10]):  # Max 10 sources
            source_data = {
                "id": idx + 1,
                "title": result.get("title", "ÏûêÎ£å"),
                "content": result.get("content", "")[:300] + "..." if len(result.get("content", "")) > 300 else result.get("content", ""),
                "url": result.get("url"),
                "source_url": result.get("source_url"),
                "source_type": result.get("source", "unknown")
            }
            sources_data.append(source_data)

            # full_data_dict (0-indexed)
            full_data_dict[idx] = {
                "title": result.get("title", "ÏûêÎ£å"),
                "content": result.get("content", ""),
                "source": result.get("source", "unknown"),
                "url": result.get("url", ""),
                "source_url": result.get("source_url", ""),
                "score": result.get("score", result.get("relevance_score", 0.0)),
                "document_type": result.get("document_type", "unknown")
            }

        # Add sources to metadata
        metadata = dict(state.get("metadata", {}))
        metadata["sources"] = sources_data
        metadata["full_data_dict"] = full_data_dict
        metadata["simple_answer_completed"] = True
        new_state["metadata"] = metadata

        # Add sources to state.sources (for LangGraph accumulation)
        new_state["sources"] = [
            {
                "title": result.get("title", "ÏûêÎ£å"),
                "content": result.get("content", ""),
                "url": result.get("url"),
                "source": result.get("source", "unknown"),
                "score": result.get("score", result.get("relevance_score", 0.0))
            }
            for result in all_search_results[:10]
        ]

        print(f"   üìë Sources prepared: {len(sources_data)} items")

    # Add to execution log
    execution_log = list(state.get("execution_log", []))
    execution_log.append(f"Answer generated: {len(full_response)} chars")
    new_state["execution_log"] = execution_log

    return new_state
