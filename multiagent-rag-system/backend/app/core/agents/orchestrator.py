import json
import sys
import asyncio
from typing import Dict, List, Any, Optional, Tuple, AsyncGenerator
from datetime import datetime
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import ChatOpenAI
import re
import os
import aiofiles

from ..models.models import StreamingAgentState, SearchResult
from .worker_agents import DataGathererAgent, ProcessorAgent
from sentence_transformers import SentenceTransformer
from ...utils.session_logger import get_session_logger

# --- í˜ë¥´ì†Œë‚˜ í”„ë¡¬í”„íŠ¸ ë¡œë“œ ---
PERSONA_PROMPTS = {}
try:
    # í˜„ì¬ íŒŒì¼(orchestrator.py)ì˜ ë””ë ‰í† ë¦¬ ê²½ë¡œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    current_dir = os.path.dirname(__file__)
    # JSON íŒŒì¼ì˜ ì ˆëŒ€ ê²½ë¡œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    file_path = os.path.join(current_dir, "prompts", "persona_prompts.json")

    with open(file_path, "r", encoding="utf-8") as f:
        PERSONA_PROMPTS = json.load(f)
    print(f"OrchestratorAgent: í˜ë¥´ì†Œë‚˜ í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì„±ê³µ. (ê²½ë¡œ: {file_path})")
except FileNotFoundError:
    print(f"ê²½ê³ : ë‹¤ìŒ ê²½ë¡œì—ì„œ persona_prompts.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
except json.JSONDecodeError:
    print(f"ê²½ê³ : {file_path} íŒŒì¼ íŒŒì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. JSON í˜•ì‹ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
# -----------------------------

class TriageAgent:
    """ìš”ì²­ ë¶„ë¥˜ ë° ë¼ìš°íŒ… ë‹´ë‹¹ Agent"""

    def __init__(self, model: str = "gemini-2.5-flash-lite", temperature: float = 0.1):
        self.llm = ChatGoogleGenerativeAI(model=model, temperature=temperature)

    async def classify_request(self, query: str, state: StreamingAgentState) -> StreamingAgentState:
        """ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ flow_type ê²°ì •"""
        session_id = state.get("session_id", "unknown")
        logger = get_session_logger(session_id, "TriageAgent")
        logger.info(f"ìš”ì²­ ë¶„ë¥˜ ì‹œì‘ - '{query}'")

        classification_prompt = f"""
ì‚¬ìš©ì ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ì²˜ë¦¬ ë°©ì‹ì„ ê²°ì •í•˜ì„¸ìš”:

ì‚¬ìš©ì ìš”ì²­: {query}

ë¶„ë¥˜ ê¸°ì¤€:
1. **chat**: ê°„ë‹¨í•œ ì§ˆë¬¸, ì¼ë°˜ì ì¸ ëŒ€í™”, ë‹µë‹´, ê·¸ë¦¬ê³  ê°„ë‹¨í•œ Web Searchë‚˜, ë‚´ë¶€ ì •ë³´ë¥¼ ì¡°íšŒí•˜ì—¬ ë‹µë³€í•  ìˆ˜ ìˆëŠ” ê²½ìš°
   - ì˜ˆ: "ì•ˆë…•í•˜ì„¸ìš”", "ê°ì‚¬í•©ë‹ˆë‹¤", "ê°„ë‹¨í•œ ì„¤ëª… ìš”ì²­", "ìµœê·¼ ~ ì‹œì„¸ ì•Œë ¤ì¤˜", "ìµœê·¼ ì´ìŠˆ Top 10ì´ ë­ì•¼?"

2. **task**: ë³µí•©ì ì¸ ë¶„ì„, ë°ì´í„° ìˆ˜ì§‘, ë¦¬í¬íŠ¸ ìƒì„±ì´ í•„ìš”, ì •í™•íˆëŠ” ì—¬ëŸ¬ ì„¹ì…˜ì— ê±¸ì¹œ ë³´ê³ ì„œ ìƒì„±ì´ í•„ìš”í•œ ì§ˆë¬¸ì¼ ê²½ìš° ë˜ëŠ”, ìì„¸í•œ ì˜ì–‘ ì •ë³´ì™€ ê°™ì€ RDBë¥¼ ì¡°íšŒ í•´ì•¼í•˜ëŠ” ì§ˆë¬¸ì¼ ê²½ìš°
   - ì˜ˆ: "~ë¥¼ ë¶„ì„í•´ì¤˜", "~ì— ëŒ€í•œ ìë£Œë¥¼ ì°¾ì•„ì¤˜", "ë³´ê³ ì„œ ì‘ì„±"

JSONìœ¼ë¡œ ì‘ë‹µ:
{{
    "flow_type": "chat" ë˜ëŠ” "task",
    "reasoning": "ë¶„ë¥˜ ê·¼ê±° ì„¤ëª…",
}}
"""

        try:
            response = await self.llm.ainvoke(classification_prompt)
            response_content = response.content.strip()

            # JSON ì‘ë‹µ ì¶”ì¶œ ì‹œë„
            classification = None
            try:
                # ì§ì ‘ íŒŒì‹± ì‹œë„
                classification = json.loads(response_content)
            except json.JSONDecodeError:
                # JSON ë¸”ë¡ ì°¾ê¸° ì‹œë„
                import re
                json_match = re.search(r'\{.*\}', response_content, re.DOTALL)
                if json_match:
                    classification = json.loads(json_match.group())
                else:
                    raise ValueError("Valid JSON not found in response")

            # í•„ìˆ˜ í•„ë“œ í™•ì¸
            required_fields = ["flow_type", "reasoning"]
            for field in required_fields:
                if field not in classification:
                    raise ValueError(f"Missing required field: {field}")

            # state ì—…ë°ì´íŠ¸ (ë”•ì…”ë„ˆë¦¬ ì ‘ê·¼ ë°©ì‹)
            state["flow_type"] = classification["flow_type"]
            state["metadata"].update({
                "triage_reasoning": classification["reasoning"],
                "classified_at": datetime.now().isoformat()
            })

            logger.info(f"ë¶„ë¥˜ ê²°ê³¼: {classification['flow_type']}")
            logger.info(f"ê·¼ê±°: {classification['reasoning']}")

        except Exception as e:
            logger.error(f"ë¶„ë¥˜ ì‹¤íŒ¨, ê¸°ë³¸ê°’(task) ì ìš©: {e}")
            state["flow_type"] = "task"  # ê¸°ë³¸ê°’
            state["metadata"].update({
                "triage_error": str(e),
                "classified_at": datetime.now().isoformat()
            })

        return state


class OrchestratorAgent:
    """ê³ ì„±ëŠ¥ ë¹„ë™ê¸° ìŠ¤ì¼€ì¤„ëŸ¬ ë° ì§€ëŠ¥í˜• ê³„íš ìˆ˜ë¦½ Agent"""

    def __init__(self, model: str = "gemini-2.5-flash", temperature: float = 0.2):
        self.llm = ChatGoogleGenerativeAI(model=model, temperature=temperature)
        self.llm_openai_mini = ChatOpenAI(model="gpt-4o-mini", temperature=temperature)
        self.data_gatherer = DataGathererAgent()
        self.processor = ProcessorAgent()
        self.personas = PERSONA_PROMPTS

    def _build_memory_context(self, conversation_history: List[dict]) -> str:
        """í˜„ì¬ ì±„íŒ…ë°©ì˜ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (OrchestratorAgentìš©)"""
        if not conversation_history:
            return ""

        memory_parts = []

        for msg in conversation_history:
            msg_type = msg.get("type", "")
            content = msg.get("content", "")

            if not content.strip():
                continue

            # ì‚¬ìš©ì ë©”ì‹œì§€
            if msg_type == "user":
                memory_parts.append(f"**ì‚¬ìš©ì**: {content}")
            # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ (ìš”ì•½)
            elif msg_type == "assistant":
                # ê¸´ ë‹µë³€ì€ ìš”ì•½ (ë³´ê³ ì„œëŠ” ë” ì§§ê²Œ)
                if len(content) > 150:
                    summary = content[:150] + "..."
                    memory_parts.append(f"**AI**: {summary}")
                else:
                    memory_parts.append(f"**AI**: {content}")

        if memory_parts:
            context = "### ì´ ì±„íŒ…ë°©ì˜ ì´ì „ ëŒ€í™” ë‚´ìš©\n" + "\n\n".join(memory_parts[-3:]) + "\n"  # ë³´ê³ ì„œìš©ì€ 3ê°œë§Œ
            print(f"ğŸ§  OrchestratorAgent ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ ìƒì„±: {len(memory_parts)}ê°œ ë©”ì‹œì§€ â†’ {len(context)}ì")
            return context

        return ""

    def _extract_key_data_from_content(self, content: str) -> dict:
        """AI ë‹µë³€ì—ì„œ í•µì‹¬ ë°ì´í„°ë¥¼ ì¶”ì¶œ"""
        import re

        extracted = {
            "regions": [],
            "food_items": [],
            "numbers": [],
            "dates": [],
            "key_facts": []
        }

        # ì§€ì—­ëª… ì¶”ì¶œ (ì˜ˆ: ê²½ê¸° ê°€í‰, ì¶©ë‚¨ ì„œì‚°, ê²½ë‚¨ ì‚°ì²­ ë“±)
        region_patterns = [
            r'(ê²½ê¸°|ì¶©ë‚¨|ì¶©ë¶|ì „ë‚¨|ì „ë¶|ê²½ë‚¨|ê²½ë¶|ê°•ì›|ì œì£¼)\s*([ê°€-í£]+[ì‹œêµ°êµ¬]?)',
            r'([ê°€-í£]+[ì‹œêµ°êµ¬])',
            r'([ê°€-í£]+êµ°|[ê°€-í£]+ì‹œ)'
        ]
        for pattern in region_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                if isinstance(match, tuple):
                    region = ' '.join(match).strip()
                else:
                    region = match.strip()
                if region and len(region) > 1 and region not in extracted["regions"]:
                    extracted["regions"].append(region)

        # ì‹ì¬ë£Œ/ë†ì‚°ë¬¼ ì¶”ì¶œ
        food_keywords = ["í¬ë„", "ë°°", "ì‚¬ê³¼", "ìŒ€", "ì±„ì†Œ", "ê³¼ì¼", "ë†ì‚°ë¬¼", "ì¶•ì‚°ë¬¼", "ìˆ˜ì‚°ë¬¼", "ê³¡ë¬¼", "ë‹­ê³ ê¸°", "ë¼ì§€ê³ ê¸°", "ì†Œê³ ê¸°"]
        for keyword in food_keywords:
            if keyword in content and keyword not in extracted["food_items"]:
                extracted["food_items"].append(keyword)

        # ìˆ˜ì¹˜ ì •ë³´ ì¶”ì¶œ (í¼ì„¼íŠ¸, ì–µì›, í†¤ ë“±)
        number_patterns = [
            r'(\d+(?:\.\d+)?)\s*%',
            r'(\d+(?:,\d+)*)\s*ì–µ',
            r'(\d+(?:,\d+)*)\s*ë§Œ',
            r'(\d+(?:\.\d+)?)\s*í†¤',
            r'(\d+(?:,\d+)*)\s*ì›'
        ]
        for pattern in number_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                if match not in extracted["numbers"]:
                    extracted["numbers"].append(match)

        # ë‚ ì§œ/ê¸°ê°„ ì¶”ì¶œ
        date_patterns = [
            r'20\d{2}ë…„\s*\d+ì›”',
            r'\d+ì›”\s*\d+ì¼',
            r'20\d{2}ë…„'
        ]
        for pattern in date_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                if match not in extracted["dates"]:
                    extracted["dates"].append(match)

        # íŠ¹ë³„ì¬ë‚œì§€ì—­, í”¼í•´ì§€ì—­ ë“± í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        key_fact_patterns = [
            r'(íŠ¹ë³„ì¬ë‚œì§€ì—­)',
            r'(ì§‘ì¤‘í˜¸ìš°\s*í”¼í•´)',
            r'(ìƒì‚°ëŸ‰\s*[ì¦ê°€ê°ì†Œ])',
            r'(ê°€ê²©\s*[ìƒìŠ¹í•˜ë½])'
        ]
        for pattern in key_fact_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                if match not in extracted["key_facts"]:
                    extracted["key_facts"].append(match)

        return extracted

    def _generate_memory_summary_for_report(self, conversation_history: List[dict], current_query: str) -> str:
        """ë³´ê³ ì„œ ìƒì„±ìš© ë©”ëª¨ë¦¬ ìš”ì•½ ìƒì„±"""
        if not conversation_history:
            return ""

        # ì—°ì†ì„± í‚¤ì›Œë“œ í™•ì¸
        continuation_keywords = [
            "ê·¸", "ê·¸ê²ƒ", "ê·¸ê±°", "ìœ„", "ì•ì„œ", "ì´ì „", "ë°©ê¸ˆ", "ì•„ê¹Œ", "ì €ê¸°", "ê±°ê¸°",
            "ê·¸ ì¤‘", "ê·¸ì¤‘", "ê·¸ëŸ°ë°", "ê·¸ëŸ¼", "ê·¸ë˜ì„œ", "ë”°ë¼ì„œ", "ì´ì–´ì„œ", "ê³„ì†í•´ì„œ",
            "ì¶”ê°€ë¡œ", "ë”", "ë˜í•œ", "ê·¸ë¦¬ê³ ", "ë˜", "í•œí¸", "ë°˜ë©´", "ëŒ€ì‹ "
        ]

        has_continuation = any(keyword in current_query for keyword in continuation_keywords)

        if has_continuation and len(conversation_history) >= 2:
            # ìµœê·¼ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ AI ë‹µë³€ ì¶”ì¶œ
            recent_user = None
            recent_ai = None

            # ì´ì „ ëŒ€í™”ì—ì„œ í•µì‹¬ ë°ì´í„° ì¶”ì¶œ
            extracted_data = {
                "regions": set(),
                "food_items": set(),
                "key_facts": set()
            }

            for msg in reversed(conversation_history):
                if msg.get("type") == "user" and not recent_user:
                    recent_user = msg.get("content", "")
                elif msg.get("type") == "assistant" and not recent_ai and recent_user:
                    recent_ai = msg.get("content", "")
                    # í•µì‹¬ ë°ì´í„° ì¶”ì¶œ
                    key_data = self._extract_key_data_from_content(recent_ai)
                    extracted_data["regions"].update(key_data["regions"])
                    extracted_data["food_items"].update(key_data["food_items"])
                    extracted_data["key_facts"].update(key_data["key_facts"])
                    break

            if recent_user and recent_ai:
                ai_summary = recent_ai[:80] + "..." if len(recent_ai) > 80 else recent_ai

                # í•µì‹¬ ë°ì´í„°ë¥¼ í¬í•¨í•œ ìš”ì•½ ìƒì„±
                context_parts = []
                if extracted_data["regions"]:
                    context_parts.append(f"**ê´€ë ¨ ì§€ì—­**: {', '.join(list(extracted_data['regions'])[:5])}")
                if extracted_data["food_items"]:
                    context_parts.append(f"**ì–¸ê¸‰ëœ ì‹ì¬ë£Œ**: {', '.join(list(extracted_data['food_items'])[:5])}")
                if extracted_data["key_facts"]:
                    context_parts.append(f"**í•µì‹¬ ì‚¬ì‹¤**: {', '.join(list(extracted_data['key_facts'])[:3])}")

                context_info = "\n".join(context_parts) if context_parts else ""

                summary = f"""
## ì´ì „ ëŒ€í™” ìš”ì•½

ì´ì „ì— ë¬¸ì˜í•˜ì‹  **'{recent_user[:40]}{'...' if len(recent_user) > 40 else ''}'**ì— ëŒ€í•´ {ai_summary}ë¼ê³  ë‹µë³€ë“œë ¸ìœ¼ë©°, ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¶”ê°€ ë¶„ì„ì„ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.

{context_info}

---
"""
                print(f"ğŸ§  ë³´ê³ ì„œìš© ë©”ëª¨ë¦¬ ìš”ì•½ ìƒì„±: ì§€ì—­ {len(extracted_data['regions'])}ê°œ, ì‹ì¬ë£Œ {len(extracted_data['food_items'])}ê°œ")
                return summary

        return ""

    def get_available_personas(self) -> List[str]:
        """
        í˜„ì¬ ë¡œë“œëœ ëª¨ë“  í˜ë¥´ì†Œë‚˜ì˜ ì´ë¦„ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì„ íƒì§€ë¥¼ ë™ì ìœ¼ë¡œ êµ¬ì„±í•˜ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        """
        if not self.personas:
            return []
        return list(self.personas.keys())

    async def suggest_team_for_query(self, query: str) -> str:
        """
        LLMì´ ì¿¼ë¦¬ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ê°€ì¥ ì í•©í•œ í˜ë¥´ì†Œë‚˜ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤.
        """
        if not self.personas:
            return "ê¸°ë³¸"

        # ì‚¬ìš© ê°€ëŠ¥í•œ í˜ë¥´ì†Œë‚˜ ëª©ë¡ê³¼ ì„¤ëª… ìƒì„±
        persona_descriptions = []
        for persona_name, persona_info in self.personas.items():
            description = persona_info.get("description", "ì„¤ëª… ì—†ìŒ")
            persona_descriptions.append(f"- {persona_name}: {description}")

        personas_text = "\n".join(persona_descriptions)

        # LLMì—ê²Œ í˜ë¥´ì†Œë‚˜ ì¶”ì²œ ìš”ì²­
        prompt = f"""ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê°€ì¥ ì í•©í•œ ì „ë¬¸ê°€ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: "{query}"

ì‚¬ìš© ê°€ëŠ¥í•œ ì „ë¬¸ê°€ë“¤:
{personas_text}

ìœ„ ì „ë¬¸ê°€ë“¤ ì¤‘ì—ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ê°€ì¥ ì í•©í•œ ì „ë¬¸ê°€ í•œ ëª…ì„ ì„ íƒí•˜ì—¬, ì •í™•íˆ ê·¸ ì´ë¦„ë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ì˜ˆ: êµ¬ë§¤ ë‹´ë‹¹ì"""

        try:
            import os
            import sys
            
            # Fallback ì‹œìŠ¤í…œ import (Docker ë³¼ë¥¨ ë§ˆìš´íŠ¸ëœ utils í´ë”)
            sys.path.append('/app')
            from utils.model_fallback import ModelFallbackManager

            # Gemini â†’ OpenAI Fallbackìœ¼ë¡œ íŒ€ ì¶”ì²œ
            full_prompt = f"""ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê°€ì¥ ì ì ˆí•œ ì „ë¬¸ê°€ë¥¼ ì¶”ì²œí•˜ëŠ” AIì…ë‹ˆë‹¤. ì •í™•íˆ ì£¼ì–´ì§„ ì „ë¬¸ê°€ ì´ë¦„ ì¤‘ í•˜ë‚˜ë§Œ ë‹µë³€í•˜ì„¸ìš”.

{prompt}"""

            suggested_persona = ModelFallbackManager.try_invoke_with_fallback(
                prompt=full_prompt,
                gemini_model="gemini-2.5-flash-lite",
                openai_model="gpt-4o-mini",
                temperature=0.1,
                max_tokens=50
            )

            suggested_persona = suggested_persona.strip()

            # ì œì•ˆëœ í˜ë¥´ì†Œë‚˜ê°€ ì‹¤ì œ ëª©ë¡ì— ìˆëŠ”ì§€ í™•ì¸
            if suggested_persona in self.personas:
                print(f"ğŸ¤– LLM ìë™ ë¼ìš°íŒ…: '{query[:30]}...' -> '{suggested_persona}'")
                return suggested_persona
            else:
                print(f"ğŸ¤– LLM ìë™ ë¼ìš°íŒ…: ì˜ëª»ëœ ì‘ë‹µ '{suggested_persona}' -> 'ê¸°ë³¸' ì‚¬ìš©")
                return "ê¸°ë³¸"

        except Exception as e:
            print(f"ğŸ¤– LLM ìë™ ë¼ìš°íŒ… ì˜¤ë¥˜: {e} -> 'ê¸°ë³¸' ì‚¬ìš©")
            return "ê¸°ë³¸"

    # âœ… ì¶”ê°€: ì¼ê´€ëœ ìƒíƒœ ë©”ì‹œì§€ ìƒì„±ì„ ìœ„í•œ í—¬í¼ í•¨ìˆ˜
    def _create_status_event(self, stage: str, sub_stage: str, message: str, details: Optional[Dict] = None) -> Dict:
        """í‘œì¤€í™”ëœ ìƒíƒœ ì´ë²¤íŠ¸ ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        return {
            "type": "status",
            "timestamp": datetime.now().isoformat(),
            "data": {
                "agent": "OrchestratorAgent",
                "stage": stage,
                "sub_stage": sub_stage,
                "message": message,
                "details": details or {}
            }
        }

    async def _invoke_with_fallback(self, prompt: str, primary_model, fallback_model):
        """Gemini API rate limit ì‹œ OpenAIë¡œ fallback ì²˜ë¦¬"""
        try:
            result = await primary_model.ainvoke(prompt)
            return result
        except Exception as e:
            error_str = str(e).lower()
            rate_limit_indicators = ['429', 'quota', 'rate limit', 'exceeded', 'resource_exhausted']

            if any(indicator in error_str for indicator in rate_limit_indicators):
                print(f"OrchestratorAgent: Gemini API rate limit ê°ì§€, fallback ì‹œë„: {e}")
                if fallback_model:
                    try:
                        result = await fallback_model.ainvoke(prompt)
                        print("OrchestratorAgent: fallback ì„±ê³µ")
                        return result
                    except Exception as fallback_error:
                        print(f"OrchestratorAgent: fallbackë„ ì‹¤íŒ¨: {fallback_error}")
                        raise fallback_error
                else:
                    print("OrchestratorAgent: fallback ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
                    raise e
            else:
                raise e

    def _inject_context_into_query(self, query: str, context: Dict[int, str]) -> str:
        """'[step-Xì˜ ê²°ê³¼]' í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ì‹¤ì œ ì»¨í…ìŠ¤íŠ¸ë¡œ êµì²´í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
        match = re.search(r"\[step-(\d+)ì˜ ê²°ê³¼\]", query)
        if match:
            step_index = int(match.group(1))
            if step_index in context:
                print(f"  - {step_index}ë‹¨ê³„ ì»¨í…ìŠ¤íŠ¸ ì£¼ì…: '{context[step_index][:]}...'")
                # í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ì´ì „ ë‹¨ê³„ì˜ ìš”ì•½ ê²°ê³¼ë¡œ ì¹˜í™˜
                return query.replace(match.group(0), f"ì´ì „ ë‹¨ê³„ ë¶„ì„ ê²°ê³¼: '{context[step_index]}'")
        return query

    async def generate_plan(self, state: StreamingAgentState) -> StreamingAgentState:
        """í˜ë¥´ì†Œë‚˜ ê´€ì ê³¼ ì˜ì¡´ì„±ì„ ë°˜ì˜í•˜ì—¬ ë‹¨ê³„ë³„ ì‹¤í–‰ ê³„íš(Hybrid Model)ì„ ìˆ˜ë¦½í•©ë‹ˆë‹¤."""
        print(f"\n>> Orchestrator: ì§€ëŠ¥í˜• ë‹¨ê³„ë³„ ê³„íš ìˆ˜ë¦½ ì‹œì‘")
        query = state["original_query"]
        current_date_str = datetime.now().strftime("%Yë…„ %mì›” %dì¼")

        # í˜ë¥´ì†Œë‚˜ ì •ë³´ ì¶”ì¶œ
        persona_name = state.get("persona", "ê¸°ë³¸")
        persona_info = self.personas.get(persona_name, {})
        persona_description = persona_info.get("description", "ì¼ë°˜ì ì¸ ë¶„ì„ê°€")
        print(f"  - ê³„íš ìˆ˜ë¦½ì— '{persona_name}' í˜ë¥´ì†Œë‚˜ ê´€ì  ì ìš©")

        planning_prompt = f"""
ë‹¹ì‹ ì€ **'{persona_name}'ì˜ ìœ ëŠ¥í•œ AI ìˆ˜ì„ ë³´ì¢Œê´€**ì´ì **ì‹¤í–‰ ê³„íš ì„¤ê³„ ì „ë¬¸ê°€**ì…ë‹ˆë‹¤.
ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì‚¬ìš©ìì˜ ìš”ì²­ì„ **ìˆëŠ” ê·¸ëŒ€ë¡œ** ë¶„ì„í•˜ê³ , **'{persona_name}'ì˜ ì „ë¬¸ì„±ì„ í™œìš©í•˜ì—¬ ê°€ì¥ íš¨ìœ¨ì ì¸ ë°ì´í„° ìˆ˜ì§‘ ê³„íš**ì„ ìˆ˜ë¦½í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

**### Strict Adherence Mandate (ì—„ê²©í•œ ì¤€ìˆ˜ ëª…ë ¹) ###**
**1. ì ˆëŒ€ ì‚¬ìš©ì ìš”ì²­ì„ í™•ì¥í•˜ê±°ë‚˜ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.** ì‚¬ìš©ìê°€ "Aì™€ Bë¥¼ ì•Œë ¤ì¤˜"ë¼ê³  í–ˆë‹¤ë©´, ì˜¤ì§ Aì™€ Bì— ëŒ€í•œ ì •ë³´ë§Œ ìˆ˜ì§‘í•´ì•¼ í•©ë‹ˆë‹¤. ê´€ë ¨ë  ìˆ˜ ìˆëŠ” C(ì˜ˆ: ì‹œì¥ ê·œëª¨, ì†Œë¹„ì ì„ í˜¸ë„)ë¥¼ ë¬»ì§€ ì•Šì•˜ë‹¤ë©´ ì ˆëŒ€ ê³„íšì— í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
**2. í˜ë¥´ì†Œë‚˜ëŠ” 'ì–´ë–»ê²Œ(How)' ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ë¶„ì„í• ì§€ì— ëŒ€í•œ ê´€ì ì´ì§€, 'ë¬´ì—‡ì„(What)' ìˆ˜ì§‘í• ì§€ë¥¼ ê²°ì •í•˜ëŠ” ì—­í• ì´ ì•„ë‹™ë‹ˆë‹¤.** í˜ë¥´ì†Œë‚˜ì˜ ê´€ì‹¬ì‚¬ë¥¼ ì´ìœ ë¡œ ì‚¬ìš©ìê°€ ë¬»ì§€ ì•Šì€ ìƒˆë¡œìš´ ì£¼ì œë¥¼ ì¶”ê°€í•´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤.
**3. ìµœì¢… ëª©í‘œëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ 'ì§ì ‘ì ì¸ ë‹µë³€'ì„ ì°¾ëŠ” ê²ƒì…ë‹ˆë‹¤.** ê´‘ë²”ìœ„í•œ ë°°ê²½ ì¡°ì‚¬ë¥¼ í•˜ëŠ” ê²ƒì´ ì•„ë‹™ë‹ˆë‹¤.
**4. ë…¼ë¦¬ì  ê³„íš ìˆ˜ë¦½ (How to ask): í•˜ìœ„ ì§ˆë¬¸ë“¤ì˜ ì„ í›„ ê´€ê³„ë¥¼ ë¶„ì„í•˜ì—¬, ì˜ì¡´ì„±ì´ ìˆëŠ” ì‘ì—…ì€ ìˆœì°¨ì ìœ¼ë¡œ, ì—†ëŠ” ì‘ì—…ì€ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ëŠ” ìµœì ì˜ ì‹¤í–‰ ë‹¨ê³„ë¥¼ ì„¤ê³„í•˜ì„¸ìš”.

**ì‚¬ìš©ì ì›ë³¸ ìš”ì²­**: "{query}"
**í˜„ì¬ ë‚ ì§œ**: {current_date_str}

---
**## ë³´ìœ  ë„êµ¬ ëª…ì„¸ì„œ ë° ì„ íƒ ê°€ì´ë“œ**

**1. rdb_search (PostgreSQL) - 1ìˆœìœ„ í™œìš©**
   - **ë°ì´í„° ì¢…ë¥˜**: ì •í˜• ë°ì´í„° (í…Œì´ë¸” ê¸°ë°˜: ì‹ìì¬ **ì˜ì–‘ì„±ë¶„**, ë†Â·ì¶•Â·ìˆ˜ì‚°ë¬¼ **ì‹œì„¸/ê°€ê²©/ê±°ë˜ëŸ‰** ë“± ìˆ˜ì¹˜ ë°ì´í„°).
   - **ì‚¬ìš© ì‹œì **: ì˜ì–‘ì„±ë¶„, í˜„ì¬ê°€ê²©, ì‹œì„¸ë³€ë™, ê°€ê²©ë¹„êµ, ìˆœìœ„/í‰ê· /í•©ê³„ ë“± **ì •í™•í•œ ìˆ˜ì¹˜ ì—°ì‚°**ì´ í•„ìš”í•  ë•Œ.
   - **íŠ¹ì§•**: ë‚ ì§œÂ·ì§€ì—­Â·í’ˆëª© ì»¬ëŸ¼ìœ¼ë¡œ **í•„í„°/ì§‘ê³„** ìµœì í™”. ë‹¤ì¤‘ ì¡°ê±´(where)ê³¼ group by, order byë¥¼ í†µí•œ **í†µê³„/ë­í‚¹** ì§ˆì˜ì— ì í•©. (ê´€ê³„ ê·¸ë˜í”„ íƒìƒ‰ì€ ë¹„ê¶Œì¥)
   - **ì˜ˆì‹œ ì§ˆì˜ ì˜ë„**: "ì‚¬ê³¼ ë¹„íƒ€ë¯¼C í•¨ëŸ‰", "ì§€ë‚œë‹¬ ì œì£¼ ê°ê·¤ í‰ê· ê°€", "ì „ë³µ ê°€ê²© ì¶”ì´", "ì˜ì–‘ì„±ë¶„ ìƒìœ„ TOP 10"

**2. vector_db_search (Elasticsearch) - 1ìˆœìœ„ í™œìš©**
   - **ë°ì´í„° ì¢…ë¥˜**: ë¹„ì •í˜• ë°ì´í„° (ë‰´ìŠ¤ê¸°ì‚¬, ë…¼ë¬¸, ë³´ê³ ì„œ ì „ë¬¸).
   - **ì‚¬ìš© ì‹œì **: ì •ì±…ë¬¸ì„œ, ë°°ê²½ì •ë³´, ì‹¤ë¬´ê°€ì´ë“œ ë“± ì„œìˆ í˜• ì •ë³´ë‚˜ ë¶„ì„ì´ í•„ìš”í•  ë•Œ.
   - **íŠ¹ì§•**: ì˜ë¯¸ê¸°ë°˜ ê²€ìƒ‰ìœ¼ë¡œ ì§ˆë¬¸ì˜ ë§¥ë½ê³¼ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œë¥¼ ì°¾ì•„ì¤Œ.

**3. graph_db_search (Neo4j) - 1ìˆœìœ„ í™œìš©**
   - **ë°ì´í„° ì¢…ë¥˜**: **ê´€ê³„í˜•(ê·¸ë˜í”„) ë°ì´í„°**. ë…¸ë“œ: í’ˆëª©(ë†ì‚°ë¬¼/ìˆ˜ì‚°ë¬¼/ì¶•ì‚°ë¬¼), **Origin(ì›ì‚°ì§€: city/region)**, **Nutrient(ì˜ì–‘ì†Œ)**.
     ê´€ê³„: `(í’ˆëª©)-[:isFrom]->(Origin)`, `(í’ˆëª©)-[:hasNutrient]->(Nutrient)`. ìˆ˜ì‚°ë¬¼ì€ í’ˆëª© ë…¸ë“œì— `fishState`(í™œì–´/ì„ ì–´/ëƒ‰ë™/ê±´ì–´) ì†ì„± ì¡´ì¬.
   - **ì‚¬ìš© ì‹œì **: **í’ˆëª© â†” ì›ì‚°ì§€**, **í’ˆëª© â†” ì˜ì–‘ì†Œ**ì²˜ëŸ¼ **ì—”í‹°í‹° ê°„ ì—°ê²°**ì´ í•µì‹¬ì¼ ë•Œ. ì§€ì—­Â·ìƒíƒœ(fishState) ì¡°ê±´ì„ ì–¹ì€ **ì›ì‚°ì§€/íŠ¹ì‚°í’ˆ íƒìƒ‰**.
   - **íŠ¹ì§•**: ì§€ì‹ê·¸ë˜í”„ ê²½ë¡œ íƒìƒ‰ì— ìµœì í™”. í‚¤ì›Œë“œëŠ” **í’ˆëª©ëª…/ì§€ì—­ëª…/ì˜ì–‘ì†Œ/ìˆ˜ì‚°ë¬¼ ìƒíƒœ(fishState)**ë¡œ ê°„ê²°íˆ í‘œí˜„í•˜ê³ , ì§ˆë¬¸ì€ **"Aì˜ ì›ì‚°ì§€", "Aì˜ ì˜ì–‘ì†Œ", "ì§€ì—­ Bì˜ íŠ¹ì‚°í’ˆ/ì›ì‚°ì§€", "í™œì–´ Aì˜ ì›ì‚°ì§€"**ì²˜ëŸ¼ **ê´€ê³„ë¥¼ ëª…ì‹œ**í• ìˆ˜ë¡ ì •í™•ë„ ìƒìŠ¹.
   - **ì˜ˆì‹œ ì§ˆì˜ ì˜ë„**: "ì‚¬ê³¼ì˜ ì›ì‚°ì§€", "ì˜¤ë Œì§€ì˜ ì˜ì–‘ì†Œ", "ì œì£¼ë„ì˜ ê°ê·¤ ì›ì‚°ì§€", "í™œì–´ ë¬¸ì–´ ì›ì‚°ì§€", "ê²½ìƒë¶ë„ ì‚¬ê³¼ ì‚°ì§€ ì—°ê²°"

**4. pubmed_search - 1ìˆœìœ„ í™œìš© (í•™ìˆ  ì—°êµ¬ í•„ìš”ì‹œ)**
   - **ë°ì´í„° ì¢…ë¥˜**: ìµœì‹  í•™ìˆ  ë…¼ë¬¸ (pubmed í”„ë¦¬í”„ë¦°íŠ¸ ë…¼ë¬¸).
   - **ì‚¬ìš© ì‹œì **: ì‹ ì œí’ˆ ê°œë°œ, ê³¼í•™ì  ê·¼ê±°, ìµœì‹  ì—°êµ¬ ë™í–¥, ëŒ€ì²´ì‹í’ˆ ì—°êµ¬, ì§€ì†ê°€ëŠ¥ì„± ì—°êµ¬ê°€ í•„ìš”í•  ë•Œ.
   - **íŠ¹ì§•**: **ë°˜ë“œì‹œ ì¿¼ë¦¬ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­ í›„ ê²€ìƒ‰ í•„ìˆ˜.** food science, biotechnology, alternative protein ë“± ì˜ë¬¸ í•™ìˆ  ìš©ì–´ë¡œ ê²€ìƒ‰. ì˜ˆì‹œ: plant-based meat alternatives, alternative protein, functional food, sustainable food ë“±
   - **ì˜ˆì‹œ ì§ˆì˜ ì˜ë„**: "Plant-based alternative meat development papers", "Latest research on fermentation technology", "Bioplastic packaging materials"

**5. web_search - ìµœì‹  ì •ë³´ í•„ìˆ˜ ì‹œ ìš°ì„  ì‚¬ìš©**
   - **ë°ì´í„° ì¢…ë¥˜**: ì‹¤ì‹œê°„ ìµœì‹  ì •ë³´, ì‹œì‚¬ ì •ë³´, ì¬í•´/ì¬ë‚œ ì •ë³´, ìµœê·¼ ë‰´ìŠ¤.
   - **ì‚¬ìš© ì¡°ê±´**:
     * **í•„ìˆ˜ ì‚¬ìš©**: 2025ë…„ íŠ¹ì • ì›”/ì¼, ìµœê·¼, í˜„ì¬, ê¸°ìƒì´ë³€, ì§‘ì¤‘í­ìš°, ì¬í•´, ì¬ë‚œ ë“±ì´ í¬í•¨ëœ ì§ˆë¬¸
     * **ìš°ì„  ì‚¬ìš©**: ë‚´ë¶€ DBì— ì—†ì„ ê°€ëŠ¥ì„±ì´ ë†’ì€ ìµœì‹  ì‚¬ê±´/ìƒí™© ì •ë³´
     * **ë³´ì¡° ì‚¬ìš©**: ë‚´ë¶€ DBë¡œ í•´ê²°ë˜ì§€ ì•ŠëŠ” ì¼ë°˜ ì§€ì‹
   - **ì‚¬ìš© ê¸ˆì§€**: ì¼ë°˜ì ì¸ ë†ì¶•ìˆ˜ì‚°ë¬¼ ì‹œì„¸, ì˜ì–‘ì •ë³´, ì›ì‚°ì§€ ë“±ì€ ë‚´ë¶€ DB ìš°ì„  ì‚¬ìš© í›„ ë³´ì™„ì ìœ¼ë¡œë§Œ ì‚¬ìš©.

**ë„êµ¬ ì„ íƒ ìš°ì„ ìˆœìœ„:**
1. **â­ ìµœì‹  ì •ë³´/ì‹œì‚¬ (2025ë…„ íŠ¹ì • ì‹œì , ì¬í•´, ê¸°ìƒì´ë³€, ë‰´ìŠ¤)** â†’ `web_search` **[ìµœìš°ì„ ]**
2. **ìˆ˜ì¹˜/í†µê³„ ë°ì´í„° (ì‹ìì¬ ì˜ì–‘ì„±ë¶„, ë†ì¶•ìˆ˜ì‚°ë¬¼ ì‹œì„¸)** â†’ `rdb_search`
3. **ê´€ê³„/ë¶„ë¥˜ ì •ë³´ (í’ˆëª©-ì›ì‚°ì§€, í’ˆëª©-ì˜ì–‘ì†Œ, ì§€ì—­-íŠ¹ì‚°í’ˆ, ìˆ˜ì‚°ë¬¼ ìƒíƒœë³„ ì›ì‚°ì§€)** â†’ `graph_db_search`
4. **ë¶„ì„/ì—°êµ¬ ë¬¸ì„œ (ì‹œì¥ë¶„ì„, ì†Œë¹„ì ì¡°ì‚¬)** â†’ `vector_db_search`
5. **í•™ìˆ  ë…¼ë¬¸/ì—°êµ¬ (ì‹ ì œí’ˆ ê°œë°œ, ê³¼í•™ì  ê·¼ê±°)** â†’ `pubmed_search`

**ê° ë„êµ¬ë³„ ì ìš© ì˜ˆì‹œ:**
- `rdb_search`: "ì‹ìì¬ ì˜ì–‘ì„±ë¶„", "ë†ì¶•ìˆ˜ì‚°ë¬¼ ì‹œì„¸", "ê°€ê²© ì¶”ì´/ë¹„êµ", "ì˜ì–‘ì„±ë¶„ ìƒìœ„ TOP"
- `graph_db_search`: "ì‚¬ê³¼ì˜ ì›ì‚°ì§€", "ì˜¤ë Œì§€ì˜ ì˜ì–‘ì†Œ", "ì œì£¼-ê°ê·¤ ê´€ê³„", "í™œì–´ ë¬¸ì–´ ì›ì‚°ì§€", "ì§€ì—­ë³„ íŠ¹ì‚°í’ˆ ì—°ê²°"
- `vector_db_search`: "ì‹œì¥ ë¶„ì„ ë³´ê³ ì„œ", "ì†Œë¹„ì í–‰ë™ ì—°êµ¬", "ì •ì±… ë¬¸ì„œ"
- `pubmed_search`: "Plant-based alternative meat development papers", "Latest research on fermentation technology", "Bioplastic packaging materials"
- `web_search`: "2025ë…„ ìµœì‹  íŠ¸ë Œë“œ", "ì‹¤ì‹œê°„ ì—…ê³„ ë™í–¥", "2025ë…„ 7ì›” ì§‘ì¤‘í­ìš° í”¼í•´ì§€ì—­", "ê¸°ìƒì´ë³€ ë†ì—… í”¼í•´", "ìµœê·¼ ì¬í•´ ë°œìƒ ì§€ì—­", "í˜„ì¬ ë†ì‚°ë¬¼ ê³µê¸‰ ìƒí™©"

---
**## ê³„íš ìˆ˜ë¦½ì„ ìœ„í•œ ë‹¨ê³„ë³„ ì‚¬ê³  í”„ë¡œì„¸ìŠ¤ (ë°˜ë“œì‹œ ì¤€ìˆ˜í•  ê²ƒ)**

**1ë‹¨ê³„: ì‚¬ìš©ì ìš”ì²­ ë¶„í•´ (Decomposition)**
- ì‚¬ìš©ìì˜ ì›ë³¸ ìš”ì²­ì„ ì˜ë¯¸ì , ë…¼ë¦¬ì  ë‹¨ìœ„ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤. ê° ë‹¨ìœ„ëŠ” ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ ìš”êµ¬í•œ í•˜ë‚˜ì˜ ì •ë³´ ì¡°ê°ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
- ì˜ˆ: "ëŒ€ì²´ì‹í’ˆì˜ ìœ í˜•ì„ ì›ë£Œì— ë”°ë¼ êµ¬ë¶„í•˜ê³ , ë¯¸ìƒë¬¼ ë°œíš¨ ì‹í’ˆì˜ ì—°êµ¬ê°œë°œ í˜„í™©ì„ ë¶„ì„í•´ì¤˜."
  - ë‹¨ìœ„ 1: "ëŒ€ì²´ì‹í’ˆì˜ ìœ í˜•ì„ ì›ë£Œì— ë”°ë¼ êµ¬ë¶„í•˜ì—¬ ì •ë¦¬"
  - ë‹¨ìœ„ 2: "ë¯¸ìƒë¬¼ ë°œíš¨ ì‹í’ˆì˜ ì—°êµ¬ê°œë°œ í˜„í™© ë¶„ì„ ë° ì •ë¦¬"

**2ë‹¨ê³„: ê° ë‹¨ìœ„ì— ëŒ€í•œ í•˜ìœ„ ì§ˆë¬¸ ìƒì„±**
- 1ë‹¨ê³„ì—ì„œ ë¶„í•´ëœ ê° ë‹¨ìœ„ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ í•„ìš”í•œ êµ¬ì²´ì ì¸ ì§ˆë¬¸ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤.
- ì´ ì§ˆë¬¸ë“¤ì€ í˜ë¥´ì†Œë‚˜ì˜ ì „ë¬¸ì„±ì„ ë°˜ì˜í•´ì•¼ í•©ë‹ˆë‹¤. (ì˜ˆ: 'ì œí’ˆ ê°œë°œ ì—°êµ¬ì›'ì€ ê¸°ìˆ , ì„±ë¶„, ë…¼ë¬¸ì— ì´ˆì ì„ ë§ì¶˜ ì§ˆë¬¸ ìƒì„±)
- ì˜ˆ (ì œí’ˆ ê°œë°œ ì—°êµ¬ì› ê´€ì ):
  - (ë‹¨ìœ„ 1 ê´€ë ¨): "ì‹ë¬¼ì„±, ê³¤ì¶©, ë°°ì–‘ìœ¡ ë“± ì›ë£Œ ê¸°ë°˜ ëŒ€ì²´ì‹í’ˆ ìœ í˜•ë³„ ê¸°ìˆ ì  ì •ì˜ ë° ë¶„ë¥˜", "ì£¼ìš” ì›ë£Œë³„ ëŒ€ì²´ì‹í’ˆì˜ í•µì‹¬ ì„±ë¶„ ë° íŠ¹ì„±"
  - (ë‹¨ìœ„ 2 ê´€ë ¨): "ë¯¸ìƒë¬¼ ë°œíš¨ ëŒ€ì²´ì‹í’ˆ ê´€ë ¨ ìµœì‹  ì—°êµ¬ ë…¼ë¬¸ ë° íŠ¹í—ˆ ë™í–¥", "ë¯¸ìƒë¬¼ ë°œíš¨ ê¸°ìˆ ì„ í™œìš©í•œ ìƒìš©í™” ì œí’ˆ ì‚¬ë¡€ ë° ì ìš© ê¸°ìˆ "
- ê°ê° ì™„ê²°ëœ í˜•íƒœì˜ êµ¬ì²´ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ë¶„í•´í•©ë‹ˆë‹¤.
- ìƒì„±ëœ ëª¨ë“  í•˜ìœ„ ì§ˆë¬¸ì€ ì›ë³¸ ìš”ì²­ì˜ í•µì‹¬ ë§¥ë½(ì˜ˆ: 'ëŒ€í•œë¯¼êµ­', 'ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ')ì„ ë°˜ë“œì‹œ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.

**3ë‹¨ê³„: ì§ˆë¬¸ ê°„ ì˜ì¡´ì„± ë¶„ì„ (ê°€ì¥ ì¤‘ìš”í•œ ë‹¨ê³„)**
- ë¶„í•´ëœ ì§ˆë¬¸ë“¤ ê°„ì˜ ì„ í›„ ê´€ê³„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
- **"ì–´ë–¤ ì§ˆë¬¸ì´ ë‹¤ë¥¸ ì§ˆë¬¸ì˜ ê²°ê³¼ë¥¼ ë°˜ë“œì‹œ ì•Œì•„ì•¼ë§Œ ì œëŒ€ë¡œ ìˆ˜í–‰ë  ìˆ˜ ìˆëŠ”ê°€?"**ë¥¼ íŒë‹¨í•©ë‹ˆë‹¤.
- ì˜ˆì‹œ: `Aë¶„ì•¼ì˜ ì‹œì¥ ê·œëª¨`ë¥¼ ì•Œì•„ì•¼ `Aë¶„ì•¼ì˜ ì£¼ìš” ê²½ìŸì‚¬`ë¥¼ ì¡°ì‚¬í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ë‘ ì§ˆë¬¸ì€ ì˜ì¡´ì„±ì´ ìˆìŠµë‹ˆë‹¤. ë°˜ë©´, `Aë¶„ì•¼ì˜ ì‹œì¥ ê·œëª¨`ì™€ `Bë¶„ì•¼ì˜ ì‹œì¥ ê·œëª¨` ì¡°ì‚¬ëŠ” ì„œë¡œ ë…ë¦½ì ì…ë‹ˆë‹¤.

**4ë‹¨ê³„: ì‹¤í–‰ ë‹¨ê³„ ê·¸ë£¹í™” (Grouping)**
- **Step 1**: ì„œë¡œ ì˜ì¡´ì„±ì´ ì—†ëŠ”, ê°€ì¥ ë¨¼ì € ìˆ˜í–‰ë˜ì–´ì•¼ í•  ë³‘ë ¬ ì‹¤í–‰ ê°€ëŠ¥í•œ ì§ˆë¬¸ë“¤ì„ ë°°ì¹˜í•©ë‹ˆë‹¤.
- **Step 2 ì´í›„**: ì´ì „ ë‹¨ê³„ì˜ ê²°ê³¼(`[step-Xì˜ ê²°ê³¼]` í”Œë ˆì´ìŠ¤í™€ë” ì‚¬ìš©)ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ì˜ì¡´ì„± ìˆëŠ” ì§ˆë¬¸ë“¤ì„ ë°°ì¹˜í•©ë‹ˆë‹¤. (ì˜ˆ: 1ë‹¨ê³„ì—ì„œ ì°¾ì€ 'ì„±ì¥ ë¶„ì•¼'ì˜ ê²½ìŸì‚¬ ì¡°ì‚¬)

**5ë‹¨ê³„: ê° ì§ˆë¬¸ì— ëŒ€í•œ ìµœì  ë„êµ¬ ì„ íƒ ì „ëµ**
- 'ë³´ìœ  ë„êµ¬ ëª…ì„¸ì„œ'ë¥¼ ì°¸ê³ í•˜ì—¬ ê° í•˜ìœ„ ì§ˆë¬¸ì— ê°€ì¥ ì í•©í•œ ë„êµ¬ë¥¼ **ë‹¨ í•˜ë‚˜ë§Œ** ì‹ ì¤‘í•˜ê²Œ ì„ íƒí•©ë‹ˆë‹¤.
- **ì¤‘ìš”**: ì§ˆë¬¸ì˜ ë³µì¡ì„±ì„ ë¶„ì„í•˜ì—¬ **í•„ìš”í•œ ë„êµ¬ë§Œ ì„ íƒ**í•©ë‹ˆë‹¤:
  * **ë‹¨ìˆœ ì§ˆë¬¸** â†’ 1ê°œ ë„êµ¬ë¡œ ì¶©ë¶„ (ì˜ˆ: "ì‚¬ê³¼ ì˜ì–‘ì„±ë¶„" â†’ `rdb_search`)
  * **ë³µí•© ì§ˆë¬¸** â†’ ì—¬ëŸ¬ ë„êµ¬ ì¡°í•© í•„ìš” (ì˜ˆ: "ìµœì‹  ì¬í•´ + ë†ì—… ë¶„ì„" â†’ ì—¬ëŸ¬ ë‹¨ê³„)
    - **"ì„±ë¶„", "ì˜ì–‘", "ì‹œì„¸", "ê°€ê²©"** í¬í•¨ â†’ `rdb_search`
    - **"ì›ì‚°ì§€", "ê´€ê³„", "ì œì¡°ì‚¬", "íŠ¹ì‚°í’ˆ", "fishState(í™œì–´/ì„ ì–´/ëƒ‰ë™/ê±´ì–´)"** í¬í•¨ â†’ `graph_db_search`
    - **"ë¶„ì„", "ì—°êµ¬", "ì¡°ì‚¬", "ë³´ê³ ì„œ", "ë™í–¥"** í¬í•¨ â†’ `vector_db_search`
    - **"ì‹ ì œí’ˆ ê°œë°œ", "ê³¼í•™ì  ê·¼ê±°", "ë…¼ë¬¸", "í•™ìˆ ", "ëŒ€ì²´ì‹í’ˆ", "ì§€ì†ê°€ëŠ¥ì„±", "ë°œíš¨ ê¸°ìˆ "** í¬í•¨ â†’ `pubmed_search`
    - **"ìµœì‹  íŠ¸ë Œë“œ", "ì‹¤ì‹œê°„ ì •ë³´", "2025ë…„", "ìµœê·¼", "í˜„ì¬", "ê¸°ìƒì´ë³€", "ì§‘ì¤‘í­ìš°", "í™ìˆ˜", "íƒœí’", "ì¬í•´", "ì¬ë‚œ", "í”¼í•´", "ë‰´ìŠ¤", "ì‚¬ê±´", "ë°œìƒ"** ë“± ìµœì‹ ì„± ê°•ì¡° ë˜ëŠ” ì‹œì‚¬ ì •ë³´ ê´€ë ¨ ì‹œ â†’ `web_search`

**ë„êµ¬ ì„ íƒ ì˜ˆì‹œ**:
- **ë‹¨ìˆœ ì¼€ì´ìŠ¤**: "ì‚¬ê³¼ì˜ ì˜ì–‘ì„±ë¶„" â†’ `rdb_search` 1ê°œë§Œ
- **ì¤‘ê°„ ì¼€ì´ìŠ¤**: "2025ë…„ ì‹í’ˆ íŠ¸ë Œë“œ" â†’ `web_search` 1ê°œë§Œ
- **ë³µí•© ì¼€ì´ìŠ¤**: "ìµœì‹  ì¬í•´ í”¼í•´ì§€ì—­ ë†ì—… í˜„í™©" â†’ `web_search` + `vector_db_search` + `graph_db_search` ì¡°í•©
- **ê³ ë„ ë³µí•©**: "ì‹ ì œí’ˆ ê°œë°œ ì „ëµ" â†’ `web_search` + `pubmed_search` + `vector_db_search` + `rdb_search` ì¡°í•©

**6ë‹¨ê³„: ìµœì¢… JSON í˜•ì‹í™”**
- ìœ„ì—ì„œ ê²°ì •ëœ ëª¨ë“  ë‚´ìš©ì„ ì•„ë˜ 'ìµœì¢… ì¶œë ¥ í¬ë§·'ì— ë§ì¶° JSONìœ¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.
- **ì¤‘ìš”**: `sub_questions` í‚¤ëŠ” ë°˜ë“œì‹œ `execution_steps` ë°°ì—´ì˜ ê° ìš”ì†Œ ì•ˆì—ë§Œ ì¡´ì¬í•´ì•¼ í•©ë‹ˆë‹¤.

---
**## ê³„íš ìˆ˜ë¦½ ì˜ˆì‹œ**

**[ì˜ˆì‹œ 1: ë‹¨ìˆœ ì¡°íšŒ - ë‹¨ì¼ Step, ë‹¨ì¼ ì‘ì—…]**
**ìš”ì²­**: "ì‚¬ê³¼ì˜ ì˜ì–‘ì„±ë¶„ê³¼ ì¹¼ë¡œë¦¬ ì •ë³´ë¥¼ ì•Œë ¤ì¤˜."
**ìƒì„±ëœ ê³„íš(JSON)**:
{{
    "title": "ì‚¬ê³¼ ì˜ì–‘ì„±ë¶„ ë° ì¹¼ë¡œë¦¬ ì •ë³´",
    "reasoning": "ì‚¬ìš©ìì˜ ìš”ì²­ì€ 'ì‚¬ê³¼ì˜ ì˜ì–‘ì„±ë¶„ ë° ì¹¼ë¡œë¦¬'ë¼ëŠ” ë‹¨ì¼ ì •ë³´ ì¡°ê°ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ì´ëŠ” RDBì—ì„œ ì§ì ‘ ì¡°íšŒê°€ ê°€ëŠ¥í•˜ë¯€ë¡œ, rdb_search ë„êµ¬ë¥¼ ì‚¬ìš©í•œ ë‹¨ì¼ ë‹¨ê³„ ê³„íšì„ ìˆ˜ë¦½í•©ë‹ˆë‹¤.",
    "execution_steps": [
        {{
            "step": 1,
            "reasoning": "ì˜ì–‘ì„±ë¶„ê³¼ ì¹¼ë¡œë¦¬ëŠ” RDBì— ì •í˜•í™”ëœ ë°ì´í„°ì´ë¯€ë¡œ rdb_searchë§Œìœ¼ë¡œ í•´ê²° ê°€ëŠ¥í•©ë‹ˆë‹¤.",
            "sub_questions": [
                {{"question": "ì‚¬ê³¼ì˜ ìƒì„¸ ì˜ì–‘ì„±ë¶„ ì •ë³´ ë° ì¹¼ë¡œë¦¬", "tool": "rdb_search"}}
            ]
        }}
    ]
}}

**[ì˜ˆì‹œ 2: ë³‘ë ¬ ì¡°íšŒ - ë‹¨ì¼ Step, ë‹¤ì¤‘ ì‘ì—…]**
**ìš”ì²­**: "ëŒ€ì²´ì‹í’ˆì˜ ìœ í˜•ì„ ì›ë£Œì— ë”°ë¼ êµ¬ë¶„í•˜ì—¬ ì •ë¦¬í•˜ê³  ì´ì¤‘ì—ì„œ ë¯¸ìƒë¬¼ ë°œíš¨ ì‹í’ˆì˜ ì—°êµ¬ê°œë°œ í˜„í™©ì„ ë¶„ì„í•˜ê³  ì •ë¦¬í•´ì¤˜."
**í˜ë¥´ì†Œë‚˜**: "ì œí’ˆ ê°œë°œ ì—°êµ¬ì›"
**ìƒì„±ëœ ê³„íš(JSON)**:
{{
  "title": "ì›ë£Œë³„ ëŒ€ì²´ì‹í’ˆ ìœ í˜• ë° ë¯¸ìƒë¬¼ ë°œíš¨ ì‹í’ˆ R&D í˜„í™© ë¶„ì„",
  "reasoning": "ì‚¬ìš©ì ìš”ì²­ì„ 'ì›ë£Œë³„ ìœ í˜• ë¶„ë¥˜'ì™€ 'ë¯¸ìƒë¬¼ ë°œíš¨ ì‹í’ˆ R&D í˜„í™©' ë‘ ê°€ì§€ ë…ë¦½ì ì¸ ì¶•ìœ¼ë¡œ ë¶„í•´í–ˆìŠµë‹ˆë‹¤. ë‘ ì£¼ì œëŠ” ì˜ì¡´ì„±ì´ ì—†ìœ¼ë¯€ë¡œ ë‹¨ì¼ ë‹¨ê³„ì—ì„œ ë³‘ë ¬ë¡œ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ê²ƒì´ ê°€ì¥ íš¨ìœ¨ì ì…ë‹ˆë‹¤. 'ì œí’ˆ ê°œë°œ ì—°êµ¬ì›'ì˜ ê´€ì ì— ë§ì¶° ê¸°ìˆ  ë° ì—°êµ¬ ìë£Œ ìˆ˜ì§‘ì— ì§‘ì¤‘í•©ë‹ˆë‹¤.",
  "execution_steps": [
    {{
      "step": 1,
      "reasoning": "ëŒ€ì²´ì‹í’ˆì˜ ê¸°ìˆ ì  ë¶„ë¥˜ì™€ ë¯¸ìƒë¬¼ ë°œíš¨ ì‹í’ˆì˜ ì—°êµ¬ ë™í–¥ì— ëŒ€í•œ ê¸°ë°˜ ì •ë³´ë¥¼ ë³‘ë ¬ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤. ì‹œì¥ ë™í–¥ ë“± ì‚¬ìš©ìê°€ ë¬»ì§€ ì•Šì€ ë‚´ìš©ì€ ì˜ë„ì ìœ¼ë¡œ ì œì™¸í–ˆìŠµë‹ˆë‹¤.",
      "sub_questions": [
        {{
          "question": "ì›ë£Œ(ì‹ë¬¼, ê³¤ì¶©, ë°°ì–‘ìœ¡, ê· ë¥˜ ë“±)ì— ë”°ë¥¸ ëŒ€ì²´ì‹í’ˆì˜ ê¸°ìˆ ì  ìœ í˜• ë¶„ë¥˜ ë° ì •ì˜",
          "tool": "vector_db_search"
        }},
        {{
          "question": "microbial fermentation for alternative protein or food ingredients latest research papers",
          "tool": "pubmed_search"
        }},
        {{
          "question": "êµ­ë‚´ì™¸ ë¯¸ìƒë¬¼ ë°œíš¨ ê¸°ìˆ  ê¸°ë°˜ ëŒ€ì²´ì‹í’ˆ ì—°êµ¬ ê°œë°œ í”„ë¡œì íŠ¸ ë˜ëŠ” ìƒìš©í™” ì‚¬ë¡€",
          "tool": "vector_db_search"
        }}
      ]
    }}
  ]
}}

**[ì˜ˆì‹œ 3: ìˆœì°¨(ì˜ì¡´ì„±) ì¡°íšŒ - ë‹¤ì¤‘ Step]**
**ìš”ì²­**: "2025ë…„ 7ì›”ê³¼ 8ì›”ì˜ ì§‘ì¤‘í­ìš° í”¼í•´ì§€ì—­ì—ì„œ ìƒì‚°ë˜ëŠ” ì£¼ìš” ì‹ì¬ë£Œë“¤ ëª©ë¡ê³¼ ìƒì‚°ì§€ë¥¼ í‘œë¡œ ì •ë¦¬í•´ì¤˜"
**ìƒì„±ëœ ê³„íš(JSON)**:
{{
    "title": "2025ë…„ ì—¬ë¦„ ì§‘ì¤‘í­ìš° í”¼í•´ì§€ì—­ì˜ ì£¼ìš” ì‹ì¬ë£Œ ë° ìƒì‚°ì§€ ë¶„ì„",
    "reasoning": "ì´ ìš”ì²­ì€ ì—¬ëŸ¬ ì •ë³´ê°€ ë…¼ë¦¬ì ìœ¼ë¡œ ì—°ê²°ë˜ì–´ì•¼ í•´ê²° ê°€ëŠ¥í•©ë‹ˆë‹¤. ë¨¼ì € 'í”¼í•´ ì§€ì—­'ì„ íŠ¹ì •í•˜ê³ (Step 1), ê·¸ ì§€ì—­ì˜ 'ì£¼ìš” ì‹ì¬ë£Œì™€ ìƒì‚°ì§€'ë¥¼ ì°¾ì•„(Step 2) ìµœì¢…ì ìœ¼ë¡œ ì¢…í•© ë¶„ì„(Step 3)í•˜ëŠ” ìˆœì°¨ì ì¸ ê³„íšì´ í•„ìš”í•©ë‹ˆë‹¤.",
    "execution_steps": [
        {{
            "step": 1,
            "reasoning": "ê°€ì¥ ë¨¼ì € ìµœì‹  ì¬í•´ ì •ë³´ë¥¼ í†µí•´ 'ì§‘ì¤‘í­ìš° í”¼í•´ ì§€ì—­'ì„ ëª…í™•íˆ íŠ¹ì •í•´ì•¼ í•©ë‹ˆë‹¤.",
            "sub_questions": [
                {{"question": "2025ë…„ 7ì›” 8ì›” ëŒ€í•œë¯¼êµ­ ì§‘ì¤‘í˜¸ìš° í”¼í•´ ì‹¬ê° ì§€ì—­ ëª©ë¡", "tool": "web_search"}}
            ]
        }},
        {{
            "step": 2,
            "reasoning": "Step 1ì—ì„œ ì‹ë³„ëœ í”¼í•´ ì§€ì—­ì„ ë°”íƒ•ìœ¼ë¡œ í•´ë‹¹ ì§€ì—­ì—ì„œ ì£¼ë¡œ ìƒì‚°ë˜ëŠ” ì‹ì¬ë£Œ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.",
            "sub_questions": [
                {{"question": "[step-1ì˜ ê²°ê³¼]ë¡œ í™•ì¸ëœ í”¼í•´ ì§€ì—­ë“¤ ê°ê°ì˜ ì£¼ìš” ìƒì‚° ë†ì¶•ìˆ˜ì‚°ë¬¼(íŠ¹ì‚°í’ˆ) ëª©ë¡", "tool": "graph_db_search"}},
                {{"question": "[step-1ì˜ ê²°ê³¼]ë¡œ í™•ì¸ëœ í”¼í•´ ì§€ì—­ë“¤ì˜ ë†ì—… í”¼í•´ í˜„í™© ë¶„ì„ ë³´ê³ ì„œ", "tool": "vector_db_search"}}
            ]
        }},
        {{
            "step": 3,
            "reasoning": "Step 2ê¹Œì§€ ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬, ì‚¬ìš©ìê°€ ìµœì¢…ì ìœ¼ë¡œ ìš”ì²­í•œ 'í”¼í•´ ì§€ì—­ë³„ ì£¼ìš” ì‹ì¬ë£Œ ë° ìƒì‚°ì§€' ëª©ë¡ì„ ì™„ì„±í•©ë‹ˆë‹¤.",
            "sub_questions": [
                {{"question": "[step-2ì˜ ê²°ê³¼]ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì§‘ì¤‘í˜¸ìš° í”¼í•´ ì§€ì—­ê³¼ í•´ë‹¹ ì§€ì—­ì˜ ì£¼ìš” ì‹ì¬ë£Œ ë° ìƒì‚°ì§€ë¥¼ ì—°ê²°í•˜ì—¬ í‘œ í˜•íƒœë¡œ ìš”ì•½", "tool": "vector_db_search"}}
            ]
        }}
    ]
}}

---
**## ìµœì¢… ì¶œë ¥ í¬ë§·**

**ì¤‘ìš” ê·œì¹™**:
- **ì§ˆë¬¸ ë³µì¡ì„±ì— ë”°ë¼ ì ì ˆí•œ ë„êµ¬ ê°œìˆ˜ ì„ íƒ**: ë‹¨ìˆœí•˜ë©´ 1ê°œ, ë³µí•©ì ì´ë©´ í•„ìš”í•œ ë§Œí¼ë§Œ
- **ìµœì‹  ì •ë³´** í¬í•¨ ì‹œ â†’ **web_search í¬í•¨**, ì¶”ê°€ë¡œ í•„ìš”í•œ ë¶„ì„/í†µê³„ ë„êµ¬ë§Œ ë³´ì™„
- **ì¼ë°˜ ì •ë³´** â†’ ë‚´ë¶€ DB(rdb, vector, graph, pubmed) ì¤‘ ê°€ì¥ ì í•©í•œ ê²ƒ ì„ íƒ
- **ê³¼ë„í•œ ë„êµ¬ ì‚¬ìš© ê¸ˆì§€**: ë¶ˆí•„ìš”í•œ ì¤‘ë³µ ê²€ìƒ‰ìœ¼ë¡œ ì„±ëŠ¥ ì €í•˜ ë°©ì§€
- **pubmed_search ì£¼ì˜ì‚¬í•­**: pubmedë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°, ì˜ì–´ë¡œ ì¿¼ë¦¬ë¥¼ ë²ˆì—­í•œ í›„ questionì— ë„£ì–´ì•¼ í•¨
- ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì•¼ í•©ë‹ˆë‹¤.

{{
    "title": "ë¶„ì„ ë³´ê³ ì„œì˜ ì „ì²´ ì œëª©",
    "reasoning": "ì´ëŸ¬í•œ ë‹¨ê³„ë³„ ê³„íšì„ ìˆ˜ë¦½í•œ í•µì‹¬ì ì¸ ì´ìœ .",
    "execution_steps": [
        {{
            "step": 1,
            "reasoning": "1ë‹¨ê³„ ê³„íšì— ëŒ€í•œ ì„¤ëª…. ë³‘ë ¬ ì‹¤í–‰ë  ì‘ì—…ë“¤ì„ ê¸°ìˆ .",
            "sub_questions": [
                {{
                    "question": "1ë‹¨ê³„ì—ì„œ ë³‘ë ¬ë¡œ ì‹¤í–‰í•  ì²« ë²ˆì§¸ í•˜ìœ„ ì§ˆë¬¸",
                    "tool": "ì„ íƒëœ ë„êµ¬ ì´ë¦„"
                }},
                {{
                    "question": "1ë‹¨ê³„ì—ì„œ ë³‘ë ¬ë¡œ ì‹¤í–‰í•  ë‘ ë²ˆì§¸ í•˜ìœ„ ì§ˆë¬¸",
                    "tool": "ì„ íƒëœ ë„êµ¬ ì´ë¦„"
                }}
            ]
        }},
        {{
            "step": 2,
            "reasoning": "2ë‹¨ê³„ ê³„íšì— ëŒ€í•œ ì„¤ëª…. 1ë‹¨ê³„ ê²°ê³¼ì— ì˜ì¡´í•¨ì„ ëª…ì‹œ.",
            "sub_questions": [
                {{
                    "question": "2ë‹¨ê³„ì—ì„œ ì‹¤í–‰í•  í•˜ìœ„ ì§ˆë¬¸ (í•„ìš”ì‹œ '[step-1ì˜ ê²°ê³¼]' í¬í•¨)",
                    "tool": "ì„ íƒëœ ë„êµ¬ ì´ë¦„"
                }}
            ]
        }}
    ]
}}
"""

        try:
            response = await self.llm.ainvoke(planning_prompt)
            content = response.content.strip()

            json_match = re.search(r'```json\s*(\{.*?\})\s*```', content, re.DOTALL)
            if not json_match:
                json_match = re.search(r'\{.*\}', content, re.DOTALL)

            if json_match:
                json_str = json_match.group(1) if '```' in json_match.group(0) else json_match.group(0)
                plan = json.loads(json_str)
            else:
                raise ValueError("Valid JSON plan not found in response")

            print(f"  - ì§€ëŠ¥í˜• ë‹¨ê³„ë³„ ê³„íš ìƒì„± ì™„ë£Œ: {plan.get('title', 'ì œëª© ì—†ìŒ')}")
            print("  - ê³„íš JSON:")
            print(json.dumps(plan, ensure_ascii=False, indent=2))
            state["plan"] = plan

        except Exception as e:
            print(f"  - ì§€ëŠ¥í˜• ê³„íš ìƒì„± ì‹¤íŒ¨, ë‹¨ì¼ ë‹¨ê³„ë¡œ ì§ì ‘ ê²€ìƒ‰ ì‹¤í–‰: {e}")
            state["plan"] = {
                "title": f"{query} ë¶„ì„",
                "reasoning": "ì§€ëŠ¥í˜• ë‹¨ê³„ë³„ ê³„íš ìˆ˜ë¦½ì— ì‹¤íŒ¨í•˜ì—¬, ì‚¬ìš©ì ì›ë³¸ ì¿¼ë¦¬ë¡œ ì§ì ‘ ê²€ìƒ‰ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.",
                "execution_steps": [{
                    "step": 1,
                    "reasoning": "Fallback ì‹¤í–‰",
                    "sub_questions": [{"question": query, "tool": "vector_db_search"}]
                }]
            }

        return state



    # â­ í•µì‹¬ ìˆ˜ì •: ìš”ì•½ëœ ë‚´ìš©ì´ ì•„ë‹Œ ì „ì²´ ì›ë³¸ ë‚´ìš©ì„ LLMì—ê²Œ ì œê³µ
    async def _select_relevant_data_for_step(self, step_info: Dict, current_collected_data: List[SearchResult], query: str) -> List[int]:
        """í˜„ì¬ ë‹¨ê³„ì—ì„œ ìˆ˜ì§‘ëœ ë°ì´í„° ì¤‘ ê´€ë ¨ ìˆëŠ” ê²ƒë§Œ LLMì´ ì„ íƒ (ì „ì²´ ë‚´ìš© ê¸°ë°˜)"""

        step_title = f"Step {step_info['step']}"
        step_reasoning = step_info.get('reasoning', '')
        sub_questions = step_info.get('sub_questions', [])

        # â­ í•µì‹¬ ê°œì„ : ì „ì²´ ì›ë³¸ ë‚´ìš©ì„ LLMì—ê²Œ ì œê³µ (ìš”ì•½ ì—†ì´)
        full_data_context = ""
        for i, res in enumerate(current_collected_data):
            source = getattr(res, 'source', 'Unknown')
            title = getattr(res, 'title', 'No Title')
            content = getattr(res, 'content', '')  # â­ ì „ì²´ ë‚´ìš© (ìš”ì•½ ì•ˆí•¨)

            full_data_context += f"""
    --- ë°ì´í„° ì¸ë±ìŠ¤ [{i}] ---
    ì¶œì²˜: {source}
    ì œëª©: {title}
    ì „ì²´ ë‚´ìš©: {content}

    """

        # í˜„ì¬ ë‹¨ê³„ì˜ ì§ˆë¬¸ë“¤
        questions_summary = ""
        for sq in sub_questions:
            questions_summary += f"- {sq.get('question', '')} ({sq.get('tool', '')})\n"

        # â­ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ê´€ë¦¬ (ì¤‘ìš”í•œ ë¶€ë¶„ë§Œ ì˜ë¼ë‚´ê¸°)
        # ë„ˆë¬´ ê¸¸ë©´ ê° ë°ì´í„°ë‹¹ ìµœëŒ€ 1000ìë¡œ ì œí•œ
        if len(full_data_context) > 15000:
            print(f"  - ì»¨í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸º ({len(full_data_context)}ì), ë°ì´í„°ë³„ 1000ìë¡œ ì œí•œ")

            truncated_context = ""
            for i, res in enumerate(current_collected_data):
                source = getattr(res, 'source', 'Unknown')
                title = getattr(res, 'title', 'No Title')
                content = getattr(res, 'content', '')[:1000]  # 1000ìë¡œ ì œí•œ

                truncated_context += f"""
    --- ë°ì´í„° ì¸ë±ìŠ¤ [{i}] ---
    ì¶œì²˜: {source}
    ì œëª©: {title}
    ë‚´ìš©: {content}{"..." if len(getattr(res, 'content', '')) > 1000 else ""}

    """
            full_data_context = truncated_context

        selection_prompt = f"""
    ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    ìˆ˜ì§‘ëœ ë°ì´í„° ì¤‘ì—ì„œ **ì°¨íŠ¸ ìƒì„±ê³¼ ë³´ê³ ì„œ ì‘ì„±ì— í•„ìš”í•œ ë°ì´í„°**ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì„ íƒí•´ì£¼ì„¸ìš”.

    **ì „ì²´ ì‚¬ìš©ì ì§ˆë¬¸**: "{query}"

    **í˜„ì¬ ë‹¨ê³„ ì •ë³´**:
    - {step_title}: {step_reasoning}
    - ì‹¤í–‰í•œ ì§ˆë¬¸ë“¤:
    {questions_summary}

    **ìˆ˜ì§‘ëœ ì „ì²´ ë°ì´í„°** (ì „ì²´ ë‚´ìš© í¬í•¨):
    {full_data_context}

    **ì„ íƒ ê¸°ì¤€**:
    1. **ì°¨íŠ¸ ìƒì„±ìš© ìˆ˜ì¹˜/í†µê³„ ë°ì´í„° ìµœìš°ì„  ì„ íƒ**:
       - ë§¤ì¶œì•¡, ì‹œì¥ê·œëª¨, ì ìœ ìœ¨, ìƒì‚°ëŸ‰, ê°€ê²© ë“± ìˆ˜ì¹˜ ë°ì´í„°
       - %, ì–µì›, ì¡°ì›, ì²œí†¤ ë“± ë‹¨ìœ„ê°€ í¬í•¨ëœ ë°ì´í„°
       - í‘œ, í†µê³„, í˜„í™©í‘œ, ìˆœìœ„, ë¹„êµ, ë¶„ì„ ë°ì´í„°
       - ì§€ì—­ë³„/í’ˆëª©ë³„/ì‹œê¸°ë³„ ë¹„êµ ë°ì´í„°
       - ì—°ë„ë³„, ì›”ë³„ ì‹œê³„ì—´ ë°ì´í„°
    2. **ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ë°ì´í„°**
    3. **ë°°ê²½ ì •ë³´ ë° ì»¨í…ìŠ¤íŠ¸ ë°ì´í„°** (ì¤‘ìš”í•œ ê²ƒë§Œ)
    4. **ì¤‘ë³µ ì œê±°**: ì™„ì „íˆ ë™ì¼í•œ ë‚´ìš©ì€ í•˜ë‚˜ë§Œ ì„ íƒ

    **ì œì™¸ ê¸°ì¤€**:
    - ì§ˆë¬¸ê³¼ ì „í˜€ ê´€ë ¨ ì—†ëŠ” ì£¼ì œ
    - ì™„ì „íˆ ì¤‘ë³µë˜ëŠ” ë‚´ìš© (ìœ ì‚¬í•˜ì§€ë§Œ ë‹¤ë¥¸ ê´€ì ì´ë©´ í¬í•¨)
    - ê´‘ê³ ì„± ë‚´ìš© (ë‹¨, ì‹œì¥ ë°ì´í„° í¬í•¨ì‹œ ì„ íƒ)
    - ì¼ë°˜ì ì¸ ì„¤ëª…ë§Œ ìˆê³  êµ¬ì²´ì  ë°ì´í„°ê°€ ì—†ëŠ” ë‚´ìš©

    **ëª©í‘œ**: ì°¨íŠ¸ ìƒì„±ì— í•„ìš”í•œ ì¶©ë¶„í•œ ë°ì´í„° í™•ë³´ (í’ˆì§ˆ ì¤‘ì‹¬ ì„ íƒ)

    ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
    {{
        "selected_indexes": [ì„ íƒëœ ì¸ë±ìŠ¤ë“¤],
        "reasoning": "ì„ íƒëœ ê° ë°ì´í„°ê°€ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ì°¨íŠ¸ ìƒì„±ì— ì–´ë–»ê²Œ ê¸°ì—¬í•˜ëŠ”ì§€ ì„¤ëª…",
        "rejected_reason": "ì œì™¸ëœ ë°ì´í„°ë“¤ì˜ ì œì™¸ ì´ìœ "
    }}
    """

        try:
            response = await self._invoke_with_fallback(
                selection_prompt,
                self.llm,
                self.llm_openai_mini
            )

            # JSON íŒŒì‹±
            result = json.loads(re.search(r'\{.*\}', response.content, re.DOTALL).group())
            selected_indexes = result.get("selected_indexes", [])
            reasoning = result.get("reasoning", "")
            rejected_reason = result.get("rejected_reason", "")

            # ì¸ë±ìŠ¤ ìœ íš¨ì„± ê²€ì¦
            max_index = len(current_collected_data) - 1
            valid_indexes = [idx for idx in selected_indexes if isinstance(idx, int) and 0 <= idx <= max_index]

            # â­ í•µì‹¬ ìˆ˜ì •: ì°¨íŠ¸ ìƒì„±ìš© ìµœì†Œ ë°ì´í„° í™•ë³´
            total_available = len(current_collected_data)
            min_selection = max(3, min(6, total_available))  # ìµœì†Œ 3ê°œ, ìµœëŒ€ 6ê°œ (ì „ì²´ ê°œìˆ˜ ê³ ë ¤)

            if len(valid_indexes) < min_selection:
                print(f"  - âš ï¸ ì°¨íŠ¸ ìƒì„±ì„ ìœ„í•´ ìµœì†Œ {min_selection}ê°œ ë°ì´í„°ê°€ í•„ìš”í•˜ì§€ë§Œ {len(valid_indexes)}ê°œë§Œ ì„ íƒë¨")
                # ì„ íƒë˜ì§€ ì•Šì€ ì¸ë±ìŠ¤ë“¤ ì¤‘ì—ì„œ ì¶”ê°€ ì„ íƒ (ì²« ë²ˆì§¸ë¶€í„° ìˆœì„œëŒ€ë¡œ)
                unselected = [i for i in range(total_available) if i not in valid_indexes]
                additional_needed = min_selection - len(valid_indexes)
                additional_selected = unselected[:additional_needed]
                valid_indexes.extend(additional_selected)
                valid_indexes = sorted(valid_indexes)
                print(f"  - ğŸ”§ ì¶”ê°€ ì„ íƒëœ ì¸ë±ìŠ¤: {additional_selected}")

            print(f"  - LLM ë°ì´í„° ì„ íƒ ì™„ë£Œ:")
            print(f"    ì„ íƒëœ ì¸ë±ìŠ¤: {valid_indexes} (ì´ {len(valid_indexes)}/{total_available}ê°œ)")
            print(f"    ì„ íƒ ì´ìœ : {reasoning}")
            if rejected_reason:
                print(f"    ì œì™¸ ì´ìœ : {rejected_reason}")

            # ì„ íƒëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
            print(f"  - ì„ íƒëœ ë°ì´í„° ëª©ë¡:")
            for idx in valid_indexes:
                data_item = current_collected_data[idx]
                title = getattr(data_item, 'title', 'No Title')[:60]
                source = getattr(data_item, 'source', 'Unknown')
                print(f"    [{idx:2d}] {source:10s} | {title}")

            return valid_indexes

        except Exception as e:
            print(f"  - LLM ë°ì´í„° ì„ íƒ ì‹¤íŒ¨: {e}")
            # fallback: í˜„ì¬ ë‹¨ê³„ì—ì„œ ìˆ˜ì§‘ëœ ëª¨ë“  ë°ì´í„° ìœ ì§€
            return list(range(len(current_collected_data)))



    async def execute_report_workflow(self, state: StreamingAgentState) -> AsyncGenerator[str, None]:
        """ë‹¨ê³„ë³„ ê³„íšì— ë”°ë¼ ìˆœì°¨ì , ë³‘ë ¬ì ìœ¼ë¡œ ë°ì´í„° ìˆ˜ì§‘ ë° ë³´ê³ ì„œ ìƒì„±"""
        query = state["original_query"]

        # ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ìš”ì•½ ìƒì„±
        conversation_history = state.get("metadata", {}).get("conversation_history", [])
        conversation_id = state.get("conversation_id", "unknown")
        memory_summary = self._generate_memory_summary_for_report(conversation_history, query)

        if memory_summary:
            print(f"ğŸ§  ì±„íŒ…ë°© {conversation_id}: ë³´ê³ ì„œì— ë©”ëª¨ë¦¬ ìš”ì•½ í¬í•¨ ({len(conversation_history)}ê°œ ë©”ì‹œì§€)")
        else:
            print(f"ğŸ§  ì±„íŒ…ë°© {conversation_id}: ë©”ëª¨ë¦¬ ì—†ìŒ (ìƒˆ ëŒ€í™” ë˜ëŠ” ì—°ì†ì„± ì—†ìŒ)")

        # --- ì¶”ê°€: í˜ë¥´ì†Œë‚˜ í™•ì¸ ë° ìƒíƒœ ì•Œë¦¼ ---
        # ì‚¬ìš©ìê°€ ì„ íƒí•œ í˜ë¥´ì†Œë‚˜ê°€ stateì— ì´ë¯¸ í¬í•¨ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
        # ì˜ˆ: state['persona'] = 'êµ¬ë§¤ ë‹´ë‹¹ì'
        selected_persona = state.get("persona")
        if not selected_persona or selected_persona not in self.personas:
            print(f"ê²½ê³ : ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ ì§€ì •ë˜ì§€ ì•Šì€ í˜ë¥´ì†Œë‚˜ ('{selected_persona}'). 'ê¸°ë³¸'ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
            selected_persona = "ê¸°ë³¸"
            state["persona"] = selected_persona

        yield self._create_status_event("PLANNING", "PERSONA_CONFIRMED", f"'{selected_persona}' í˜ë¥´ì†Œë‚˜ë¡œ ë³´ê³ ì„œ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

        # ì°¨íŠ¸ ì¹´ìš´í„° ì´ˆê¸°í™”
        state['chart_counter'] = 0

        # ëˆ„ì  context ì´ˆê¸°í™”
        accumulated_context = {
            "generated_sections": [],  # ìƒì„±ëœ ì„¹ì…˜ ë‚´ìš©ë“¤
            "chart_data": [],  # ìƒì„±ëœ ì°¨íŠ¸ ë°ì´í„°ë“¤
            "insights": [],  # ê° ì„¹ì…˜ì˜ ì£¼ìš” ì¸ì‚¬ì´íŠ¸
            "persona": selected_persona  # ì„ íƒëœ í˜ë¥´ì†Œë‚˜
        }
        state['accumulated_context'] = accumulated_context

        # 1. ë‹¨ê³„ë³„ ê³„íš ìˆ˜ë¦½
        yield self._create_status_event("PLANNING", "GENERATE_PLAN_START", "ë¶„ì„ ê³„íš ìˆ˜ë¦½ ì¤‘...")
        state_with_plan = await self.generate_plan(state)
        plan = state_with_plan.get("plan", {})

        yield {"type": "plan", "data": {"plan": plan}}

        yield self._create_status_event("PLANNING", "GENERATE_PLAN_COMPLETE", "ë¶„ì„ ê³„íš ìˆ˜ë¦½ ì™„ë£Œ.", details={
            "plan_title": plan.get('title'),
            "plan_reasoning": plan.get('reasoning'),
            "step_count": len(plan.get("execution_steps", []))
        })

        await asyncio.sleep(0.01)

        # 2. ë‹¨ê³„ë³„ ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰
        execution_steps = plan.get("execution_steps", [])
        final_collected_data: List[SearchResult] = []
        step_results_context: Dict[int, str] = {}
        cumulative_selected_indexes: List[int] = []


        for i, step_info in enumerate(execution_steps):
            current_step_index = step_info["step"]
            yield self._create_status_event("GATHERING", "STEP_START", f"ë°ì´í„° ìˆ˜ì§‘ ({i + 1}/{len(execution_steps)}) ì‹œì‘.")

            tasks_for_this_step = []
            for sq in step_info.get("sub_questions", []):
                injected_query = self._inject_context_into_query(sq["question"], step_results_context)
                tasks_for_this_step.append({"tool": sq["tool"], "inputs": {"query": injected_query}})
            if not tasks_for_this_step:
                continue

            # step_collected_dataë¥¼ SearchResult ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
            step_collected_data: List[SearchResult] = []

            async for event in self.data_gatherer.execute_parallel_streaming(tasks_for_this_step, state=state):
                if event["type"] == "search_results":
                    yield event
                elif event["type"] == "collection_complete":
                    # workerë¡œë¶€í„° ë°›ì€ dict ë¦¬ìŠ¤íŠ¸ë¥¼ ë‹¤ì‹œ SearchResult ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
                    collected_dicts = event["data"]["collected_data"]
                    step_collected_data = [SearchResult(**data_dict) for data_dict in collected_dicts]

            # ì¤‘ë³µ ì œê±° ì—†ì´ ì›ë³¸ ë°ì´í„° ì‚¬ìš©
            unique_step_data = step_collected_data

            # ì´ì œ unique_step_dataëŠ” List[SearchResult] íƒ€ì…ì´ë¯€ë¡œ .content ì ‘ê·¼ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
            summary_of_step = " ".join([res.content for res in unique_step_data])
            step_results_context[current_step_index] = summary_of_step[:2000]
            final_collected_data.extend(unique_step_data)

            print(f">> {current_step_index}ë‹¨ê³„ ì™„ë£Œ: {len(step_collected_data)}ê°œ ìˆ˜ì§‘ â†’ {len(unique_step_data)}ê°œ ìœ ì§€ (ì´ {len(final_collected_data)}ê°œ)")

            if len(final_collected_data) > 0:
                yield self._create_status_event("PROCESSING", "FILTER_DATA_START", "ìˆ˜ì§‘ ë°ì´í„° ì„ ë³„ ì¤‘...")

                # reasoningì„ ë°˜í™˜í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ selected_indexesë§Œ ë°›ìŠµë‹ˆë‹¤.
                selected_indexes = await self._select_relevant_data_for_step(
                    step_info, final_collected_data, state["original_query"]
                )

                yield self._create_status_event("PROCESSING", "FILTER_DATA_COMPLETE", f"í•µì‹¬ ë°ì´í„° {len(selected_indexes)}ê°œ ì„ ë³„ ì™„ë£Œ.", details={
                    "selected_indices": selected_indexes
                })
                cumulative_selected_indexes = sorted(list(set(cumulative_selected_indexes + selected_indexes)))

        # >> í•µì‹¬ ìˆ˜ì •: ì „ì²´ ë°ì´í„° ë”•ì…”ë„ˆë¦¬ë¥¼ í”„ë¡ íŠ¸ë¡œ ë¨¼ì € ì „ì†¡
        print(f"\n>> ì „ì²´ ë°ì´í„° ë”•ì…”ë„ˆë¦¬ ìƒì„± ë° ì „ì†¡")

        # ì „ì²´ ë°ì´í„°ë¥¼ ì¸ë±ìŠ¤:ë°ì´í„° í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        full_data_dict = {}
        print(f"\nğŸ” === FULL_DATA_DICT ìƒì„± ë””ë²„ê¹… ===")
        print(f"final_collected_data ì´ ê°œìˆ˜: {len(final_collected_data)}")

        for idx, data in enumerate(final_collected_data):
            full_data_dict[idx] = {
                "title": getattr(data, 'title', 'No Title'),
                "content": getattr(data, 'content', ''),
                "source": getattr(data, 'source', 'Unknown'),
                "url": getattr(data, 'url', ''),
                "source_url": getattr(data, 'source_url', ''),
                "score": getattr(data, 'score', 0.0),
                "document_type": getattr(data, 'document_type', 'unknown')
            }

            # ì²« 5ê°œì™€ ë§ˆì§€ë§‰ 5ê°œë§Œ ìƒì„¸ ë¡œê·¸
            if idx < 5 or idx >= len(final_collected_data) - 5:
                print(f"  [{idx}]: ì œëª©='{getattr(data, 'title', 'No Title')[:50]}...' ì¶œì²˜='{getattr(data, 'source', 'Unknown')}'")

        print(f"ì „ì²´ ë°ì´í„° ë”•ì…”ë„ˆë¦¬ í‚¤ë“¤: {list(full_data_dict.keys())}")
        print(f"ì „ì²´ ë°ì´í„° ë”•ì…”ë„ˆë¦¬ í¬ê¸°: {len(full_data_dict)}ê°œ")

        # ì „ì²´ ë°ì´í„° ë”•ì…”ë„ˆë¦¬ë¥¼ í”„ë¡ íŠ¸ë¡œ ì „ì†¡
        print(f"\nğŸš€ === í”„ë¡ íŠ¸ì—”ë“œë¡œ FULL_DATA_DICT ì „ì†¡ ===")
        print(f"ì „ì†¡í•  ë°ì´í„° êµ¬ì¡°:")
        print(f"  type: 'full_data_dict'")
        print(f"  data.data_dict í‚¤ë“¤: {list(full_data_dict.keys())}")
        print(f"  data.data_dict í¬ê¸°: {len(full_data_dict)}")

        # ìƒ˜í”Œ ë°ì´í„° í™•ì¸ (ì²« ë²ˆì§¸ ê²ƒë§Œ)
        if full_data_dict:
            first_key = list(full_data_dict.keys())[0]
            first_item = full_data_dict[first_key]
            print(f"  ìƒ˜í”Œ [{first_key}]: ì œëª©='{first_item['title'][:30]}...' ì¶œì²˜='{first_item['source']}'")

        yield {"type": "full_data_dict", "data": {"data_dict": full_data_dict}}

        # 3. ì„¹ì…˜ë³„ ë°ì´í„° ìƒíƒœ ë¶„ì„ ë° ë³´ê³ ì„œ êµ¬ì¡° ì„¤ê³„
        # ì¤‘ë³µëœ full_data_dict ìƒì„± ë° ì „ì†¡ ì œê±° (ì´ë¯¸ ìœ„ì—ì„œ ì²˜ë¦¬ë¨)

        yield self._create_status_event("PROCESSING", "DESIGN_STRUCTURE_START", "ë³´ê³ ì„œ êµ¬ì¡° ì„¤ê³„ ì¤‘...")

        design = None
        async for result in self.processor.process("design_report_structure", final_collected_data, cumulative_selected_indexes, query, state=state):
            if result.get("type") == "result":
                design = result.get("data")
                break

        if not design or "structure" not in design or not design["structure"]:
            yield {"type": "error", "data": {"message": "ë³´ê³ ì„œ êµ¬ì¡° ì„¤ê³„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."}}
            return

        # â­ í•µì‹¬ ì¶”ê°€: ë°ì´í„° ë¶€ì¡± ì„¹ì…˜ í™•ì¸ ë° ì¶”ê°€ ê²€ìƒ‰
        insufficient_sections = []
        for i, section in enumerate(design.get("structure", [])):
            if not section.get("is_sufficient", True):
                insufficient_sections.append({"original_index": i, "section_info": section})

        if insufficient_sections:
            print(f"ğŸ” ë°ì´í„° ë¶€ì¡± ì„¹ì…˜ {len(insufficient_sections)}ê°œ ë°œê²¬, ì¶”ê°€ ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰")
            yield self._create_status_event("GATHERING", "ADDITIONAL_SEARCH_START", f"{len(insufficient_sections)}ê°œ í•­ëª©ì— ëŒ€í•œ ë°ì´í„° ë³´ê°•ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

            additional_tasks = []
            for item in insufficient_sections:
                section = item["section_info"]
                feedback = section.get("feedback_for_gatherer", {})
                if isinstance(feedback, dict) and feedback:
                    tool = feedback.get("tool", "vector_db_search")
                    query_to_run = feedback.get("query", f"{section.get('section_title', '')} ìƒì„¸ ë°ì´í„°")
                    additional_tasks.append({"tool": tool, "inputs": {"query": query_to_run}})

            additional_data_collected_objects: List[SearchResult] = []
            if additional_tasks:
                async for event in self.data_gatherer.execute_parallel_streaming(additional_tasks, state=state):
                    if event["type"] == "search_results":
                        yield event
                    elif event["type"] == "collection_complete":
                        additional_data_dicts = event["data"]["collected_data"]
                        for data_dict in additional_data_dicts:
                            additional_data_collected_objects.append(SearchResult(**data_dict))

            # if ë¬¸ì˜ ì¡°ê±´ ë³€ìˆ˜ë¥¼ additional_data_collected_objectsë¡œ ë³€ê²½í•©ë‹ˆë‹¤.
            if additional_data_collected_objects:
                print(f"âœ… ì´ {len(additional_data_collected_objects)}ê°œ ì¶”ê°€ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ. ë³´ê³ ì„œ êµ¬ì¡°ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.")
                yield self._create_status_event("PROCESSING", "DATA_ENHANCED", f"{len(additional_data_collected_objects)}ê°œì˜ ì¶”ê°€ ë°ì´í„°ë¡œ ë³´ê³ ì„œ êµ¬ì¡°ë¥¼ ë³´ê°•í•©ë‹ˆë‹¤.")

                original_data_count = len(final_collected_data)
                new_data_indexes = list(range(original_data_count, original_data_count + len(additional_data_collected_objects)))
                # final_collected_dataì— ì¶”ê°€í•˜ëŠ” ë³€ìˆ˜ë„ ë³€ê²½í•©ë‹ˆë‹¤.
                final_collected_data.extend(additional_data_collected_objects)

                for item in insufficient_sections:
                    section_index = item["original_index"]
                    section_info = item["section_info"]
                    original_indexes = section_info.get("use_contents", [])

                    updated_use_contents = await self._update_use_contents_after_recollection(
                        section_info, final_collected_data, original_indexes, new_data_indexes, query
                    )

                    design["structure"][section_index]["use_contents"] = updated_use_contents
                    design["structure"][section_index]["is_sufficient"] = True
                    design["structure"][section_index]["feedback_for_gatherer"] = ""

                print(f"ğŸ”„ ë°ì´í„° ë³´ê°• ë° êµ¬ì¡° ì—…ë°ì´íŠ¸ ì™„ë£Œ. ìµœì¢… ë°ì´í„° ìˆ˜: {len(final_collected_data)}ê°œ")

            else:
                print(f"âš ï¸ ì¶”ê°€ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨, ê¸°ì¡´ ë°ì´í„°ë¡œ ì§„í–‰")

        # 5. ëª¨ë“  ë°ì´í„° ìˆ˜ì§‘/ë³´ê°•ì´ ëë‚œ í›„, ìµœì¢… ë°ì´í„° ëª©ë¡ìœ¼ë¡œ full_data_dict ìƒì„± ë° ì „ì†¡
        print(f"\n>> ìµœì¢… ë°ì´í„° ëª©ë¡ìœ¼ë¡œ ë”•ì…”ë„ˆë¦¬ ìƒì„± ë° ì „ì†¡ (ì´ {len(final_collected_data)}ê°œ)")
        full_data_dict = {}
        for idx, data in enumerate(final_collected_data):
            # SearchResult ê°ì²´ë¥¼ JSONìœ¼ë¡œ ë³€í™˜ ê°€ëŠ¥í•œ dictë¡œ ë³€í™˜
            full_data_dict[idx] = data.model_dump()

        yield {"type": "full_data_dict", "data": {"data_dict": full_data_dict}}

        section_titles = [s.get('section_title', 'ì œëª© ì—†ìŒ') for s in design.get('structure', [])]
        yield self._create_status_event("PROCESSING", "DESIGN_STRUCTURE_COMPLETE", "ë³´ê³ ì„œ êµ¬ì¡° ì„¤ê³„ ì™„ë£Œ.", details={
            "report_title": design.get("title"),
            "section_titles": section_titles
        })

        # ë³´ê³ ì„œ ì œëª©ê³¼ ë©”ëª¨ë¦¬ ìš”ì•½ì„ ê°€ì¥ ë¨¼ì € ìŠ¤íŠ¸ë¦¬ë°
        report_start = f"# {design.get('title', query)}\n\n"

        # ë©”ëª¨ë¦¬ ìš”ì•½ì´ ìˆìœ¼ë©´ í¬í•¨
        if memory_summary:
            report_start += memory_summary

        report_start += "---\n\n"

        yield {"type": "content", "data": {"chunk": report_start}}

        # 4. ì „ì²´ ë³´ê³ ì„œ êµ¬ì¡° ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        # ê° ì„¹ì…˜ ìƒì„± Agentê°€ ì „ì²´ ê·¸ë¦¼ì„ ì¸ì§€í•˜ë„ë¡ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë§Œë“¤ì–´ ì „ë‹¬í•©ë‹ˆë‹¤.
        awareness_context = "ì•„ë˜ëŠ” ì „ì²´ ë³´ê³ ì„œì˜ ëª©ì°¨ì…ë‹ˆë‹¤. ë‹¹ì‹ ì€ ì´ ì¤‘ í•˜ë‚˜ì˜ ì„¹ì…˜ ì‘ì„±ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.\n\n"

        # ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        if conversation_history:
            memory_context = self._build_memory_context(conversation_history)
            if memory_context:
                awareness_context += memory_context + "\n"

        structure = design.get("structure", [])
        for i, sec in enumerate(structure):
            awareness_context += f"- **ì„¹ì…˜ {i+1}. {sec.get('section_title', '')}**: {sec.get('description', '')}\n"

        # 5. ê° ì„¹ì…˜ì˜ ê²°ê³¼ë¥¼ ë‹´ì„ Queue ë¦¬ìŠ¤íŠ¸ì™€ ë³‘ë ¬ ì‹¤í–‰í•  Task ë¦¬ìŠ¤íŠ¸ ìƒì„±
        section_queues = [asyncio.Queue() for _ in structure]
        producer_tasks = []

        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ê° ì„¹ì…˜ ë‚´ìš©ì„ ìƒì„±í•˜ê³  Queueì— ë„£ëŠ” ì½”ë£¨í‹´
        async def _produce_section_content(section_index: int):
            section_info = structure[section_index]
            q = section_queues[section_index]

            use_contents = section_info.get("use_contents", [])

            try:
                # generate_section_streamingì„ í˜¸ì¶œí•˜ì—¬ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì²­í¬ë¥¼ ë°›ìŒ
                async for chunk in self.processor.generate_section_streaming(
                    section_info, full_data_dict, query, use_contents,
                    awareness_context=awareness_context,
                    state=state
                ):
                    await q.put(chunk)  # ë°›ì€ ì²­í¬ë¥¼ íì— ë„£ìŒ
            except Exception as e:
                # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ íì— ë„£ìŒ
                error_message = f"*'{section_info.get('section_title', '')}' ì„¹ì…˜ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}*\n\n"
                await q.put(error_message)
                print(f">> ì„¹ì…˜ ìƒì„±(Producer) ì˜¤ë¥˜: {error_message}")
            finally:
                # ìŠ¤íŠ¸ë¦¼ì´ ëë‚˜ë©´ Noneì„ ë„£ì–´ ì¢…ë£Œë¥¼ ì•Œë¦¼
                await q.put(None)

        # ëª¨ë“  ì„¹ì…˜ì— ëŒ€í•œ Producer Taskë¥¼ ìƒì„±í•˜ê³  ì‹¤í–‰ ëª©ë¡ì— ì¶”ê°€
        for i in range(len(structure)):
            task = asyncio.create_task(_produce_section_content(i))
            producer_tasks.append(task)

        # 6. ìˆœì°¨ì ìœ¼ë¡œ Queueì—ì„œ ê²°ê³¼ë¥¼ êº¼ë‚´ ìŠ¤íŠ¸ë¦¬ë° (Consumer)
        for i, section in enumerate(structure):
            section_title = section.get('section_title', f'ì„¹ì…˜ {i+1}')
            use_contents = section.get("use_contents", [])

            yield self._create_status_event("GENERATING", "GENERATE_SECTION_START", f"'{section_title}' ì„¹ì…˜ ìƒì„± ì¤‘...", details={
                "section_index": i, "section_title": section_title, "using_indices": use_contents
            })

            buffer = ""
            # âœ… section_full_content ë³€ìˆ˜ ì´ˆê¸°í™” ì¶”ê°€
            section_full_content = ""
            section_data_list = [final_collected_data[idx] for idx in use_contents if 0 <= idx < len(final_collected_data)]
             # âœ… section_content_generated ë³€ìˆ˜ ì¶”ê°€ (ìƒì„± ì‹¤íŒ¨ ì—¬ë¶€ í™•ì¸ìš©)
            section_content_generated = False

            # í•´ë‹¹ ì„¹ì…˜ì˜ Queueì—ì„œ ê²°ê³¼ê°€ ë‚˜ì˜¬ ë•Œê¹Œì§€ ëŒ€ê¸°
            while True:
                chunk = await section_queues[i].get()

                # Noneì„ ë°›ìœ¼ë©´ í•´ë‹¹ ì„¹ì…˜ ìŠ¤íŠ¸ë¦¬ë°ì´ ëë‚œ ê²ƒ
                if chunk is None:
                    break

                # âœ… ì²­í¬ë¥¼ ë°›ì•˜ë‹¤ë©´ ë‚´ìš©ì´ ìƒì„±ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼
                section_content_generated = True
                buffer += chunk
                # âœ… ì „ì²´ ì„¹ì…˜ ë‚´ìš©ë„ ë³„ë„ë¡œ ëˆ„ì 
                section_full_content += chunk

                # ì°¨íŠ¸ ìƒì„± ë§ˆì»¤ê°€ ìˆëŠ”ì§€ í™•ì¸
                if "[GENERATE_CHART]" in buffer:
                    parts = buffer.split("[GENERATE_CHART]", 1)

                    if parts[0]:
                        yield {"type": "content", "data": {"chunk": parts[0]}}

                    buffer = parts[1]

                    yield self._create_status_event("GENERATING", "GENERATE_CHART_START", f"'{section_title}' ì°¨íŠ¸ ìƒì„± ì¤‘...")

                    # ì°¨íŠ¸ ìƒì„± ê³¼ì •ì˜ ìƒíƒœ ë©”ì‹œì§€ë¥¼ ìœ„í•œ ì½œë°±
                    async def chart_yield_callback(event_data):
                        print(f"ì°¨íŠ¸ ìƒì„± ìƒíƒœ: {event_data}")
                        return event_data

                    # ì´ì „ê¹Œì§€ ìƒì„±ëœ contextë¥¼ ì°¨íŠ¸ ìƒì„±ì— ì „ë‹¬
                    chart_context = {
                        "previous_sections": accumulated_context["generated_sections"],
                        "previous_charts": accumulated_context["chart_data"],
                        "current_section_content": buffer
                    }
                    state['chart_context'] = chart_context

                    chart_data = None
                    async for result in self.processor.process("create_chart_data", section_data_list, section_title, buffer, "", chart_yield_callback, state=state):
                        if result.get("type") == "chart":
                            chart_data = result.get("data")
                            accumulated_context["chart_data"].append({
                                "section": section_title,
                                "chart": chart_data
                            })

                            # ì°¨íŠ¸ ìƒì„± ê²€ì¦ ë¡œê·¸ ê¸°ë¡ (ì„¹ì…˜ description í¬í•¨)
                            section_description = section.get('description', 'ì„¤ëª… ì—†ìŒ')
                            await self._log_chart_verification(query, section_title, section_description, section_data_list, chart_data, state)
                            break

                    if chart_data and "error" not in chart_data:
                        current_chart_index = state.get('chart_counter', 0)
                        chart_placeholder = f"\n\n[CHART-PLACEHOLDER-{current_chart_index}]\n\n"
                        yield {"type": "content", "data": {"chunk": chart_placeholder}}
                        yield {"type": "chart", "data": chart_data}
                        state['chart_counter'] = current_chart_index + 1
                    else:
                        print(f"   ì°¨íŠ¸ ìƒì„± ì‹¤íŒ¨: {chart_data}")
                        yield self._create_status_event("GENERATING", "GENERATE_CHART_FAILURE", f"'{section_title}' ì°¨íŠ¸ ìƒì„±ì´ ì™„ë£Œë˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                        yield {"type": "content", "data": {"chunk": "\n\n*[ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ì°¨íŠ¸ í‘œì‹œê°€ ì œí•œë©ë‹ˆë‹¤]*\n\n"}}

                else:
                    # í…ìŠ¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ë²„í¼ ê´€ë¦¬
                    potential_chart_marker = "[GENERATE_CHART]"
                    # ë²„í¼ ëì— ë§ˆì»¤ ì¼ë¶€ê°€ ê±¸ì³ìˆëŠ”ì§€ í™•ì¸
                    has_partial_marker = any(potential_chart_marker.startswith(buffer[-j:]) for j in range(1, min(len(buffer) + 1, len(potential_chart_marker) + 1)))

                    should_flush = (
                        not has_partial_marker and (
                            len(buffer) >= 120 or
                            buffer.endswith(('.', '!', '?', '\n', 'ë‹¤.', 'ìš”.', 'ë‹ˆë‹¤.', 'ìŠµë‹ˆë‹¤.', 'ë©ë‹ˆë‹¤.', 'ìˆìŠµë‹ˆë‹¤.')) or
                            '\n\n' in buffer
                        )
                    )

                    if should_flush:
                        yield {"type": "content", "data": {"chunk": buffer}}
                        buffer = ""

            # while ë£¨í”„ ì¢…ë£Œ í›„ ë‚¨ì€ ë²„í¼ ì²˜ë¦¬
            if buffer.strip():
                yield {"type": "content", "data": {"chunk": buffer}}

            # ìƒì„±ëœ ì„¹ì…˜ ë‚´ìš©ì„ ëˆ„ì  contextì— ì¶”ê°€
            if section_full_content:
                accumulated_context["generated_sections"].append({
                    "title": section_title,
                    "content": section_full_content,
                    "data_indices": use_contents
                })
                # ì£¼ìš” ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ
                first_paragraph = section_full_content.split("\n\n")[0] if "\n\n" in section_full_content else section_full_content[:200]
                accumulated_context["insights"].append({
                    "section": section_title,
                    "insight": first_paragraph
                })

                # ì„¹ì…˜ ìƒì„± ê²€ì¦ ë¡œê·¸ ê¸°ë¡ (ì„¹ì…˜ description í¬í•¨)
                section_description = section.get('description', 'ì„¤ëª… ì—†ìŒ')
                await self._log_section_verification(query, section_title, section_description, section_data_list, section_full_content, state)

            # ë‚´ìš©ì´ ì „í˜€ ìƒì„±ë˜ì§€ ì•Šì€ ê²½ìš° ê²½ê³  ì²˜ë¦¬
            if not section_content_generated:
                print(f">> ê²½ê³ : ì„¹ì…˜ '{section_title}' ë‚´ìš© ìƒì„± ì‹¤íŒ¨")
                yield {"type": "content", "data": {"chunk": f"*'{section_title}' ì„¹ì…˜ ìƒì„± ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.*\n\n"}}

            # ê° ì„¹ì…˜ ì‚¬ì´ì— ê³µë°± ì¶”ê°€
            yield {"type": "content", "data": {"chunk": "\n\n"}}

        # ì›Œí¬í”Œë¡œìš° ì™„ë£Œ í›„ ì¶œì²˜ ì •ë³´ ì„¤ì • (ì‹¤ì œ ì‚¬ìš©ëœ ì¸ë±ìŠ¤ë§Œ)
        # ëª¨ë“  ì„¹ì…˜ì—ì„œ ì‚¬ìš©ëœ ì¸ë±ìŠ¤ë“¤ì„ ìˆ˜ì§‘
        used_indexes = set()
        for section in design.get("structure", []):
            use_contents = section.get("use_contents", [])
            used_indexes.update(use_contents)

        print(f">> ì‹¤ì œ ì‚¬ìš©ëœ ì¸ë±ìŠ¤ë“¤: {sorted(used_indexes)}")
        print(f">> final_collected_data ê¸¸ì´: {len(final_collected_data) if final_collected_data else 0}")
        print(f">> used_indexes ê¸¸ì´: {len(used_indexes)}")

        # sources ì´ë²¤íŠ¸ëŠ” ë” ì´ìƒ ë³´ë‚´ì§€ ì•ŠìŒ (full_data_dictë§Œ ì‚¬ìš©)
        # ëŒ€ì‹  ì‚¬ìš©ëœ ì¸ë±ìŠ¤ ì •ë³´ë§Œ ë¡œê¹…
        if final_collected_data and used_indexes:
            print(f">> ë³´ê³ ì„œì—ì„œ ì‹¤ì œ ì‚¬ìš©ëœ ì¸ë±ìŠ¤ë“¤: {sorted(used_indexes)}")
            print(f">> ì´ {len(used_indexes)}ê°œ ì¶œì²˜ ì‚¬ìš© (ì „ì²´ {len(final_collected_data)}ê°œ ì¤‘)")

        yield {"type": "complete", "data": {
            "message": "ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ"
        }}

    async def _update_use_contents_after_recollection(
    self,
    section_info: Dict,
    all_data: List[SearchResult],
    original_indexes: List[int],
    new_data_indexes: List[int],
    query: str
    ) -> List[int]:
        """ë³´ê°• í›„ í•´ë‹¹ ì„¹ì…˜ì˜ use_contentsë¥¼ LLMì´ ì—…ë°ì´íŠ¸ (ì „ì²´ ë‚´ìš© ê¸°ë°˜)"""

        section_title = section_info.get('section_title', 'ì„¹ì…˜')

        # â­ í•µì‹¬ ê°œì„ : ì „ì²´ ë‚´ìš©ì„ LLMì—ê²Œ ì œê³µ
        data_summary = ""

        # ê¸°ì¡´ ë°ì´í„° (ì „ì²´ ë‚´ìš©)
        data_summary += "=== ê¸°ì¡´ ì„ íƒëœ ë°ì´í„° (ì „ì²´ ë‚´ìš©) ===\n"
        for idx in original_indexes[:3]:  # ì²˜ìŒ 3ê°œë§Œ (ê¸¸ì´ ì œí•œ)
            if 0 <= idx < len(all_data):
                res = all_data[idx]
                content = getattr(res, 'content', '')[:800]  # 800ìë¡œ ì œí•œ
                data_summary += f"""
    [{idx:2d}] [{getattr(res, 'source', 'Unknown')}] {getattr(res, 'title', 'No Title')}
    ë‚´ìš©: {content}{"..." if len(getattr(res, 'content', '')) > 800 else ""}

    """

        # ìƒˆ ë°ì´í„° (ì „ì²´ ë‚´ìš©)
        data_summary += "=== ìƒˆë¡œ ì¶”ê°€ëœ ë°ì´í„° (ì „ì²´ ë‚´ìš©) ===\n"
        for idx in new_data_indexes:
            if 0 <= idx < len(all_data):
                res = all_data[idx]
                content = getattr(res, 'content', '')[:800]  # 800ìë¡œ ì œí•œ
                data_summary += f"""
    [{idx:2d}] [NEW] [{getattr(res, 'source', 'Unknown')}] {getattr(res, 'title', 'No Title')}
    ë‚´ìš©: {content}{"..." if len(getattr(res, 'content', '')) > 800 else ""}

    """

        update_prompt = f"""
    "{section_title}" ì„¹ì…˜ì„ ìœ„í•´ ê¸°ì¡´ ë°ì´í„°ì™€ ìƒˆë¡œ ì¶”ê°€ëœ ë°ì´í„°ì˜ **ì „ì²´ ë‚´ìš©ì„ ì½ê³ ** ê°€ì¥ ì í•©í•œ ë°ì´í„°ë“¤ì„ ì„ íƒí•´ì£¼ì„¸ìš”.

    **ì„¹ì…˜**: "{section_title}"
    **ì „ì²´ ì§ˆë¬¸**: "{query}"

    {data_summary[:8000]}

    **ì„ íƒ ê¸°ì¤€**:
    1. **ê° ë°ì´í„°ì˜ ì „ì²´ ë‚´ìš©ì„ ì½ê³ ** ì„¹ì…˜ ì£¼ì œì™€ì˜ ê´€ë ¨ì„± íŒë‹¨
    2. ì œëª©ë§Œ ë³´ê³  ê²°ì •í•˜ì§€ ë§ê³  **ì‹¤ì œ ë‚´ìš©ì˜ ì§ˆê³¼ ê´€ë ¨ì„±** í™•ì¸
    3. ìƒˆ ë°ì´í„°ëŠ” í•´ë‹¹ ì„¹ì…˜ì„ ìœ„í•´ íŠ¹ë³„íˆ ìˆ˜ì§‘ëœ ê²ƒì´ë¯€ë¡œ ì ê·¹ ê³ ë ¤
    4. ì‹¤ì œë¡œ ìœ ìš©í•œ ì •ë³´ê°€ ë‹´ê¸´ ë°ì´í„°ë§Œ ìµœëŒ€ 8ê°œ ì„ ë³„

    **ì›ë³¸**: {original_indexes}
    **ìƒˆ ë°ì´í„°**: {new_data_indexes}

    JSONìœ¼ë¡œë§Œ ì‘ë‹µ:
    {{
        "updated_use_contents": [0, 2, 5, 8],
        "reasoning": "ê° ë°ì´í„°ë¥¼ ì„ íƒ/ì œì™¸í•œ êµ¬ì²´ì  ì´ìœ  (ë‚´ìš© ê¸°ë°˜)"
    }}
    """

        try:
            response = await self._invoke_with_fallback(update_prompt, self.llm, self.llm_openai_mini)
            result = json.loads(re.search(r'\{.*\}', response.content, re.DOTALL).group())

            updated_indexes = result.get("updated_use_contents", [])
            reasoning = result.get("reasoning", "")

            # ìœ íš¨ì„± ê²€ì¦
            max_index = len(all_data) - 1
            valid_indexes = [idx for idx in updated_indexes if isinstance(idx, int) and 0 <= idx <= max_index]

            print(f"  - use_contents ì—…ë°ì´íŠ¸ ì™„ë£Œ (ì „ì²´ ë‚´ìš© ê¸°ë°˜):")
            print(f"    ìµœì¢… ì„ íƒ: {valid_indexes}")
            print(f"    ì„ íƒ ì´ìœ : {reasoning}")

            return valid_indexes

        except Exception as e:
            print(f"  - use_contents ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            # fallback: ì›ë³¸ + ìƒˆ ë°ì´í„° í•©ì¹˜ê¸° (ìµœëŒ€ 8ê°œ)
            combined = original_indexes + new_data_indexes
            return combined[:8]

    async def _log_chart_verification(self, query: str, section_title: str, section_description: str, section_data_list: List[SearchResult], chart_data: dict, state: dict):
        """ì°¨íŠ¸ ìƒì„± ê²€ì¦ì„ ìœ„í•œ ìƒì„¸ ë¡œê·¸ ê¸°ë¡ (ì¿¼ë¦¬ë³„ í´ë” êµ¬ì¡°)"""
        try:
            # ì¿¼ë¦¬ë³„ ë¡œê·¸ ë””ë ‰í† ë¦¬ ì‚¬ìš©
            session_id = state.get("session_id", "unknown")
            query_dir = self._get_query_log_dir(query, session_id, state)

            # ì°¨íŠ¸ ì¸ë±ìŠ¤ ìƒì„± (ê°™ì€ ì¿¼ë¦¬ì—ì„œ ì—¬ëŸ¬ ì°¨íŠ¸ê°€ ìƒì„±ë  ê²½ìš°)
            chart_index = state.get('chart_counter', 0) + 1
            filepath = f"{query_dir}/chart_verification_section{chart_index}.txt"

            # ê²€ì¦ ë¡œê·¸ ë‚´ìš© ìƒì„±
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
            verification_log = self._generate_chart_verification_content(
                query, section_title, section_description, section_data_list, chart_data, timestamp
            )

            # íŒŒì¼ì— ë¹„ë™ê¸°ë¡œ ì“°ê¸°
            async with aiofiles.open(filepath, 'w', encoding='utf-8') as f:
                await f.write(verification_log)

            print(f"ğŸ“‹ ì°¨íŠ¸ ê²€ì¦ ë¡œê·¸ ì €ì¥ì™„ë£Œ: {filepath}")

        except Exception as e:
            print(f"âŒ ì°¨íŠ¸ ê²€ì¦ ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}")

    async def _log_section_verification(self, query: str, section_title: str, section_description: str, section_data_list: List[SearchResult], section_content: str, state: dict):
        """ì„¹ì…˜ ìƒì„± ê²€ì¦ì„ ìœ„í•œ ìƒì„¸ ë¡œê·¸ ê¸°ë¡ (ì¿¼ë¦¬ë³„ í´ë” êµ¬ì¡°)"""
        try:
            # ì¿¼ë¦¬ë³„ ë¡œê·¸ ë””ë ‰í† ë¦¬ ì‚¬ìš©
            session_id = state.get("session_id", "unknown")
            query_dir = self._get_query_log_dir(query, session_id, state)

            # ì„¹ì…˜ ì¸ë±ìŠ¤ ìƒì„±
            section_index = len(state.get('accumulated_context', {}).get('generated_sections', [])) + 1
            filepath = f"{query_dir}/section_verification_section{section_index}.txt"

            # ê²€ì¦ ë¡œê·¸ ë‚´ìš© ìƒì„±
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
            verification_log = self._generate_section_verification_content(
                query, section_title, section_description, section_data_list, section_content, timestamp
            )

            # íŒŒì¼ì— ë¹„ë™ê¸°ë¡œ ì“°ê¸°
            async with aiofiles.open(filepath, 'w', encoding='utf-8') as f:
                await f.write(verification_log)

            print(f"ğŸ“„ ì„¹ì…˜ ê²€ì¦ ë¡œê·¸ ì €ì¥ì™„ë£Œ: {filepath}")

        except Exception as e:
            print(f"âŒ ì„¹ì…˜ ê²€ì¦ ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}")

    def _generate_chart_verification_content(self, query: str, section_title: str, section_description: str, section_data_list: List[SearchResult], chart_data: dict, timestamp: str) -> str:
        """ì°¨íŠ¸ ê²€ì¦ ë¡œê·¸ ë‚´ìš© ìƒì„±"""
        content = f"""
================================================================================
                          ì°¨íŠ¸ ìƒì„± ê²€ì¦ ë¡œê·¸
================================================================================

ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
íƒ€ì„ìŠ¤íƒ¬í”„: {timestamp}

================================================================================
ì°¨íŠ¸ ìƒì„± ì •ë³´
================================================================================

ì‚¬ìš©ì ì¿¼ë¦¬:
{query}

ì„¹ì…˜ ì œëª©:
{section_title}

ì„¹ì…˜ ì„¤ëª…:
{section_description}

================================================================================
ì„¹ì…˜ì—ì„œ ì‚¬ìš©í•œ ë°ì´í„° (ì´ {len(section_data_list)}ê°œ)
================================================================================
"""

        # ì„¹ì…˜ ë°ì´í„° ìƒì„¸ ì •ë³´
        for i, data_item in enumerate(section_data_list):
            source = getattr(data_item, 'source', 'Unknown')
            title = getattr(data_item, 'title', 'No Title')
            content_text = getattr(data_item, 'content', '')
            url = getattr(data_item, 'url', '')
            score = getattr(data_item, 'score', 0.0)
            doc_type = getattr(data_item, 'document_type', 'unknown')

            content += f"""
[ë°ì´í„° ì¸ë±ìŠ¤ {i}]
  ì¶œì²˜: {source}
  ì œëª©: {title}
  URL: {url}
  ì ìˆ˜: {score:.3f}
  íƒ€ì…: {doc_type}
  ë‚´ìš©:
  {content_text[:]}

"""

        content += f"""
================================================================================
ìƒì„±ëœ ì°¨íŠ¸ ë°ì´í„° (JSON)
================================================================================

{json.dumps(chart_data, ensure_ascii=False, indent=2)}

================================================================================
ê²€ì¦ í¬ì¸íŠ¸
================================================================================

1. ë°ì´í„° ì •í™•ì„±: ì°¨íŠ¸ì˜ ìˆ˜ì¹˜ê°€ ì‹¤ì œ ë°ì´í„°ì™€ ì¼ì¹˜í•˜ëŠ”ê°€?
2. ë°ì´í„° ì¶œì²˜: ì°¨íŠ¸ì— ì‚¬ìš©ëœ ì •ë³´ê°€ ìœ„ì˜ ì„¹ì…˜ ë°ì´í„°ì— ê¸°ë°˜í•˜ëŠ”ê°€?
3. ë…¼ë¦¬ì  ì¼ê´€ì„±: ì°¨íŠ¸ ìœ í˜•ì´ ë°ì´í„° íŠ¹ì„±ì— ì í•©í•œê°€?
4. í• ë£¨ì‹œë„¤ì´ì…˜ ì—¬ë¶€: ì‹¤ì œ ë°ì´í„°ì— ì—†ëŠ” ë‚´ìš©ì´ ì°¨íŠ¸ì— í¬í•¨ë˜ì—ˆëŠ”ê°€?

================================================================================
ê²€ì¦ ì™„ë£Œ í›„ ì´ ë¶€ë¶„ì— ê²€í†  ê²°ê³¼ë¥¼ ê¸°ë¡í•˜ì„¸ìš”
================================================================================

[ ] ê²€ì¦ ì™„ë£Œ
[ ] ë°ì´í„° ì •í™•ì„± í™•ì¸
[ ] í• ë£¨ì‹œë„¤ì´ì…˜ ì—†ìŒ í™•ì¸
[ ] ì°¨íŠ¸ ìœ í˜• ì ì ˆì„± í™•ì¸

ê²€ì¦ì: ___________
ê²€ì¦ì¼: ___________
ê²€ì¦ ê²°ê³¼:
_________________________________________________________________________

================================================================================
"""

        return content

    def _generate_section_verification_content(self, query: str, section_title: str, section_description: str, section_data_list: List[SearchResult], section_content: str, timestamp: str) -> str:
        """ì„¹ì…˜ ê²€ì¦ ë¡œê·¸ ë‚´ìš© ìƒì„±"""
        content = f"""
================================================================================
                          ì„¹ì…˜ ìƒì„± ê²€ì¦ ë¡œê·¸
================================================================================

ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
íƒ€ì„ìŠ¤íƒ¬í”„: {timestamp}

================================================================================
ì„¹ì…˜ ìƒì„± ì •ë³´
================================================================================

ì‚¬ìš©ì ì¿¼ë¦¬:
{query}

ì„¹ì…˜ ì œëª©:
{section_title}

ì„¹ì…˜ ì„¤ëª…:
{section_description}

================================================================================
ì„¹ì…˜ì—ì„œ ì‚¬ìš©í•œ ë°ì´í„° (ì´ {len(section_data_list)}ê°œ)
================================================================================
"""

        # ì„¹ì…˜ ë°ì´í„° ìƒì„¸ ì •ë³´
        for i, data_item in enumerate(section_data_list):
            source = getattr(data_item, 'source', 'Unknown')
            title = getattr(data_item, 'title', 'No Title')
            content_text = getattr(data_item, 'content', '')
            url = getattr(data_item, 'url', '')
            score = getattr(data_item, 'score', 0.0)
            doc_type = getattr(data_item, 'document_type', 'unknown')

            content += f"""
[ë°ì´í„° ì¸ë±ìŠ¤ {i}]
  ì¶œì²˜: {source}
  ì œëª©: {title}
  URL: {url}
  ì ìˆ˜: {score:.3f}
  íƒ€ì…: {doc_type}
  ë‚´ìš©:
  {content_text[:]}

"""

        content += f"""
================================================================================
ìƒì„±ëœ ì„¹ì…˜ ë‚´ìš©
================================================================================

{section_content}

================================================================================
ê²€ì¦ í¬ì¸íŠ¸
================================================================================

1. ë‚´ìš© ì •í™•ì„±: ì„¹ì…˜ ë‚´ìš©ì´ ì‹¤ì œ ë°ì´í„°ì— ê¸°ë°˜í•˜ëŠ”ê°€?
2. ë°ì´í„° ì¶œì²˜: ì„¹ì…˜ì— ì–¸ê¸‰ëœ ì •ë³´ê°€ ìœ„ì˜ ì„¹ì…˜ ë°ì´í„°ì—ì„œ ë‚˜ì˜¨ ê²ƒì¸ê°€?
3. ë…¼ë¦¬ì  ì¼ê´€ì„±: ì„¹ì…˜ êµ¬ì¡°ì™€ ë‚´ìš©ì´ ë…¼ë¦¬ì ìœ¼ë¡œ ì¼ê´€ëœê°€?
4. í• ë£¨ì‹œë„¤ì´ì…˜ ì—¬ë¶€: ì‹¤ì œ ë°ì´í„°ì— ì—†ëŠ” ë‚´ìš©ì´ ì„¹ì…˜ì— í¬í•¨ë˜ì—ˆëŠ”ê°€?
5. ì™„ì„±ë„: ì„¹ì…˜ì´ ì‚¬ìš©ì ì§ˆë¬¸ì— ì ì ˆíˆ ë‹µë³€í•˜ê³  ìˆëŠ”ê°€?

================================================================================
ê²€ì¦ ì™„ë£Œ í›„ ì´ ë¶€ë¶„ì— ê²€í†  ê²°ê³¼ë¥¼ ê¸°ë¡í•˜ì„¸ìš”
================================================================================

[ ] ê²€ì¦ ì™„ë£Œ
[ ] ë‚´ìš© ì •í™•ì„± í™•ì¸
[ ] í• ë£¨ì‹œë„¤ì´ì…˜ ì—†ìŒ í™•ì¸
[ ] ë°ì´í„° ì¶œì²˜ ì ì ˆì„± í™•ì¸
[ ] ì™„ì„±ë„ í™•ì¸

ê²€ì¦ì: ___________
ê²€ì¦ì¼: ___________
ê²€ì¦ ê²°ê³¼:
_________________________________________________________________________

================================================================================
"""

        return content


    # ================================================================================================
    # ì¢…í•© ë¡œê·¸ ì‹œìŠ¤í…œ - ë³´ê³ ì„œ ìƒì„± ê³¼ì • ê²€ì¦ìš© (ì¿¼ë¦¬ë³„ í´ë” êµ¬ì¡°)
    # ================================================================================================

    def _sanitize_query_for_folder_name(self, query: str, max_length: int = 50) -> str:
        """ì¿¼ë¦¬ í…ìŠ¤íŠ¸ë¥¼ íŒŒì¼ì‹œìŠ¤í…œì— ì•ˆì „í•œ í´ë”ëª…ìœ¼ë¡œ ë³€í™˜"""
        import re

        # í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê³µë°±ë§Œ ìœ ì§€
        sanitized = re.sub(r'[^\wê°€-í£\s]', '', query)

        # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ ì¤„ì´ê³  ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€í™˜
        sanitized = re.sub(r'\s+', '_', sanitized.strip())

        # ê¸¸ì´ ì œí•œ
        if len(sanitized) > max_length:
            sanitized = sanitized[:max_length]

        # ëì— ì–¸ë”ìŠ¤ì½”ì–´ê°€ ìˆìœ¼ë©´ ì œê±°
        sanitized = sanitized.rstrip('_')

        return sanitized if sanitized else "unknown_query"

    def _get_query_log_dir(self, query: str, session_id: str, state: dict = None) -> str:
        """ì¿¼ë¦¬ë³„ ë¡œê·¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ ìƒì„± (ì„¸ì…˜ë³„ ìºì‹±, í”„ë¡œì íŠ¸ëª… í¬í•¨)"""
        # stateê°€ ìˆê³  ì´ë¯¸ ìƒì„±ëœ ë¡œê·¸ ë””ë ‰í† ë¦¬ê°€ ìˆìœ¼ë©´ ì¬ì‚¬ìš©
        if state and 'query_log_dir' in state:
            existing_dir = state['query_log_dir']
            if os.path.exists(existing_dir):
                return existing_dir

        log_base_dir = "/app/logs"

        # í”„ë¡œì íŠ¸ ì •ë³´ ì¶”ì¶œ
        project_name = "Unknown_Project"
        if state:
            # stateì—ì„œ í”„ë¡œì íŠ¸ ì •ë³´ í™•ì¸ (ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ì‹œë„)
            project_info = state.get("project_name") or state.get("project_title")
            if not project_info and "metadata" in state:
                project_info = state["metadata"].get("project_name") or state["metadata"].get("project_title")
                print(f">> ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„± - metadataì—ì„œ í”„ë¡œì íŠ¸ ì •ë³´ í™•ì¸: {project_info}")

            if project_info:
                project_name = self._sanitize_query_for_folder_name(project_info, 30)  # í”„ë¡œì íŠ¸ëª…ì€ 30ìë¡œ ì œí•œ
                print(f">> ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„± - ìµœì¢… í”„ë¡œì íŠ¸ëª…: {project_name}")
            else:
                print(f">> ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„± - í”„ë¡œì íŠ¸ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ. state.keys(): {state.keys() if state else 'state is None'}")
                if state and "metadata" in state:
                    print(f">> metadata ë‚´ìš©: {state['metadata']}")

        # ì¿¼ë¦¬ í…ìŠ¤íŠ¸ ì •ì œ
        sanitized_query = self._sanitize_query_for_folder_name(query)

        # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€ (í•œ ë²ˆë§Œ ìƒì„±)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # í´ë”ëª…: [{í”„ë¡œì íŠ¸ëª…}]_{ì¿¼ë¦¬}_{ì„¸ì…˜ID}_{íƒ€ì„ìŠ¤íƒ¬í”„}
        folder_name = f"[{project_name}]_{sanitized_query}_{session_id}_{timestamp}"

        query_dir = f"{log_base_dir}/{folder_name}"
        os.makedirs(query_dir, exist_ok=True)

        # stateì— ìºì‹œ ì €ì¥
        if state:
            state['query_log_dir'] = query_dir

        return query_dir

    async def _log_plan_reasoning(self, query: str, persona: str, plan: dict, state: dict):
        """ê³„íš ìˆ˜ë¦½ ê³¼ì •ì˜ ì¶”ë¡  ë¡œì§ ìƒì„¸ ë¡œê·¸"""
        try:
            session_id = state.get("session_id", "unknown")
            query_dir = self._get_query_log_dir(query, session_id, state)
            filepath = f"{query_dir}/plan_reasoning.txt"

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
            planning_log = self._generate_plan_reasoning_content(query, persona, plan, timestamp)

            async with aiofiles.open(filepath, 'w', encoding='utf-8') as f:
                await f.write(planning_log)

            print(f"ğŸ¯ ê³„íš ì¶”ë¡  ë¡œê·¸ ì €ì¥ì™„ë£Œ: {filepath}")

        except Exception as e:
            print(f"âŒ ê³„íš ì¶”ë¡  ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}")

    async def _log_data_selection_reasoning(self, step_info: dict, collected_data: list, selected_indexes: list, reasoning: str, state: dict):
        """ë°ì´í„° ì„ ë³„ ê³¼ì •ì˜ ì¶”ë¡  ë¡œì§ ìƒì„¸ ë¡œê·¸"""
        try:
            query = state.get("original_query", "unknown_query")
            session_id = state.get("session_id", "unknown")
            query_dir = self._get_query_log_dir(query, session_id, state)

            step_num = step_info.get("step", "unknown")
            filepath = f"{query_dir}/data_selection_step{step_num}.txt"

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
            selection_log = self._generate_data_selection_content(
                step_info, collected_data, selected_indexes, reasoning, timestamp
            )

            async with aiofiles.open(filepath, 'w', encoding='utf-8') as f:
                await f.write(selection_log)

            print(f"ğŸ” ë°ì´í„° ì„ ë³„ ë¡œê·¸ ì €ì¥ì™„ë£Œ (Step {step_num}): {filepath}")

        except Exception as e:
            print(f"âŒ ë°ì´í„° ì„ ë³„ ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}")

    def _generate_plan_reasoning_content(self, query: str, persona: str, plan: dict, timestamp: str) -> str:
        """ê³„íš ìˆ˜ë¦½ ì¶”ë¡  ë¡œê·¸ ë‚´ìš© ìƒì„±"""
        content = f"""
================================================================================
                          ê³„íš ìˆ˜ë¦½ ì¶”ë¡  ê³¼ì • ë¡œê·¸
================================================================================

ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
íƒ€ì„ìŠ¤íƒ¬í”„: {timestamp}

================================================================================
ìš”ì²­ ë¶„ì„
================================================================================

ì‚¬ìš©ì ì¿¼ë¦¬: {query}
í˜ë¥´ì†Œë‚˜: {persona}
í˜ë¥´ì†Œë‚˜ ê´€ì : {self.personas.get(persona, {}).get('description', 'ì •ë³´ ì—†ìŒ')}

================================================================================
ê³„íš ìˆ˜ë¦½ ì¶”ë¡  ê³¼ì •
================================================================================

1. í•µì‹¬ í‚¤ì›Œë“œ ë¶„ì„: {self._extract_keywords_from_query(query)}
2. í•„ìš”í•œ ì •ë³´ ìœ í˜• ë¶„ì„:
- ì •í˜• ë°ì´í„° í•„ìš”ì„±: {self._analyze_structured_data_need(query)}
- ë¹„ì •í˜• ë°ì´í„° í•„ìš”ì„±: {self._analyze_unstructured_data_need(query)}
- ê´€ê³„í˜• ë°ì´í„° í•„ìš”ì„±: {self._analyze_graph_data_need(query)}
- í•™ìˆ  ì—°êµ¬ í•„ìš”ì„±: {self._analyze_research_data_need(query)}

3. ë„êµ¬ ì„ íƒ ì¶”ë¡ :
{self._generate_tool_selection_reasoning(query)}

================================================================================
ìµœì¢… ê³„íš
================================================================================

ê³„íš ì œëª©: {plan.get('title', '')}
ì´ ì‹¤í–‰ ë‹¨ê³„: {len(plan.get('execution_steps', []))}

ë‹¨ê³„ë³„ ìƒì„¸:
"""

        for step in plan.get('execution_steps', []):
            content += f"""
Step {step.get('step')}: {step.get('title', '')}
  ì¶”ë¡ : {step.get('reasoning', '')}
  í•˜ìœ„ ì§ˆë¬¸: {len(step.get('sub_questions', []))}ê°œ
"""

        content += """
================================================================================
í’ˆì§ˆ ì²´í¬
================================================================================

[ ] ì‚¬ìš©ì ìš”ì²­ ì™„ì „ ì»¤ë²„ë¦¬ì§€ í™•ì¸
[ ] í˜ë¥´ì†Œë‚˜ ê´€ì  ì ì ˆì„± í™•ì¸
[ ] ë„êµ¬ ì„ íƒ ìµœì ì„± í™•ì¸
[ ] ë‹¨ê³„ ìˆœì„œ ë…¼ë¦¬ì„± í™•ì¸
[ ] ì²˜ë¦¬ íš¨ìœ¨ì„± í™•ì¸
================================================================================
"""
        return content

    def _generate_data_selection_content(self, step_info: dict, collected_data: list, selected_indexes: list, reasoning: str, timestamp: str) -> str:
        """ë°ì´í„° ì„ ë³„ ê³¼ì • ë¡œê·¸ ë‚´ìš© ìƒì„±"""
        step_num = step_info.get('step', '?')
        step_title = step_info.get('title', 'ì œëª© ì—†ìŒ')

        content = f"""
================================================================================
                    ë°ì´í„° ì„ ë³„ ì¶”ë¡  ê³¼ì • ë¡œê·¸ (Step {step_num})
================================================================================

ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
íƒ€ì„ìŠ¤íƒ¬í”„: {timestamp}

================================================================================
ë‹¨ê³„ ì •ë³´
================================================================================

ë‹¨ê³„ ë²ˆí˜¸: {step_num}
ë‹¨ê³„ ì œëª©: {step_title}
ë‹¨ê³„ ì¶”ë¡ : {step_info.get('reasoning', 'ì •ë³´ ì—†ìŒ')}

í•˜ìœ„ ì§ˆë¬¸ë“¤:
"""

        for i, sub_q in enumerate(step_info.get('sub_questions', [])):
            content += f"  {i+1}. {sub_q.get('question', 'ì§ˆë¬¸ ì—†ìŒ')} (ë„êµ¬: {sub_q.get('tool', 'ë„êµ¬ ì—†ìŒ')})\n"

        content += f"""

================================================================================
ìˆ˜ì§‘ëœ ë°ì´í„° í˜„í™©
================================================================================

ì´ ìˆ˜ì§‘ ë°ì´í„°: {len(collected_data)}ê°œ
ì„ ë³„ëœ ë°ì´í„°: {len(selected_indexes)}ê°œ
ì„ ë³„ ë¹„ìœ¨: {(len(selected_indexes) / max(len(collected_data), 1) * 100):.1f}%

ë°ì´í„° ì¶œì²˜ë³„ ë¶„í¬:
"""

        source_counts = {}
        for data in collected_data:
            source = getattr(data, 'source', 'Unknown')
            source_counts[source] = source_counts.get(source, 0) + 1

        for source, count in source_counts.items():
            content += f"  - {source}: {count}ê°œ\n"

        content += f"""

================================================================================
ì„ ë³„ ê¸°ì¤€ ë° ì¶”ë¡  ê³¼ì •
================================================================================

LLM ì¶”ë¡  ê²°ê³¼:
{reasoning}

ì„ ë³„ëœ ë°ì´í„° ìƒì„¸:
"""

        for i, idx in enumerate(selected_indexes):
            if idx < len(collected_data):
                data = collected_data[idx]
                content += f"""
[ì„ ë³„ ë°ì´í„° {i+1}] ì¸ë±ìŠ¤ {idx}
  ì¶œì²˜: {getattr(data, 'source', 'Unknown')}
  ì œëª©: {getattr(data, 'title', 'No Title')}
  ì ìˆ˜: {getattr(data, 'score', 0.0):.3f}
  íƒ€ì…: {getattr(data, 'document_type', 'unknown')}
  ë‚´ìš© (ì²« 200ì): {getattr(data, 'content', '')[:200]}...
"""

        content += f"""

================================================================================
ì œì™¸ëœ ë°ì´í„° ë¶„ì„
================================================================================

ì œì™¸ëœ ë°ì´í„°: {len(collected_data) - len(selected_indexes)}ê°œ

ì œì™¸ ì‚¬ìœ ë³„ ë¶„ì„:
"""

        excluded_indexes = [i for i in range(len(collected_data)) if i not in selected_indexes]
        for idx in excluded_indexes[:5]:  # ì²˜ìŒ 5ê°œë§Œ ìƒì„¸ ë¶„ì„
            if idx < len(collected_data):
                data = collected_data[idx]
                content += f"""
[ì œì™¸ ë°ì´í„°] ì¸ë±ìŠ¤ {idx}
  ì¶œì²˜: {getattr(data, 'source', 'Unknown')}
  ì œëª©: {getattr(data, 'title', 'No Title')}
  ì ìˆ˜: {getattr(data, 'score', 0.0):.3f}
  ì¶”ì • ì œì™¸ ì‚¬ìœ : ê´€ë ¨ì„± ë‚®ìŒ/ì¤‘ë³µì„±/í’ˆì§ˆ ì´ìŠˆ
"""

        if len(excluded_indexes) > 5:
            content += f"\n... ì™¸ {len(excluded_indexes) - 5}ê°œ ë°ì´í„° ì œì™¸\n"

        content += f"""

================================================================================
í’ˆì§ˆ ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸
================================================================================

[ ] ë‹¨ê³„ ëª©ì  ë‹¬ì„±ë„: ì´ ë‹¨ê³„ì˜ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸°ì— ì¶©ë¶„í•œ ë°ì´í„°ì¸ê°€?
[ ] ë°ì´í„° í’ˆì§ˆ: ì„ ë³„ëœ ë°ì´í„°ì˜ ì‹ ë¢°ë„ì™€ ì •í™•ì„±ì€ ì ì ˆí•œê°€?
[ ] ë‹¤ì–‘ì„± í™•ë³´: ë‹¤ì–‘í•œ ê´€ì ê³¼ ì¶œì²˜ê°€ í¬í•¨ë˜ì—ˆëŠ”ê°€?
[ ] ì¤‘ë³µì„± ì œê±°: ìœ ì‚¬í•œ ë‚´ìš©ì˜ ì¤‘ë³µì´ ì ì ˆíˆ ì œê±°ë˜ì—ˆëŠ”ê°€?
[ ] ê´€ë ¨ì„± ê²€ì¦: ëª¨ë“  ì„ ë³„ ë°ì´í„°ê°€ ì§ˆë¬¸ê³¼ ì§ì ‘ ê´€ë ¨ì´ ìˆëŠ”ê°€?

ê²€ì¦ì: ___________    ê²€ì¦ì¼: ___________    ì„ ë³„ í’ˆì§ˆ í‰ê°€: ___________
================================================================================
"""

        return content

    def _extract_keywords_from_query(self, query: str) -> str:
        """ì¿¼ë¦¬ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = []
        if "ê°€ê²©" in query or "ì‹œì„¸" in query or "ë¹„ìš©" in query:
            keywords.append("ê°€ê²©/ì‹œì„¸ ì •ë³´")
        if "ì˜ì–‘" in query or "ì„±ë¶„" in query:
            keywords.append("ì˜ì–‘ì„±ë¶„ ì •ë³´")
        if "ì›ì‚°ì§€" in query or "ì‚°ì§€" in query:
            keywords.append("ì›ì‚°ì§€/ì‚°ì§€ ì •ë³´")
        if "ì‹œì¥" in query or "ë™í–¥" in query:
            keywords.append("ì‹œì¥ ë™í–¥")
        if "ì—°êµ¬" in query or "ë…¼ë¬¸" in query:
            keywords.append("í•™ìˆ  ì—°êµ¬")

        return ", ".join(keywords) if keywords else "ì¼ë°˜ì  ì •ë³´ ìš”ì²­"

    def _analyze_structured_data_need(self, query: str) -> str:
        """ì •í˜• ë°ì´í„° í•„ìš”ì„± ë¶„ì„"""
        if any(keyword in query for keyword in ["ê°€ê²©", "ì‹œì„¸", "ì˜ì–‘", "ì„±ë¶„", "ìˆœìœ„", "ë¹„êµ", "TOP", "í‰ê· "]):
            return "ë†’ìŒ - ìˆ˜ì¹˜ ë°ì´í„° ë° í†µê³„ ì •ë³´ í•„ìš”"
        return "ë‚®ìŒ - ì •í˜• ë°ì´í„° ë¶ˆí•„ìš”"

    def _analyze_unstructured_data_need(self, query: str) -> str:
        """ë¹„ì •í˜• ë°ì´í„° í•„ìš”ì„± ë¶„ì„"""
        if any(keyword in query for keyword in ["ë™í–¥", "ë¶„ì„", "ì •ì±…", "ë°°ê²½", "í˜„í™©", "ì„¤ëª…"]):
            return "ë†’ìŒ - í…ìŠ¤íŠ¸ ê¸°ë°˜ ì •ë³´ ë° ë¶„ì„ í•„ìš”"
        return "ë‚®ìŒ - ë¹„ì •í˜• ë°ì´í„° ë¶ˆí•„ìš”"

    def _analyze_graph_data_need(self, query: str) -> str:
        """ê´€ê³„í˜• ë°ì´í„° í•„ìš”ì„± ë¶„ì„"""
        if any(keyword in query for keyword in ["ì›ì‚°ì§€", "ì‚°ì§€", "íŠ¹ì‚°í’ˆ", "ì—°ê²°", "ê´€ê³„"]):
            return "ë†’ìŒ - ì—”í‹°í‹° ê°„ ê´€ê³„ ì •ë³´ í•„ìš”"
        return "ë‚®ìŒ - ê´€ê³„í˜• ë°ì´í„° ë¶ˆí•„ìš”"

    def _analyze_research_data_need(self, query: str) -> str:
        """í•™ìˆ  ì—°êµ¬ ë°ì´í„° í•„ìš”ì„± ë¶„ì„"""
        if any(keyword in query for keyword in ["ì—°êµ¬", "ë…¼ë¬¸", "ê°œë°œ", "ì‹ ì œí’ˆ", "ê³¼í•™", "ìµœì‹ "]):
            return "ë†’ìŒ - ìµœì‹  í•™ìˆ  ì—°êµ¬ ì •ë³´ í•„ìš”"
        return "ë‚®ìŒ - í•™ìˆ  ì—°êµ¬ ë°ì´í„° ë¶ˆí•„ìš”"

    def _generate_tool_selection_reasoning(self, query: str) -> str:
        """ë„êµ¬ ì„ íƒ ì¶”ë¡  ê³¼ì •"""
        reasoning_parts = []

        if self._analyze_structured_data_need(query).startswith("ë†’ìŒ"):
            reasoning_parts.append("RDB ê²€ìƒ‰: ì •í˜• ë°ì´í„°(ê°€ê²©, ì˜ì–‘ì„±ë¶„, í†µê³„) í•„ìš”")

        if self._analyze_unstructured_data_need(query).startswith("ë†’ìŒ"):
            reasoning_parts.append("Vector DB ê²€ìƒ‰: ë¹„ì •í˜• í…ìŠ¤íŠ¸(ë‰´ìŠ¤, ë³´ê³ ì„œ) í•„ìš”")

        if self._analyze_graph_data_need(query).startswith("ë†’ìŒ"):
            reasoning_parts.append("Graph DB ê²€ìƒ‰: ê´€ê³„í˜• ë°ì´í„°(ì›ì‚°ì§€-í’ˆëª© ì—°ê²°) í•„ìš”")

        if self._analyze_research_data_need(query).startswith("ë†’ìŒ"):
            reasoning_parts.append("PubMed ê²€ìƒ‰: ìµœì‹  í•™ìˆ  ì—°êµ¬ ì •ë³´ í•„ìš”")

        return "\n".join(f"- {part}" for part in reasoning_parts) if reasoning_parts else "ê¸°ë³¸ì ì¸ ì •ë³´ ê²€ìƒ‰ ë„êµ¬ ì‚¬ìš©"
