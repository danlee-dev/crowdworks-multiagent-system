# ë³´ê³ ì„œ í‰ê°€ ì‹œìŠ¤í…œ

ìƒì„±ëœ ë³´ê³ ì„œì˜ í’ˆì§ˆì„ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨

- [ê°œìš”](#ê°œìš”)
- [í•µì‹¬ ì„±ê³¼ì§€í‘œ (KPI)](#í•µì‹¬-ì„±ê³¼ì§€í‘œ-kpi)
- [ì‹œìŠ¤í…œ êµ¬ì„±](#ì‹œìŠ¤í…œ-êµ¬ì„±)
- [ì„¤ì¹˜ ë° ì‚¬ìš©ë²•](#ì„¤ì¹˜-ë°-ì‚¬ìš©ë²•)
- [í‰ê°€ ë©”íŠ¸ë¦­ ìƒì„¸](#í‰ê°€-ë©”íŠ¸ë¦­-ìƒì„¸)
- [API ë ˆí¼ëŸ°ìŠ¤](#api-ë ˆí¼ëŸ°ìŠ¤)
- [ì˜ˆì‹œ](#ì˜ˆì‹œ)

## ê°œìš”

ì´ í‰ê°€ ì‹œìŠ¤í…œì€ ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì´ ìƒì„±í•œ ë³´ê³ ì„œë¥¼ ë‹¤ìŒê³¼ ê°™ì´ í‰ê°€í•©ë‹ˆë‹¤:

1. **ìë™ í‰ê°€**: ì •ëŸ‰ì  ë©”íŠ¸ë¦­ ìë™ ê³„ì‚°
2. **AI ì‹¬íŒ í‰ê°€**: LLMì„ í™œìš©í•œ ì •ì„±ì  í‰ê°€
3. **ì¢…í•© í‰ê°€**: ìë™ + AI í‰ê°€ í†µí•©

## í•µì‹¬ ì„±ê³¼ì§€í‘œ (KPI)

### 1. íš¨ê³¼ì„± ì§€í‘œ (Effectiveness)

#### (1) ì‘ì—… ì„±ê³µë¥  (Task Success Rate)
- ì—ì´ì „íŠ¸ê°€ ë¶€ì—¬ëœ ë¦¬ì„œì¹˜ ì‘ì—…ì„ ì™„ì „íˆ ì˜¬ë°”ë¥´ê²Œ ì™„ë£Œí•œ ë¹„ìœ¨
- ì¸¡ì •: `success_rate` (0.0 ~ 1.0)
- ê¸°ì¤€:
  - **ì™„ì „ ì„±ê³µ**: 90% ì´ìƒ
  - **ë¶€ë¶„ ì„±ê³µ**: 50% ~ 90%
  - **ì‹¤íŒ¨**: 50% ë¯¸ë§Œ

#### (2) ì¶œë ¥ í’ˆì§ˆ ë° ì •í™•ë„ (Output Quality and Accuracy)
- **ì‚¬ì‹¤ ì •í™•ë„** (`factual_accuracy_score`): ìƒì„±ëœ ë‚´ìš©ì˜ ì‚¬ì‹¤ ì¼ì¹˜ ì •ë„
- **ë…¼ë¦¬ì  ì¼ê´€ì„±** (`logical_coherence_score`): ë…¼ë¦¬ì  íë¦„ê³¼ ì¼ê´€ì„±
- **ìš”êµ¬ì‚¬í•­ ë¶€í•©ë„** (`relevance_score`): ì‚¬ìš©ì ì˜ë„ ë¶€í•© ì •ë„
- ì¸¡ì •: 0~10 ì ìˆ˜ (AI ì‹¬íŒ í‰ê°€)

#### (3) ì™„ì„±ë„ (Completeness)
- ìš”ì²­ëœ ì •ë³´ë‚˜ ë³´ê³ ì„œ ìŠ¤í‚¤ë§ˆì˜ ëª¨ë“  í•„ë“œê°€ ëˆ„ë½ ì—†ì´ ì±„ì›Œì¡ŒëŠ”ì§€ í‰ê°€
- ì¸¡ì •: `completeness_rate` (0.0 ~ 1.0)
- ê²€ì¦ í•­ëª©:
  - í•„ìˆ˜ ì„¹ì…˜ í¬í•¨ ì—¬ë¶€
  - ìŠ¤í‚¤ë§ˆ í•„ë“œ ì™„ì„±ë„
  - ë¶ˆì™„ì „í•œ ì„¹ì…˜ ê°ì§€

#### (4) í™˜ê° í˜„ìƒ ë¹„ìœ¨ (Hallucination Rate)
- ë¶€ì •í™•í•˜ê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì •ë³´ë¥¼ ì‚¬ì‹¤ì²˜ëŸ¼ ìƒì„±í•˜ëŠ” ë¹ˆë„
- ì¸¡ì •: `hallucination_rate` (0.0 ~ 1.0)
- ê°ì§€ ë°©ë²•:
  - ì¶œì²˜ì™€ ë¶ˆì¼ì¹˜í•˜ëŠ” ë‚´ìš©
  - ê²€ì¦ ë¶ˆê°€ëŠ¥í•œ ì£¼ì¥
  - ë‚´ë¶€ ëª¨ìˆœ
  - AI ì‹¬íŒì„ í†µí•œ í™˜ê° íƒì§€

### 2. íš¨ìœ¨ì„± ì§€í‘œ (Efficiency)

#### ì‘ë‹µ ì‹œê°„ / ì§€ì—° ì‹œê°„
- **ì´ ì‹¤í–‰ ì‹œê°„** (`total_execution_time`): ì „ì²´ ì‘ì—… ì†Œìš” ì‹œê°„
- **í‰ê·  ë‹¨ê³„ ì‹œê°„** (`average_step_time`): ë‹¨ê³„ë³„ í‰ê·  ì‹œê°„
- **ì²« ì‘ë‹µ ì‹œê°„** (`time_to_first_response`): ì²« ì‘ë‹µê¹Œì§€ ì‹œê°„

#### ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰
- **í† í° ì‚¬ìš©ëŸ‰** (`total_tokens_used`): LLM API í˜¸ì¶œ í† í° ìˆ˜
- **API í˜¸ì¶œ íšŸìˆ˜** (`total_api_calls`): ì´ API í˜¸ì¶œ ìˆ˜
- **ì¶”ì • ë¹„ìš©** (`estimated_cost`): ì˜ˆìƒ ì‹¤í–‰ ë¹„ìš© (USD)

#### ë‹¨ê³„ ìˆ˜ (Step Count)
- **ì´ ë‹¨ê³„ ìˆ˜** (`total_steps`): ì‹¤í–‰ëœ ì´ ë‹¨ê³„
- **ì¤‘ë³µ ë‹¨ê³„** (`redundant_steps`): ë¶ˆí•„ìš”í•œ ë°˜ë³µ ë‹¨ê³„
- **íš¨ìœ¨ì„± ì ìˆ˜** (`efficiency_score`): ì¢…í•© íš¨ìœ¨ì„± (0~10)

### 3. í’ˆì§ˆ ì§€í‘œ (Quality)

#### ì¶œì²˜ í’ˆì§ˆ
- **ì´ ì¶œì²˜ ìˆ˜** (`total_sources`)
- **ì‹ ë¢° ì¶œì²˜ ìˆ˜** (`reliable_sources`): ì‹ ë¢°ë„ â‰¥ 0.7
- **ì¶œì²˜ ë‹¤ì–‘ì„±** (`source_diversity`): ê³ ìœ  ì¶œì²˜ íƒ€ì… ìˆ˜
- **í‰ê·  ì¶œì²˜ ì‹ ë¢°ë„** (`average_source_reliability`)
- **ì¸ìš© ì •í™•ë„** (`citation_accuracy`): ì¶œì²˜ ì¸ìš© ë¹„ìœ¨

#### ì½˜í…ì¸  ë©”íŠ¸ë¦­
- ë‹¨ì–´ ìˆ˜, ë¬¸ì ìˆ˜
- ì„¹ì…˜ ìˆ˜, ì°¨íŠ¸ ìˆ˜, í…Œì´ë¸” ìˆ˜
- ì¸ìš© ìˆ˜
- êµ¬ì¡°ì  ìš”ì†Œ (ìš”ì•½, ë°©ë²•ë¡ , ê²°ë¡ ) í¬í•¨ ì—¬ë¶€

## ì‹œìŠ¤í…œ êµ¬ì„±

### íŒŒì¼ êµ¬ì¡°

```
app/core/evaluation/
â”œâ”€â”€ __init__.py                  # íŒ¨í‚¤ì§€ ì´ˆê¸°í™”
â”œâ”€â”€ evaluation_models.py         # í‰ê°€ ëª¨ë¸ ì •ì˜ (Pydantic)
â”œâ”€â”€ automated_evaluator.py       # ìë™ í‰ê°€ê¸°
â”œâ”€â”€ ai_judge_evaluator.py        # AI ì‹¬íŒ í‰ê°€ê¸°
â”œâ”€â”€ report_evaluator.py          # ì¢…í•© í‰ê°€ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°
â”œâ”€â”€ evaluate_report_cli.py       # CLI ë„êµ¬
â”œâ”€â”€ example_usage.py             # ì‚¬ìš© ì˜ˆì‹œ
â””â”€â”€ README.md                    # ì´ ë¬¸ì„œ
```

### ì£¼ìš” í´ë˜ìŠ¤

1. **`AutomatedEvaluator`**: ìë™ìœ¼ë¡œ ì¸¡ì • ê°€ëŠ¥í•œ ë©”íŠ¸ë¦­ ê³„ì‚°
2. **`AIJudgeEvaluator`**: LLMì„ ì‚¬ìš©í•œ ì •ì„±ì  í‰ê°€
3. **`ReportEvaluator`**: ì „ì²´ í‰ê°€ í”„ë¡œì„¸ìŠ¤ ê´€ë¦¬

## ì„¤ì¹˜ ë° ì‚¬ìš©ë²•

### í™˜ê²½ ì„¤ì •

```bash
# OpenAI API í‚¤ ì„¤ì • (AI ì‹¬íŒ ì‚¬ìš© ì‹œ í•„ìˆ˜)
export OPENAI_API_KEY="your-api-key"
```

### CLI ì‚¬ìš©ë²•

#### 1. ìƒíƒœ íŒŒì¼ë¡œ í‰ê°€

```bash
python -m app.core.evaluation.evaluate_report_cli \
  --state state.json \
  --output evaluation_result.json
```

#### 2. ë³´ê³ ì„œ íŒŒì¼ë¡œ í‰ê°€

```bash
python -m app.core.evaluation.evaluate_report_cli \
  --query "2024ë…„ AI ì‹œì¥ ë™í–¥ ë¶„ì„" \
  --report report.md \
  --output evaluation_result.json
```

#### 3. AI ì‹¬íŒ ì—†ì´ ë¹ ë¥¸ í‰ê°€

```bash
python -m app.core.evaluation.evaluate_report_cli \
  --state state.json \
  --no-ai-judge
```

#### 4. ì»¤ìŠ¤í…€ ìš”êµ¬ì‚¬í•­ ì§€ì •

```bash
python -m app.core.evaluation.evaluate_report_cli \
  --state state.json \
  --expected-requirements "ì‹œì¥ ê·œëª¨" "íŠ¸ë Œë“œ" "ì˜ˆì¸¡" \
  --expected-sections "ìš”ì•½" "ë¶„ì„" "ê²°ë¡ " \
  --expected-word-count 1500
```

### Python API ì‚¬ìš©ë²•

```python
from app.core.evaluation import ReportEvaluator

# í‰ê°€ê¸° ìƒì„±
evaluator = ReportEvaluator(
    use_ai_judge=True,  # AI ì‹¬íŒ ì‚¬ìš©
    ai_model="gpt-4o-mini"  # ì‚¬ìš©í•  ëª¨ë¸
)

# ë³´ê³ ì„œ í‰ê°€
result = evaluator.evaluate_report(
    query="ì›ë³¸ ì§ˆë¬¸",
    state=streaming_agent_state,
    expected_requirements=["ìš”êµ¬ì‚¬í•­1", "ìš”êµ¬ì‚¬í•­2"],
    expected_sections=["ì„¹ì…˜1", "ì„¹ì…˜2"],
    expected_word_count=1000
)

# ê²°ê³¼ í™•ì¸
print(f"ì¢…í•© ì ìˆ˜: {result.overall_score}/10")
print(f"ë“±ê¸‰: {result.grade}")
print(f"ì„±ê³µë¥ : {result.task_success.success_rate:.2%}")
print(f"í™˜ê° í˜„ìƒ: {result.hallucination.hallucination_count}ê±´")

# ê°•ì /ì•½ì /ê¶Œì¥ì‚¬í•­
for strength in result.strengths:
    print(f"âœ“ {strength}")

for weakness in result.weaknesses:
    print(f"âœ— {weakness}")

for rec in result.recommendations:
    print(f"â†’ {rec}")
```

## í‰ê°€ ë©”íŠ¸ë¦­ ìƒì„¸

### ì¢…í•© ì ìˆ˜ ê³„ì‚° (Overall Score)

ì¢…í•© ì ìˆ˜ëŠ” ë‹¤ìŒ ê°€ì¤‘ì¹˜ë¡œ ê³„ì‚°ë©ë‹ˆë‹¤:

```
Overall Score =
  ì‘ì—… ì„±ê³µë¥  Ã— 25% +
  ì¶œë ¥ í’ˆì§ˆ Ã— 25% +
  ì™„ì„±ë„ Ã— 20% +
  í™˜ê° ë°©ì§€ Ã— 15% +
  íš¨ìœ¨ì„± Ã— 10% +
  ì¶œì²˜ í’ˆì§ˆ Ã— 5%
```

### ë“±ê¸‰ (Grade)

| ì ìˆ˜ ë²”ìœ„ | ë“±ê¸‰ |
|---------|------|
| 9.5~10.0 | A+ |
| 9.0~9.5 | A |
| 8.5~9.0 | B+ |
| 8.0~8.5 | B |
| 7.5~8.0 | C+ |
| 7.0~7.5 | C |
| 6.0~7.0 | D |
| < 6.0 | F |

### ìë™ í‰ê°€ vs AI ì‹¬íŒ í‰ê°€

| í‰ê°€ í•­ëª© | ìë™ í‰ê°€ | AI ì‹¬íŒ |
|---------|----------|---------|
| ì‘ì—… ì„±ê³µë¥  | âœ“ | |
| ì™„ì„±ë„ | âœ“ | |
| íš¨ìœ¨ì„± | âœ“ | |
| ì¶œì²˜ í’ˆì§ˆ | âœ“ | |
| ì½˜í…ì¸  ë©”íŠ¸ë¦­ | âœ“ | |
| ì¶œë ¥ í’ˆì§ˆ | | âœ“ |
| í™˜ê° í˜„ìƒ | | âœ“ |
| ì •ì„±ì  ë¶„ì„ | | âœ“ |

**ê¶Œì¥ ì‚¬í•­**:
- ë¹ ë¥¸ í‰ê°€ê°€ í•„ìš”í•  ë•Œ: ìë™ í‰ê°€ë§Œ ì‚¬ìš© (`use_ai_judge=False`)
- ì •í™•í•œ í‰ê°€ê°€ í•„ìš”í•  ë•Œ: AI ì‹¬íŒ í¬í•¨ (`use_ai_judge=True`)

## API ë ˆí¼ëŸ°ìŠ¤

### `ReportEvaluator`

```python
class ReportEvaluator:
    def __init__(
        self,
        use_ai_judge: bool = True,
        ai_model: str = "gpt-4o-mini"
    ):
        """
        Args:
            use_ai_judge: AI ì‹¬íŒ ì‚¬ìš© ì—¬ë¶€
            ai_model: AI ì‹¬íŒì— ì‚¬ìš©í•  ëª¨ë¸
        """

    def evaluate_report(
        self,
        query: str,
        state: Dict[str, Any],
        report_text: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None,
        expected_requirements: Optional[List[str]] = None,
        expected_sections: Optional[List[str]] = None,
        expected_word_count: Optional[int] = None,
    ) -> EvaluationResult:
        """
        ë³´ê³ ì„œ ì¢…í•© í‰ê°€

        Args:
            query: ì›ë³¸ ì§ˆë¬¸/ìš”ì²­
            state: StreamingAgentState
            report_text: ìƒì„±ëœ ë³´ê³ ì„œ (ì—†ìœ¼ë©´ stateì—ì„œ ì¶”ì¶œ)
            metadata: ë©”íƒ€ë°ì´í„° (ì‹¤í–‰ ì‹œê°„, í† í° ì‚¬ìš©ëŸ‰ ë“±)
            expected_requirements: ê¸°ëŒ€ ìš”êµ¬ì‚¬í•­
            expected_sections: í•„ìˆ˜ ì„¹ì…˜
            expected_word_count: ê¸°ëŒ€ ë‹¨ì–´ ìˆ˜

        Returns:
            EvaluationResult: ì¢…í•© í‰ê°€ ê²°ê³¼
        """
```

### `EvaluationResult`

```python
class EvaluationResult(BaseModel):
    evaluation_id: str
    report_id: str
    evaluation_timestamp: str

    # ë©”íƒ€ë°ì´í„°
    query: str
    team_type: Optional[str]
    report_type: Optional[str]

    # í•µì‹¬ ì„±ê³¼ì§€í‘œ
    task_success: TaskSuccessMetrics
    output_quality: OutputQualityMetrics
    completeness: CompletenessMetrics
    hallucination: HallucinationMetrics

    # íš¨ìœ¨ì„± ì§€í‘œ
    efficiency: EfficiencyMetrics

    # ì¶”ê°€ ë©”íŠ¸ë¦­
    source_quality: SourceQualityMetrics
    content_metrics: ContentMetrics

    # ì¢…í•© ì ìˆ˜
    overall_score: float  # 0~10
    grade: str  # A+/A/B+/B/C+/C/D/F

    # ìƒì„¸ ë¶„ì„
    strengths: List[str]
    weaknesses: List[str]
    recommendations: List[str]

    # AI ì‹¬íŒ í‰ê°€
    ai_judge_evaluation: Optional[Dict[str, Any]]
```

## ì˜ˆì‹œ

### ì˜ˆì‹œ 1: ê¸°ë³¸ ì‚¬ìš©

```python
from app.core.evaluation import ReportEvaluator

# í‰ê°€ê¸° ìƒì„±
evaluator = ReportEvaluator(use_ai_judge=True)

# ìƒíƒœ ë°ì´í„° ì¤€ë¹„
state = {
    'original_query': '2024 AI ì‹œì¥ ë™í–¥',
    'final_answer': '# ë³´ê³ ì„œ ë‚´ìš©...',
    'step_results': [...],
    'execution_log': [...],
}

# í‰ê°€ ì‹¤í–‰
result = evaluator.evaluate_report(
    query=state['original_query'],
    state=state
)

# ê²°ê³¼ ì¶œë ¥
print(f"ì ìˆ˜: {result.overall_score}/10")
```

### ì˜ˆì‹œ 2: ë°°ì¹˜ í‰ê°€

```python
from app.core.evaluation import ReportEvaluator

evaluator = ReportEvaluator(use_ai_judge=False)  # ë¹ ë¥¸ í‰ê°€

reports = [state1, state2, state3]
results = []

for state in reports:
    result = evaluator.evaluate_report(
        query=state['original_query'],
        state=state
    )
    results.append(result)

# í†µê³„
avg_score = sum(r.overall_score for r in results) / len(results)
print(f"í‰ê·  ì ìˆ˜: {avg_score:.2f}")
```

### ì˜ˆì‹œ 3: ê²°ê³¼ ì €ì¥

```python
import json
from app.core.evaluation import ReportEvaluator

evaluator = ReportEvaluator()
result = evaluator.evaluate_report(query="...", state=state)

# JSONìœ¼ë¡œ ì €ì¥
with open('evaluation.json', 'w') as f:
    json.dump(result.model_dump(), f, indent=2)
```

## ë²¤ì¹˜ë§ˆí¬

### ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

| í‰ê°€ ëª¨ë“œ | í‰ê·  ì‹¤í–‰ ì‹œê°„ | ë¹„ìš© |
|----------|--------------|------|
| ìë™ í‰ê°€ë§Œ | ~1ì´ˆ | ë¬´ë£Œ |
| AI ì‹¬íŒ í¬í•¨ (gpt-4o-mini) | ~5-10ì´ˆ | ~$0.01 |
| AI ì‹¬íŒ í¬í•¨ (gpt-4o) | ~10-20ì´ˆ | ~$0.05 |

### ì •í™•ë„ ë²¤ì¹˜ë§ˆí¬

ì¸ê°„ í‰ê°€ì™€ì˜ ì¼ì¹˜ë„:
- ìë™ í‰ê°€: ~70%
- AI ì‹¬íŒ í¬í•¨: ~85%

## í™•ì¥ ë° ì»¤ìŠ¤í„°ë§ˆì´ì§•

### ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ì¶”ê°€

```python
from app.core.evaluation.automated_evaluator import AutomatedEvaluator

class CustomEvaluator(AutomatedEvaluator):
    def evaluate_custom_metric(self, report_text: str) -> float:
        # ì»¤ìŠ¤í…€ ë¡œì§
        score = custom_calculation(report_text)
        return score
```

### ì»¤ìŠ¤í…€ AI í”„ë¡¬í”„íŠ¸

```python
from app.core.evaluation.ai_judge_evaluator import AIJudgeEvaluator

class CustomAIJudge(AIJudgeEvaluator):
    def _build_quality_evaluation_prompt(self, query, report_text, sources):
        # ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸
        return f"Custom prompt: {query} {report_text}"
```

## ë¬¸ì œ í•´ê²°

### Q: AI ì‹¬íŒ í‰ê°€ê°€ ì‹¤íŒ¨í•©ë‹ˆë‹¤
A: `OPENAI_API_KEY` í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.

### Q: í‰ê°€ê°€ ë„ˆë¬´ ëŠë¦½ë‹ˆë‹¤
A: `use_ai_judge=False`ë¡œ ì„¤ì •í•˜ì—¬ ìë™ í‰ê°€ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.

### Q: í™˜ê° í˜„ìƒì´ ê°ì§€ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤
A: AI ì‹¬íŒ í‰ê°€ë¥¼ í™œì„±í™”í•˜ê³ , ë” ê°•ë ¥í•œ ëª¨ë¸(gpt-4o)ì„ ì‚¬ìš©í•˜ì„¸ìš”.

## ë¼ì´ì„ ìŠ¤ ë° ê¸°ì—¬

ì´ í”„ë¡œì íŠ¸ëŠ” Crowdworks ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì˜ ì¼ë¶€ì…ë‹ˆë‹¤.

## ì°¸ê³  ìë£Œ

- [BLEU Score](https://en.wikipedia.org/wiki/BLEU)
- [Mind2Web Benchmark](https://arxiv.org/abs/2306.06070)
- [LLM as a Judge](https://arxiv.org/abs/2306.05685)

---

**Last Updated**: 2024-01-15
