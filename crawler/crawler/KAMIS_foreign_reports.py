from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError
import requests
from pathlib import Path
from urllib.parse import urljoin
import time
from datetime import datetime, timedelta
import re
import json

BASE_URL = "https://www.kamis.or.kr/customer/trend/foreign_info/foreign_info.do"
DOWNLOAD_DIR = Path("/ReportPDF")
REFERENCE_PATH = Path("/elasticsearch/referenceURL.json")

DOWNLOAD_DIR.mkdir(exist_ok=True)
REFERENCE_PATH.parent.mkdir(parents=True, exist_ok=True)

# ê¸°ì¤€ì¼: 30ì¼ ì „
threshold = datetime.now() - timedelta(days=30)

# referenceURL.json ë¡œë“œ
if REFERENCE_PATH.exists():
    with open(REFERENCE_PATH, "r", encoding="utf-8") as f:
        reference_data = json.load(f)
else:
    reference_data = {}

session = requests.Session()
session.headers.update({"User-Agent": "Mozilla/5.0"})

def fetch_with_retry(url, retries=3, backoff=3):
    for i in range(retries):
        try:
            r = session.get(url, stream=True, timeout=30)
            r.raise_for_status()
            return r
        except Exception as e:
            print(f"    âš ï¸ ì‹œë„ {i+1} ì‹¤íŒ¨: {e}")
            time.sleep(backoff)
    raise RuntimeError("ìµœì¢… ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")

def process_item(page, idx):
    items = page.query_selector_all("ul.thum_list li")
    item = items[idx]
    item.click()
    try:
        page.wait_for_load_state("networkidle", timeout=30000)
    except PlaywrightTimeoutError:
        page.wait_for_load_state("domcontentloaded")

    # ì‘ì„±ì¼ ì¶”ì¶œ
    row = page.query_selector_all("table.tbl.row.board tbody tr")[1]
    tds = row.query_selector_all("td")
    if len(tds) < 2:
        print("    âŒ ì‘ì„±ì¼ ì…€ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ â†’ ì „ì²´ ì¢…ë£Œ")
        return False

    date_txt = tds[1].inner_text().strip()
    try:
        published = datetime.strptime(date_txt, "%Y-%m-%d")
    except ValueError:
        print(f"    âš ï¸ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ ({date_txt}) â†’ ì´ í˜ì´ì§€ë§Œ ìŠ¤í‚µ")
        page.go_back(wait_until="networkidle")
        return True

    if published < threshold:
        print(f"    âš ï¸ ê²Œì‹œì¼ {published.date()} (< {threshold.date()}) â†’ ì „ì²´ ì¢…ë£Œ")
        return False

    # ë§í¬ ë° íŒŒì¼ëª… ì¶”ì¶œ
    link = page.query_selector("ul.file_li li a")
    if not link:
        print("    âŒ PDF ë§í¬ ì—†ìŒ â†’ ë’¤ë¡œ")
        page.go_back(wait_until="networkidle")
        return True

    href = link.get_attribute("href")
    title = link.get_attribute("title") or link.inner_text().strip() or "unknown.pdf"
    pdf_url = href if href.startswith("http") else urljoin(BASE_URL, href)

    safe = re.sub(r'[\\/:*?"<>|]', '_', title)
    if not safe.lower().endswith(".pdf"):
        safe += ".pdf"
    out = DOWNLOAD_DIR / safe

    # ì´ë¯¸ ì¡´ì¬í•˜ë©´ ìŠ¤í‚µ
    if out.exists():
        print(f"    Â· ì´ë¯¸ ì¡´ì¬: {safe}")
    else:
        print(f"    â†“ ë‹¤ìš´ë¡œë“œ ì¤‘: {safe}")
        try:
            resp = fetch_with_retry(pdf_url)
            with open(out, "wb") as f:
                for chunk in resp.iter_content(1024):
                    if chunk:
                        f.write(chunk)
            print(f"      âœ… ì €ì¥ë¨: {safe}")
        except Exception as e:
            print(f"      âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            page.go_back(wait_until="networkidle")
            return True

    # referenceURL.json ê¸°ë¡
    if safe in reference_data:
        print(f"    Â· referenceURL.jsonì— ì´ë¯¸ ìˆìŒ: {safe}")
    else:
        reference_data[safe] = page.url
        with open(REFERENCE_PATH, "w", encoding="utf-8") as f:
            json.dump(reference_data, f, ensure_ascii=False, indent=2)
        print(f"    ğŸ“ referenceURL.jsonì— ì¶”ê°€ë¨: {safe}")

    page.go_back(wait_until="networkidle")
    time.sleep(1)
    return True

def main():
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        print(f"âœ… ì‹œì‘: {BASE_URL}")
        page.goto(BASE_URL, wait_until="networkidle")
        time.sleep(1)

        page_idx = 1
        while True:
            print(f"\nğŸ“„ ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ {page_idx}")
            if page.query_selector("ul.thum_list li.no_data"):
                print("  âš ï¸ ë°ì´í„° ì—†ìŒ â†’ ì¢…ë£Œ")
                break

            total = len(page.query_selector_all("ul.thum_list li"))
            print(f"  â†’ {total}ê°œ í•­ëª© ë°œê²¬")

            for idx in range(total):
                print(f"  â–¶ [{idx+1}/{total}] ì²˜ë¦¬ ì¤‘â€¦")
                ok = process_item(page, idx)
                if not ok:
                    browser.close()
                    return

            page_idx += 1
            next_url = f"{BASE_URL}?action=list&pagenum={page_idx}"
            print(f"â¡ï¸ ë‹¤ìŒ ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€: {next_url}")
            page.goto(next_url, wait_until="networkidle")
            time.sleep(1)

        browser.close()
        print("\nğŸ‰ ì™„ë£Œ!")

if __name__ == "__main__":
    main()
