from playwright.sync_api import sync_playwright
import requests
from pathlib import Path
from urllib.parse import urljoin
from datetime import datetime, timedelta
import json

BASE_LIST_URL = "https://www.atfis.or.kr/home/board/FB0002.do"
DOWNLOAD_DIR = Path("ReportPDF")
REFERENCE_PATH = Path("/elasticsearch/referenceURL.json")

DOWNLOAD_DIR.mkdir(exist_ok=True)
REFERENCE_PATH.parent.mkdir(parents=True, exist_ok=True)

# ê¸°ì¡´ referenceURL.json ë¶ˆëŸ¬ì˜¤ê¸°
if REFERENCE_PATH.exists():
    with open(REFERENCE_PATH, "r", encoding="utf-8") as f:
        reference_data = json.load(f)
else:
    reference_data = {}

threshold_date = datetime.now() - timedelta(days=30)

def save_reference(filename: str, detail_url: str):
    reference_data[filename] = detail_url
    with open(REFERENCE_PATH, "w", encoding="utf-8") as f:
        json.dump(reference_data, f, ensure_ascii=False, indent=2)

    # ì €ì¥ í™•ì¸
    try:
        with open(REFERENCE_PATH, "r", encoding="utf-8") as f:
            confirm_data = json.load(f)
        if filename in confirm_data:
            print(f"      ğŸ“ referenceURL.jsonì— ì¶”ê°€ë¨: {filename}")
        else:
            print(f"      â— referenceURL.jsonì— ì €ì¥ ì‹¤íŒ¨: {filename}")
    except Exception as e:
        print(f"      â— referenceURL.json í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

def main():
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()

        page_idx = 1
        while True:
            list_url = f"{BASE_LIST_URL}?subSkinYn=N&bcaId=0&pageIndex={page_idx}"
            page.goto(list_url, wait_until="networkidle")
            anchors = page.query_selector_all("ul.galleryList li a")
            if not anchors:
                print(f"âœ… Page {page_idx}: ë§í¬ ì—†ìŒ â†’ ì¢…ë£Œ")
                break

            detail_urls = [
                urljoin(list_url, a.get_attribute("href"))
                for a in anchors if a.get_attribute("href")
            ]

            print(f"\nâ—¾ í˜ì´ì§€ {page_idx}: {len(detail_urls)}ê°œ ìƒì„¸í˜ì´ì§€ ë§í¬ ì²˜ë¦¬ ì‹œì‘")
            for idx, detail_url in enumerate(detail_urls, start=1):
                print(f" â–¶ ìƒì„¸í˜ì´ì§€ [{idx}]: {detail_url}")

                if page_idx == 1 and idx == 1:
                    print("    ğŸ”¶ 1í˜ì´ì§€ ì²« ë²ˆì§¸ ìƒì„¸í˜ì´ì§€ì´ë¯€ë¡œ ë‹¤ìš´ë¡œë“œ ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue

                page.goto(detail_url, wait_until="networkidle")

                date_el = page.query_selector('ul.boardWriteInfo li[title="ì‘ì„±ì¼"]')
                if not date_el:
                    print("    âš ï¸ ì‘ì„±ì¼ ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ â†’ ìŠ¤í‚µ")
                    continue

                date_text = date_el.inner_text().strip().split("T")[0]
                try:
                    published = datetime.strptime(date_text, "%Y-%m-%d")
                except ValueError:
                    print(f"    âš ï¸ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ ({date_text}) â†’ ìŠ¤í‚µ")
                    continue

                if published < threshold_date:
                    print(f"    âš ï¸ ê²Œì‹œì¼ {published.date()} (< {threshold_date.date()}) â†’ ì „ì²´ ì¢…ë£Œ")
                    browser.close()
                    return

                items = page.query_selector_all("div.boardFile ul li")
                for li in items:
                    name_tag = li.query_selector("span.fileName")
                    btn_tag  = li.query_selector("a.btn")
                    if not (name_tag and btn_tag):
                        continue

                    filename = name_tag.inner_text().strip() or "unknown.pdf"
                    href     = btn_tag.get_attribute("href")
                    if not href:
                        continue

                    safe = filename.replace("/", "_")
                    out = DOWNLOAD_DIR / safe

                    if out.exists():
                        print(f"    Â· ì´ë¯¸ ì¡´ì¬: {safe}")
                        continue

                    pdf_url = urljoin(detail_url, href)
                    print(f"    â†“ ë‹¤ìš´ë¡œë“œ ì¤‘: {safe}")

                    res = requests.get(pdf_url, headers={"User-Agent": "Mozilla/5.0"},
                                       stream=True, timeout=30)
                    if res.status_code != 200:
                        print(f"      [WARN] HTTP {res.status_code} - ìŠ¤í‚µ")
                        continue

                    with open(out, "wb") as f:
                        for chunk in res.iter_content(chunk_size=1024):
                            if chunk:
                                f.write(chunk)
                    print(f"      âœ… ì €ì¥ë¨: {safe}")

                    if safe in reference_data:
                        print(f"      Â· referenceURL.jsonì— ì´ë¯¸ ìˆìŒ â†’ ê±´ë„ˆëœë‹ˆë‹¤.")
                    else:
                        save_reference(safe, detail_url)

            page_idx += 1

        browser.close()
        print("\nğŸ‰ ëª¨ë“  PDF ë‹¤ìš´ë¡œë“œ ë° referenceURL ì €ì¥ ì™„ë£Œ!")

if __name__ == "__main__":
    main()
