from playwright.sync_api import sync_playwright
import requests
from pathlib import Path
from urllib.parse import urljoin
import re
import time
from datetime import datetime, timedelta
import json

BASE_URL = "https://www.kati.net/board/etcReportList.do?menu_dept3=367"
DOWNLOAD_DIR = Path("ReportPDF")
REFERENCE_PATH = Path("/elasticsearch/referenceURL.json")

DOWNLOAD_DIR.mkdir(exist_ok=True)
REFERENCE_PATH.parent.mkdir(parents=True, exist_ok=True)

# referenceURL.json ë¡œë”©
if REFERENCE_PATH.exists():
    with open(REFERENCE_PATH, "r", encoding="utf-8") as f:
        reference_data = json.load(f)
else:
    reference_data = {}

def save_reference(filename: str, detail_url: str):
    if filename in reference_data:
        print(f"      Â· referenceURL.jsonì— ì´ë¯¸ ìˆìŒ: {filename} â†’ ê±´ë„ˆëœë‹ˆë‹¤.")
        return
    reference_data[filename] = detail_url
    with open(REFERENCE_PATH, "w", encoding="utf-8") as f:
        json.dump(reference_data, f, ensure_ascii=False, indent=2)
    print(f"      ğŸ“ referenceURL.jsonì— ì¶”ê°€ë¨: {filename}")

# 30ì¼ ì „ ê¸°ì¤€ì¼
threshold = datetime.now() - timedelta(days=30)

def main():
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        print(f"ğŸŒ ì´ë™: {BASE_URL}")
        page.goto(BASE_URL, wait_until="domcontentloaded", timeout=60000)
        print(f"âœ… í˜ì´ì§€ ë„ì°©: {page.title()}")

        page_num = 1
        while True:
            print(f"\nğŸ“„ í˜ì´ì§€ {page_num}")
            report_items = page.query_selector_all("div.report-row div.report-item")
            print(f"ğŸ” {len(report_items)}ê°œì˜ ë³´ê³ ì„œ ë°œê²¬")

            for idx, item in enumerate(report_items, start=1):
                title = item.query_selector("em.report-tit").inner_text().strip()
                raw_date = item.query_selector("span.report-date").inner_text().strip()

                m = re.search(r"\d{4}-\d{2}-\d{2}", raw_date)
                if not m:
                    print(f"âš ï¸ ë‚ ì§œ í˜•ì‹ ì´ìƒ ({raw_date}) â†’ ìŠ¤í‚µ")
                    continue
                date_str = m.group(0)
                print(f"\nâ–¶ï¸ [{idx}] {date_str} | {title}")

                published = datetime.strptime(date_str, "%Y-%m-%d")
                if published < threshold:
                    print(f"ğŸš« ê²Œì‹œì¼ {published.date()} (< {threshold.date()}) â†’ ì „ì²´ ì¢…ë£Œ")
                    browser.close()
                    return

                if "2023" in title:
                    print("ğŸš« 2023ë…„ ìë£Œ â†’ ìŠ¤í‚µ")
                    continue

                link = item.query_selector("div.download-area a[href*='file/down.do']")
                if not link:
                    print("âš ï¸ ë‹¤ìš´ë¡œë“œ ë§í¬ ì—†ìŒ â†’ ìŠ¤í‚µ")
                    continue

                href = link.get_attribute("href")
                full_url = href if href.startswith("http") else urljoin(BASE_URL, href)

                safe_name = re.sub(r'[\\/:*?"<>|]', "_", f"{title}.pdf")
                out_path = DOWNLOAD_DIR / safe_name

                if out_path.exists():
                    print("âœ”ï¸ ì´ë¯¸ ì¡´ì¬ â†’ ìŠ¤í‚µ")
                    continue

                print(f"â¬‡ï¸ ë‹¤ìš´ë¡œë“œ ì¤‘: {safe_name}")
                try:
                    res = requests.get(full_url, stream=True, timeout=60)
                    res.raise_for_status()
                    with open(out_path, "wb") as f:
                        for chunk in res.iter_content(1024):
                            if chunk:
                                f.write(chunk)
                    print(f"âœ… ì €ì¥ë¨: {out_path}")
                    save_reference(safe_name, full_url)
                except Exception as e:
                    print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")

            # ë‹¤ìŒ í˜ì´ì§€ ì°¾ê¸°
            next_el = page.query_selector("div.paging a.current + a")
            onclick = next_el.get_attribute("onclick") if next_el else None
            if not onclick:
                print("âœ… ë§ˆì§€ë§‰ í˜ì´ì§€ ë„ë‹¬ â†’ ì¢…ë£Œ")
                break

            m2 = re.search(r"getPaging\((\d+)\)", onclick)
            if not m2:
                print("âš ï¸ ë‹¤ìŒ í˜ì´ì§€ ë²ˆí˜¸ ì—†ìŒ â†’ ì¢…ë£Œ")
                break

            next_page = int(m2.group(1))
            print(f"â¡ï¸ ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™: {next_page}")
            page.evaluate("num => getPaging(num)", next_page)
            time.sleep(1.5)
            page_num = next_page

        print("\nğŸ‰ ëª¨ë“  ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
        browser.close()

if __name__ == "__main__":
    main()
