from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError
import requests
from pathlib import Path
from urllib.parse import urljoin
import time
from datetime import datetime, timedelta
import re
import json

BASE_DOMAIN = "https://www.kamis.or.kr"
TAB_PATHS = [
    "/customer/nature/domestic.do",
    "/customer/nature/agriculture.do",
    "/customer/nature/schoolfood.do",
    "/customer/nature/market.do",
    "/customer/nature/customer.do",
    "/customer/nature/overseas.do",
]
DOWNLOAD_DIR = Path("/ReportPDF")
DOWNLOAD_DIR.mkdir(exist_ok=True)

REFERENCE_PATH = Path("/elasticsearch/referenceURL.json")
REFERENCE_PATH.parent.mkdir(parents=True, exist_ok=True)

# 30ì¼ ì „ ê¸°ì¤€ì¼
threshold = datetime.now() - timedelta(days=30)

# referenceURL.json ë¡œë”©
if REFERENCE_PATH.exists():
    with open(REFERENCE_PATH, "r", encoding="utf-8") as f:
        reference_data = json.load(f)
else:
    reference_data = {}

session = requests.Session()
session.headers.update({"User-Agent": "Mozilla/5.0"})

def fetch_with_retry(url, retries=3, backoff=3):
    for attempt in range(1, retries + 1):
        try:
            r = session.get(url, stream=True, timeout=30)
            r.raise_for_status()
            return r
        except Exception as e:
            print(f"    âš ï¸ ì‹œë„ {attempt} ì‹¤íŒ¨: {e}")
            if attempt == retries:
                raise
            time.sleep(backoff)

def process_item(page, idx):
    """
    í´ë¦­ â†’ ì‘ì„±ì¼ ì²´í¬ â†’ 30ì¼ ì´ë‚´ë©´ ë‹¤ìš´ë¡œë“œ
    30ì¼ ì´ì „ì´ë©´ False ë°˜í™˜ â†’ ì´ íƒ­(loop) ê±´ë„ˆë›¸ ì‹ í˜¸
    """
    items = page.query_selector_all("ul.thum_list li:not(.no_data)")
    item = items[idx]
    item.click()
    try:
        page.wait_for_load_state("networkidle", timeout=30000)
    except PlaywrightTimeoutError:
        page.wait_for_load_state("domcontentloaded")

    # ì‘ì„±ì¼ ì½ê¸° (XPath)
    date_td = page.query_selector(
        "//th[normalize-space(text())='ì‘ì„±ì¼']/following-sibling::td"
    )
    if not date_td:
        print("    âš ï¸ ì‘ì„±ì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ â†’ ì´ í˜ì´ì§€ë§Œ ìŠ¤í‚µ")
        page.go_back(wait_until="networkidle")
        return True

    date_text = date_td.inner_text().strip()
    try:
        published = datetime.strptime(date_text, "%Y-%m-%d")
    except ValueError:
        print(f"    âš ï¸ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ ({date_text}) â†’ ì´ í˜ì´ì§€ë§Œ ìŠ¤í‚µ")
        page.go_back(wait_until="networkidle")
        return True

    if published < threshold:
        print(f"    âš ï¸ ê²Œì‹œì¼ {published.date()} (< {threshold.date()}) â†’ ì´ íƒ­ ê±´ë„ˆëœ€")
        page.go_back(wait_until="networkidle")
        return False  # signal to skip rest of this tab

    # PDF ë§í¬ & ë‹¤ìš´ë¡œë“œ
    link = page.query_selector("ul.file_li li a")
    if not link:
        print("    âŒ PDF ë§í¬ ì—†ìŒ â†’ ë’¤ë¡œ")
        page.go_back(wait_until="networkidle")
        return True

    href = link.get_attribute("href")
    title = link.get_attribute("title") or link.inner_text().strip() or "unknown.pdf"
    pdf_url = href if href.startswith("http") else urljoin(BASE_DOMAIN, href)

    safe = re.sub(r'[\\/:*?"<>|]', "_", title)
    if not safe.lower().endswith(".pdf"):
        safe += ".pdf"
    out = DOWNLOAD_DIR / safe

    if out.exists():
        print(f"    Â· ì´ë¯¸ ì¡´ì¬: {safe}")
    else:
        print(f"    â†“ ë‹¤ìš´ë¡œë“œ ì¤‘: {safe}")
        try:
            resp = fetch_with_retry(pdf_url)
            with open(out, "wb") as f:
                for chunk in resp.iter_content(1024):
                    if chunk:
                        f.write(chunk)
            print(f"      âœ… ì €ì¥ë¨: {safe}")
        except Exception as e:
            print(f"      âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")

    # referenceURL ê¸°ë¡
    if safe in reference_data:
        print(f"    Â· referenceURL.jsonì— ì´ë¯¸ ìˆìŒ: {safe}")
    else:
        reference_data[safe] = page.url
        with open(REFERENCE_PATH, "w", encoding="utf-8") as f:
            json.dump(reference_data, f, ensure_ascii=False, indent=2)
        print(f"    ğŸ“ referenceURL.jsonì— ì¶”ê°€ë¨: {safe}")

    page.go_back(wait_until="networkidle")
    time.sleep(1)
    return True

def main():
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()

        for tab_path in TAB_PATHS:
            tab_url = f"{BASE_DOMAIN}{tab_path}"
            print(f"\n==============\nâ–¶ íƒ­: {tab_url}")
            page.goto(tab_url, wait_until="networkidle")
            time.sleep(1)

            page_idx = 1
            while True:
                print(f"\nğŸ“„ í˜ì´ì§€ {page_idx}")
                if page.query_selector("ul.thum_list li.no_data"):
                    print("  âš ï¸ ë°ì´í„° ì—†ìŒ â†’ íƒ­ ì¢…ë£Œ")
                    break

                items = page.query_selector_all("ul.thum_list li:not(.no_data)")
                total = len(items)
                print(f"  â†’ {total}ê°œ í•­ëª© ë°œê²¬")

                # ì´ íƒ­ì—ì„œ ë‚ ì§œ ë¯¸ë‹¬ ì‹œ ë¹ ì ¸ë‚˜ì˜¤ëŠ” í”Œë˜ê·¸
                skip_tab = False

                for idx in range(total):
                    print(f"  â–¶ [{idx+1}/{total}] ì²˜ë¦¬ ì¤‘â€¦")
                    ok = process_item(page, idx)
                    if not ok:
                        skip_tab = True
                        break

                if skip_tab:
                    print("  â­ï¸ ì´ íƒ­ ê±´ë„ˆë›°ê³  ë‹¤ìŒ íƒ­ìœ¼ë¡œ ì´ë™í•©ë‹ˆë‹¤.")
                    break  # exit while True for this tab

                # ë‹¤ìŒ í˜ì´ì§€ ì´ë™
                page_idx += 1
                next_url = f"{tab_url}?action=list&pagenum={page_idx}"
                print(f"â¡ï¸ ë‹¤ìŒ í˜ì´ì§€: {next_url}")
                page.goto(next_url, wait_until="networkidle")
                time.sleep(1)

        browser.close()
        print("\nğŸ‰ ëª¨ë“  PDF ë‹¤ìš´ë¡œë“œ ë° referenceURL ê¸°ë¡ ì™„ë£Œ!")

if __name__ == "__main__":
    main()
