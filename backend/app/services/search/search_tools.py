import os
from typing import List
import requests
import json
# import asyncio  # ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
# import concurrent.futures  # ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
import io
from pypdf import PdfReader

# ê° RAG íˆ´ì˜ ë©”ì¸ í•¨ìˆ˜ë¥¼ import
from ..database.postgres_rag_tool import postgres_rdb_search
from ..database.neo4j_rag_tool import neo4j_search_sync
from ..database.elasticsearch.elastic_search_rag_tool import MultiIndexRAGSearchEngine, RAGConfig


from ...core.models.models import ScrapeInput



from playwright.sync_api import sync_playwright
from langchain_core.tools import tool
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# ì„¸ì…˜ë³„ ë¡œê·¸ ì‹œìŠ¤í…œ import
from ...utils.session_logger import print_session_separated, session_print




# --------------------------------------------------
# Tool Definitions
# --------------------------------------------------

@tool
def debug_web_search(query: str) -> str:
    """
    ë‚´ë¶€ ë°ì´í„°ë² ì´ìŠ¤(RDB, Vector, Graph)ì— ì—†ëŠ” ìµœì‹  ì •ë³´ë‚˜ ì¼ë°˜ì ì¸ ì§€ì‹ì„ ì‹¤ì œ ì›¹(êµ¬ê¸€)ì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    - ì‚¬ìš© ì‹œì :
      1. 'ì˜¤ëŠ˜', 'í˜„ì¬', 'ì‹¤ì‹œê°„' ë“± ë‚´ë¶€ DBì— ì•„ì§ ë°˜ì˜ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆëŠ” ìµœì‹  ì •ë³´ê°€ í•„ìš”í•  ë•Œ (ì˜ˆ: 'ì˜¤ëŠ˜ì Aê¸°ì—… ì£¼ê°€', 'í˜„ì¬ ì„œìš¸ ë‚ ì”¨')
      2. ë‚´ë¶€ DBì˜ ì£¼ì œ(ë†ì—…/ì‹í’ˆ)ë¥¼ ë²—ì–´ë‚˜ëŠ” ì¼ë°˜ì ì¸ ì§ˆë¬¸ì¼ ë•Œ (ì˜ˆ: 'ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì•¼?')
      3. íŠ¹ì • ì¸ë¬¼, ì‚¬ê±´, ì œí’ˆì— ëŒ€í•œ ìµœì‹  ë‰´ìŠ¤ì™€ ê°™ì´ ì‹œì˜ì„±ì´ ë§¤ìš° ì¤‘ìš”í•œ ì •ë³´ë¥¼ ì°¾ì„ ë•Œ
    - ì£¼ì˜: ë†ì‚°ë¬¼ ì‹œì„¸, ì˜ì–‘ ì •ë³´, ë¬¸ì„œ ë‚´ìš© ë¶„ì„, ë°ì´í„° ê´€ê³„ ë¶„ì„ ë“± ë‚´ë¶€ DBë¡œ í•´ê²° ê°€ëŠ¥í•œ ì§ˆë¬¸ì—ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”. ìµœí›„ì˜ ìˆ˜ë‹¨ìœ¼ë¡œ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
    """
    session_print("WebSearch", f"Web ê²€ìƒ‰ ì‹¤í–‰: {query}")
    try:
        api_key = os.environ.get("SERPER_API_KEY")
        if not api_key:
            return "SERPER_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

        url = "https://google.serper.dev/search"
        headers = {"X-API-KEY": api_key, "Content-Type": "application/json"}
        payload = {"q": query, "num": 5, "gl": "kr", "hl": "ko"}  # ê²°ê³¼ ìˆ˜ ì¦ê°€
        response = requests.post(url, headers=headers, json=payload, timeout=10)

        if response.status_code == 200:
            data = response.json()
            results = []

            # Answer box ìš°ì„  ì²˜ë¦¬
            if "answerBox" in data:
                answer = data["answerBox"].get("answer", "")
                if answer:
                    results.append({
                        "title": "Direct Answer",
                        "snippet": answer,
                        "link": "google_answer_box",
                        "source": "google_answer_box"
                    })

            # Organic ê²°ê³¼ ì²˜ë¦¬
            if "organic" in data and data["organic"]:
                for result in data["organic"][:5]:  # ìƒìœ„ 5ê°œ
                    link = result.get("link", "")
                    title = result.get("title", "No title")
                    snippet = result.get("snippet", "No snippet")

                    # ìœ íš¨í•œ URLì¸ì§€ í™•ì¸
                    if link and link.startswith(('http://', 'https://')):
                        # PDF íŒŒì¼ì¸ ê²½ìš° ì œëª© ìˆ˜ì •
                        if link.endswith('.pdf'):
                            title = f"PDF: {title}"

                        results.append({
                            "title": title,
                            "snippet": snippet,
                            "link": link,
                            "source": "web_search"
                        })
                    else:
                        print(f"- ìœ íš¨í•˜ì§€ ì•Šì€ URL ìŠ¤í‚µ: {link}")

            if results:
                # í…ìŠ¤íŠ¸ í˜•íƒœë¡œ ë°˜í™˜ (ReAct ì—ì´ì „íŠ¸ìš©)
                result_text = f"ì›¹ ê²€ìƒ‰ ê²°ê³¼ (ê²€ìƒ‰ì–´: {query}):\n\n"
                for i, result in enumerate(results):
                    result_text += f"{i+1}. {result['title']}\n"
                    result_text += f"   ì¶œì²˜ ë§í¬: {result['link']}\n"
                    result_text += f"   ìš”ì•½: {result['snippet']}\n\n"

                session_print("WebSearch", f"ìœ íš¨í•œ ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ")
                return result_text
            else:
                return f"'{query}'ì— ëŒ€í•œ ìœ íš¨í•œ ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        else:
            return f"API ì˜¤ë¥˜: {response.status_code}, {response.text}"
    except Exception as e:
        return f"ì›¹ ê²€ìƒ‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}"


@tool
def scrape_and_extract_content(action_input: str) -> str:
    """
    ì£¼ì–´ì§„ URLì˜ ì›¹í˜ì´ì§€ ë˜ëŠ” PDFì— ì ‘ì†í•˜ì—¬ ë³¸ë¬¸ ë‚´ìš©ì„ ì¶”ì¶œí•˜ê³ , ì‚¬ìš©ìì˜ ì›ë˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ í•µì‹¬ ì •ë³´ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤.
    Action Inputì€ ë°˜ë“œì‹œ '{"url": "...", "query": "..."}' í˜•íƒœì˜ JSON(ë”•ì…”ë„ˆë¦¬) ë¬¸ìì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
    """
    try:
        input_data = json.loads(action_input)
        url = input_data['url']
        query = input_data['query']
        session_print("Scraper", f"Scraping ì‹œì‘ (URL: {url}, Query: {query})")

    except (json.JSONDecodeError, KeyError) as e:
        return f"ì…ë ¥ê°’ íŒŒì‹± ì˜¤ë¥˜: Action Inputì€ '{{\"url\": \"...\", \"query\": \"...\"}}' í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤. ì˜¤ë¥˜: {e}"

    # URLì´ PDFë¡œ ëë‚˜ëŠ”ì§€ì— ë”°ë¼ ë‹¤ë¥¸ ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ í˜¸ì¶œ
    if url.lower().endswith('.pdf'):
        return _scrape_pdf_content(url, query)
    else:
        return _scrape_html_content(url, query)


def _scrape_pdf_content(url: str, query: str) -> str:
    """PDF URLì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  ìš”ì•½í•©ë‹ˆë‹¤."""
    print(f"  â†’ PDF ì²˜ë¦¬ ëª¨ë“œ ì‹œì‘: {url}")
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status() # HTTP ì˜¤ë¥˜ê°€ ìˆìœ¼ë©´ ì˜ˆì™¸ ë°œìƒ

        with io.BytesIO(response.content) as f:
            reader = PdfReader(f)
            text = "".join(page.extract_text() for page in reader.pages if page.extract_text())

        if not text:
            return "PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤."

        print(f"  âœ“ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ ({len(text)}ì)")
        return _extract_key_info(text, query)

    except Exception as e:
        return f"PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"


def _scrape_html_content(url: str, query: str) -> str:
    """HTML ì›¹í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  ìš”ì•½í•©ë‹ˆë‹¤."""
    print(f"  â†’ HTML ì²˜ë¦¬ ëª¨ë“œ ì‹œì‘: {url}")
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            page = browser.new_page()
            page.goto(url, timeout=30000) # íƒ€ì„ì•„ì›ƒ ì¦ê°€

            # ë” ê²¬ê³ í•œ ë°©ì‹ìœ¼ë¡œ ë³¸ë¬¸ íƒìƒ‰ (article -> main -> body ìˆœì„œ)
            locators = ['article', 'main', '[role="main"]']
            content = ""
            for loc in locators:
                try:
                    # í•´ë‹¹ ì„ íƒìì˜ ì²« ë²ˆì§¸ ìš”ì†Œë§Œ ì„ íƒ
                    content = page.locator(loc).first.inner_text(timeout=2000)
                    if len(content) > 100:
                        print(f"  âœ“ '{loc}' ì„ íƒìì—ì„œ ë³¸ë¬¸ ë°œê²¬")
                        break
                except Exception:
                    continue

            # ìœ„ì—ì„œ ëª» ì°¾ìœ¼ë©´ body ì „ì²´ë¥¼ ìµœí›„ì˜ ìˆ˜ë‹¨ìœ¼ë¡œ ì‚¬ìš©
            if not content:
                content = page.locator('body').inner_text(timeout=2000)
                print("  âœ“ ìµœí›„ì˜ ìˆ˜ë‹¨ìœ¼ë¡œ 'body' ì„ íƒì ì‚¬ìš©")

            browser.close()

            if not content:
                return "ì›¹í˜ì´ì§€ì—ì„œ ë‚´ìš©ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤."

            print(f"  âœ“ HTML í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ ({len(content)}ì)")
            return _extract_key_info(content, query)

    except Exception as e:
        return f"ì›¹í˜ì´ì§€ ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"


def _extract_key_info(content: str, query: str) -> str:
    """ì¶”ì¶œëœ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ LLMì„ ì´ìš©í•´ í•µì‹¬ ì •ë³´ë¥¼ ë‹¤ì‹œ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    print(f"  â†’ LLM ë¶„ì„ ì‹œì‘...")
    try:
        # Gemini 2.5 Flash-Liteë¡œ ë³€ê²½ (ë¹ ë¥¸ ì •ë³´ ì¶”ì¶œ)
        extractor_llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash-lite", temperature=0)
        prompt = ChatPromptTemplate.from_template(
            """ë‹¹ì‹ ì€ ìœ ëŠ¥í•œ ë°ì´í„° ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê°€ì¥ ê´€ë ¨ ìˆëŠ” í•µì‹¬ ì •ë³´, íŠ¹íˆ ìˆ˜ì¹˜ ë°ì´í„°, í†µê³„, ì£¼ìš” ì‚¬ì‹¤ë“¤ì„ ì •í™•í•˜ê²Œ ì¶”ì¶œí•˜ê³  ìš”ì•½í•´ì£¼ì„¸ìš”.

            ì‚¬ìš©ì ì§ˆë¬¸: "{user_query}"

            ì›ë³¸ í…ìŠ¤íŠ¸ (ìµœëŒ€ 15000ì):
            {web_content}

            í•µì‹¬ ì •ë³´ ìš”ì•½:"""
        )
        chain = prompt | extractor_llm | StrOutputParser()

        extracted_info = chain.invoke({
            "user_query": query,
            "web_content": content[:15000]
        })

        print(f"  âœ“ LLM ë¶„ì„ ì™„ë£Œ")
        return extracted_info
    except Exception as e:
        return f"LLM ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"


@tool
def rdb_search(query: str) -> str:
    """
    # PostgreSQL DBì— ì €ì¥ëœ ì‹ìì¬ ê´€ë ¨ ì •í˜• ë°ì´í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.

    ## í¬í•¨ëœ ë°ì´í„°:
    1. ì‹ìì¬ ì˜ì–‘ì†Œ ì •ë³´ (ë‹¨ë°±ì§ˆ, íƒ„ìˆ˜í™”ë¬¼, ì§€ë°©, ë¹„íƒ€ë¯¼, ë¯¸ë„¤ë„ ë“±ì˜ ìƒì„¸ ì˜ì–‘ ì„±ë¶„)
    2. ë†ì‚°ë¬¼/ìˆ˜ì‚°ë¬¼ ì‹œì„¸ ë°ì´í„° (ë§¤ì¼ í¬ë¡¤ë§ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë˜ëŠ” ìµœì‹  ê°€ê²© ì •ë³´)

    ### ê²€ìƒ‰ ì „ëµ - ê²°ê³¼ê°€ ë§Œì¡±ìŠ¤ëŸ½ì§€ ì•Šìœ¼ë©´ ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ì¬ì‹œë„í•˜ì„¸ìš”:

    1ì°¨ ì‹œë„: êµ¬ì²´ì ì¸ ì‹í’ˆëª…ìœ¼ë¡œ ê²€ìƒ‰
    - ì˜ˆ: "ìœ ê¸°ë† ì±„ì†Œ" (not "organic vegetables")
    - ì˜ˆ: "ì¹œí™˜ê²½ ë†ì‚°ë¬¼" (not "eco-friendly agricultural products")

    2ì°¨ ì‹œë„: ë” ë„“ì€ ì¹´í…Œê³ ë¦¬ë¡œ ê²€ìƒ‰
    - ì˜ˆ: "ì±„ì†Œë¥˜", "ê³¼ì¼ë¥˜", "ê³¡ë¬¼ë¥˜"

    3ì°¨ ì‹œë„: ê´€ë ¨ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
    - ì˜ˆ: "ë†ì‚°ë¬¼ ì˜ì–‘", "ì‹í’ˆ ì„±ë¶„"

    ## ê²€ìƒ‰ ê²°ê³¼ í‰ê°€ ê¸°ì¤€:
    - ê²°ê³¼ê°€ ì§ˆë¬¸ê³¼ ê´€ë ¨ ì—†ìœ¼ë©´ â†’ ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ì¬ì‹œë„
    - ì˜ì–‘ì„±ë¶„ë§Œ ë‚˜ì˜¤ëŠ”ë° ìƒì‚°/ì†Œë¹„ ì •ë³´ê°€ í•„ìš”í•˜ë©´ â†’ vector_dbë‚˜ graph_db ì‚¬ìš©
    - ê°€ê²© ì •ë³´ë§Œ ë‚˜ì˜¤ëŠ”ë° ë‹¤ë¥¸ ì •ë³´ê°€ í•„ìš”í•˜ë©´ â†’ ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ì¬ì‹œë„ í›„ ë‹¤ë¥¸ DB í™œìš©

    ì‚¬ìš© ì‹œì :
    1. íŠ¹ì • ì‹ìì¬ì˜ ì˜ì–‘ ì„±ë¶„ì„ ì •í™•íˆ ì•Œê³  ì‹¶ì„ ë•Œ
    2. ë†ì‚°ë¬¼/ìˆ˜ì‚°ë¬¼ì˜ í˜„ì¬ ì‹œì„¸ë‚˜ ê°€ê²© ë³€ë™ì„ í™•ì¸í•˜ê³  ì‹¶ì„ ë•Œ
    3. íŠ¹ì • ì§€ì—­/í’ˆì¢…ë³„ ê°€ê²© ë¹„êµê°€ í•„ìš”í•  ë•Œ
    4. ì˜ì–‘ì†Œ ê¸°ì¤€ìœ¼ë¡œ ì‹ìì¬ë¥¼ ë¹„êµ/ë¶„ì„í•˜ê³  ì‹¶ì„ ë•Œ

    ## ê²€ìƒ‰ íŒ:
    - ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ê²€ìƒ‰ (ì˜ˆ: "ìœ ê¸°ë†" not "organic")
    - ì²« ê²€ìƒ‰ ê²°ê³¼ê°€ ì›í•˜ëŠ” ì •ë³´ê°€ ì•„ë‹ˆë©´ í‚¤ì›Œë“œë¥¼ ë°”ê¿”ì„œ 2-3íšŒ ë” ì‹œë„
    - êµ¬ì²´ì ì¸ ì‹í’ˆëª… â†’ ì¹´í…Œê³ ë¦¬ëª… â†’ ê´€ë ¨ í‚¤ì›Œë“œ ìˆœìœ¼ë¡œ ì ì§„ì  í™•ì¥
    - ìƒì‚°ëŸ‰/ì†Œë¹„ëŸ‰ í†µê³„ê°€ í•„ìš”í•˜ë©´ ì´ DBë¡œëŠ” í•œê³„ê°€ ìˆìœ¼ë‹ˆ ë‹¤ë¥¸ DB í™œìš©

    ì˜ˆì‹œ ê²€ìƒ‰ ì‹œí€€ìŠ¤:
    1. "ìœ ê¸°ë† ì±„ì†Œ" â†’ ê´€ë ¨ ì—†ëŠ” ê²°ê³¼ â†’
    2. "ì±„ì†Œë¥˜ ì˜ì–‘" â†’ ì¼ë¶€ ê´€ë ¨ ê²°ê³¼ â†’
    3. "ì¹œí™˜ê²½ ë†ì‚°ë¬¼" â†’ ì›í•˜ëŠ” ê²°ê³¼ ë˜ëŠ” ë‹¤ë¥¸ DBë¡œ ì „í™˜

    ì£¼ì˜: ì‹œì„¸ ë°ì´í„°ëŠ” ë§¤ì¼ ì—…ë°ì´íŠ¸ë˜ë¯€ë¡œ 'ì˜¤ëŠ˜', 'í˜„ì¬' ê°€ê²© ì§ˆë¬¸ì— ì í•©í•©ë‹ˆë‹¤.
    """

    session_print("RDB", f"PostgreSQL ê²€ìƒ‰ ì‹œì‘: {query}")
    try:
        result = postgres_rdb_search(query)
        # print(f"- ê²€ìƒ‰ ê²°ê³¼: {result}")
        return result
    except Exception as e:
        error_msg = f"PostgreSQL ì—°ê²° ì˜¤ë¥˜: {str(e)}"
        print(f">> {error_msg}")
        return error_msg



@tool
def vector_db_search(query: str, top_k = 20) -> List:
    """
    Elasticsearchì— ì €ì¥ëœ ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸, ë…¼ë¬¸, ë³´ê³ ì„œ ì „ë¬¸ì—ì„œ 'ì˜ë¯¸ ê¸°ë°˜'ìœ¼ë¡œ ìœ ì‚¬í•œ ë‚´ìš©ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    """
    try:
        config = RAGConfig()
        print(">> Vector DB ê²€ìƒ‰ ì´ˆê¸°í™” ì¤‘...")

        # Google API Key ì‚¬ìš© (OpenAIê°€ ì•„ë‹˜)
        google_api_key = os.environ.get("GOOGLE_API_KEY") or os.environ.get("GEMINI_API_KEY")

        search_engine = MultiIndexRAGSearchEngine(google_api_key=google_api_key, config=config)

        session_print("VectorDB", f"Vector DB ê²€ìƒ‰ ì‹œì‘: {query}")
        results = search_engine.advanced_rag_search(query)
        top_results = results.get('results', [])[:top_k]

        session_print("VectorDB", f"ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(top_results)}ê°œ")

        if not top_results:
            print(">> Vector DB ê²€ìƒ‰ ì™„ë£Œ: ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ")
            return []

        # ReActê°€ íŒë‹¨í•˜ê¸° ì‰¬ìš´ í•µì‹¬ ì •ë³´ë§Œ ì¶”ì¶œ
        processed_docs = []
        for i, doc in enumerate(top_results):
            # í•µì‹¬ í•„ë“œ ì¶”ì¶œ
            title = doc.get('name', doc.get('title', f'Document {i+1}'))
            content = doc.get('page_content', doc.get('content', ''))
            metadata = doc.get('meta_data', doc.get('metadata', {}))
            similarity = doc.get('score', doc.get('similarity_score', 0.7))
            relavance = doc.get('relevance_score', similarity)  # relevance_scoreê°€ ì—†ìœ¼ë©´ similarityë¡œ ëŒ€ì²´
            rerank_score = doc.get('rerank_score', 0.0)
            # ì¶œì²˜ ì •ë³´ ë” ëª…í™•íˆ í¬í•¨
            source_info = metadata.get('document_link', 'Vector DB')
            page_number = metadata.get('page_number', 'N/A')

            formatted_result = {
                "content": content,
                "title": title,
                "document_id": f"doc_{i+1}",
                "similarity_score": similarity,
                "metadata": metadata,
                "source_url": source_info,  # ì¶œì²˜ ì •ë³´ ì¶”ê°€
                "page_number": page_number,  # í˜ì´ì§€ ë²ˆí˜¸ ì¶”ê°€
                "relevance_score": relavance,
                "score": rerank_score
            }
            processed_docs.append(formatted_result)

        return processed_docs

    except Exception as e:
        print(f"Vector DB ê²€ìƒ‰ ì˜¤ë¥˜: {e}")


@tool
def graph_db_search(query: str) -> str:
    """
    ìƒìœ„ ë ˆë²¨ ë„êµ¬ ì§„ì…ì :
    - ì‹¤í–‰ì¤‘ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ìˆìœ¼ë©´ ThreadPoolì—ì„œ ë™ê¸° í•¨ìˆ˜ ì‹¤í–‰
    - ì—†ìœ¼ë©´ ë™ê¸° ì‹¤í–‰
    """
    session_print("GraphDB", f"Graph DB search called: {query}")
    try:
        # ì´ë²¤íŠ¸ ë£¨í”„ ìƒíƒœì™€ ê´€ê³„ì—†ì´ ì§ì ‘ ë™ê¸° í˜¸ì¶œë¡œ í†µì¼
        print("Using direct sync call for Graph DB")
        return neo4j_search_sync(query)
    except Exception as e:
        print(f"graph_db_search error: {e}")
        return f"Graph DB ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}"


@tool
def arxiv_search(query: str, max_results: int = 10) -> str:
    """
    # arXiv í•™ìˆ  ë…¼ë¬¸ ê²€ìƒ‰ - ì‹í’ˆê³¼í•™/AI/ìƒëª…ê³µí•™ ë¶„ì•¼ ìµœì‹  ì—°êµ¬
    
    ## ì‚¬ìš© ëª©ì :
    - ì‹ ì œí’ˆ ê°œë°œì„ ìœ„í•œ ìµœì‹  ê³¼í•™ì  ì—°êµ¬ ë™í–¥ íŒŒì•…
    - ì‹í’ˆ ê³µí•™, ì˜ì–‘í•™, ìƒëª…ê³µí•™ ê´€ë ¨ í•™ìˆ ì  ê·¼ê±° í™•ë³´
    - AI/ML ê¸°ë°˜ ì‹í’ˆ ë¶„ì„ ë° ì˜ˆì¸¡ ëª¨ë¸ ì—°êµ¬
    - ëŒ€ì²´ ì‹í’ˆ, ê¸°ëŠ¥ì„± ì›ë£Œ, ì‹ ì†Œì¬ ê°œë°œ ì—°êµ¬
    
    ## ì£¼ìš” ê²€ìƒ‰ ë¶„ì•¼:
    1. **ì‹í’ˆê³¼í•™**: food science, nutrition, fermentation, food engineering
    2. **ìƒëª…ê³µí•™**: biotechnology, synthetic biology, protein engineering  
    3. **ë†ì—…ê¸°ìˆ **: agriculture, crop science, precision farming
    4. **AI/ë°ì´í„°**: machine learning for food, predictive analytics
    5. **ì§€ì†ê°€ëŠ¥ì„±**: sustainable food, alternative protein, food waste
    
    ## ê²€ìƒ‰ ì „ëµ:
    - ì˜ë¬¸ í‚¤ì›Œë“œ í•„ìˆ˜ (arXivëŠ” ì˜ë¬¸ ë…¼ë¬¸ë§Œ ì œê³µ)
    - êµ¬ì²´ì ì¸ ê¸°ìˆ ëª…ì´ë‚˜ ë°©ë²•ë¡  í¬í•¨ ì‹œ ì •í™•ë„ í–¥ìƒ
    - ìµœì‹ ìˆœ ì •ë ¬ë¡œ ìµœê·¼ ì—°êµ¬ íŠ¸ë Œë“œ íŒŒì•…
    
    ## í™œìš© ì˜ˆì‹œ:
    - "ëŒ€ì²´ìœ¡ ê°œë°œì„ ìœ„í•œ ì‹ë¬¼ì„± ë‹¨ë°±ì§ˆ ì—°êµ¬" 
    - "ë°œíš¨ ê¸°ìˆ ì„ í™œìš©í•œ ê¸°ëŠ¥ì„± ì‹í’ˆ ê°œë°œ"
    - "AI ê¸°ë°˜ ì‹í’ˆ í’ˆì§ˆ ì˜ˆì¸¡ ëª¨ë¸"
    - "ì§€ì†ê°€ëŠ¥í•œ ì‹í’ˆ í¬ì¥ì¬ ê°œë°œ"
    
    ì£¼ì˜: í•™ìˆ  ë…¼ë¬¸ì´ë¯€ë¡œ ì‹¤ë¬´ ì ìš© ì‹œ ê²€ì¦ í•„ìš”
    """
    import urllib.parse
    import urllib.request
    import xml.etree.ElementTree as ET
    from datetime import datetime
    
    session_print("arXiv", f"arXiv ë…¼ë¬¸ ê²€ìƒ‰ ì‹œì‘: {query}")
    
    try:
        # ê²€ìƒ‰ ì¿¼ë¦¬ URL ì¸ì½”ë”©
        base_url = "http://export.arxiv.org/api/query?"
        
        # ì‹í’ˆ/ë†ì—… ê´€ë ¨ ì¹´í…Œê³ ë¦¬ ì¶”ê°€ (q-bio, cs.AI, physics.bio-ph ë“±)
        search_query = urllib.parse.quote(query)
        
        # arXiv API íŒŒë¼ë¯¸í„°
        params = {
            'search_query': f'all:{search_query}',
            'start': 0,
            'max_results': max_results,
            'sortBy': 'lastUpdatedDate',  # ìµœì‹ ìˆœ ì •ë ¬
            'sortOrder': 'descending'
        }
        
        # URL ìƒì„±
        url = base_url + urllib.parse.urlencode(params)
        print(f"  - API URL: {url}")
        
        # API í˜¸ì¶œ
        response = urllib.request.urlopen(url, timeout=10)
        data = response.read().decode('utf-8')
        
        # XML íŒŒì‹±
        root = ET.fromstring(data)
        
        # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì •ì˜
        ns = {
            'atom': 'http://www.w3.org/2005/Atom',
            'arxiv': 'http://arxiv.org/schemas/atom'
        }
        
        # ê²°ê³¼ íŒŒì‹±
        entries = root.findall('atom:entry', ns)
        
        if not entries:
            return f"'{query}'ì— ëŒ€í•œ arXiv ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì˜ë¬¸ í‚¤ì›Œë“œë¡œ ë‹¤ì‹œ ê²€ìƒ‰í•´ë³´ì„¸ìš”."
        
        results = []
        for i, entry in enumerate(entries[:max_results], 1):
            # ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ
            title = entry.find('atom:title', ns).text.strip().replace('\n', ' ')
            
            # ì €ì ì •ë³´
            authors = entry.findall('atom:author', ns)
            author_names = [author.find('atom:name', ns).text for author in authors]
            author_str = ', '.join(author_names[:3])  # ì²˜ìŒ 3ëª…ë§Œ
            if len(author_names) > 3:
                author_str += f' ì™¸ {len(author_names)-3}ëª…'
            
            # ì´ˆë¡
            summary = entry.find('atom:summary', ns).text.strip()
            # ì´ˆë¡ì„ 300ìë¡œ ì œí•œ
            if len(summary) > 300:
                summary = summary[:297] + "..."
            
            # ë°œí–‰ì¼
            published = entry.find('atom:published', ns).text
            pub_date = datetime.strptime(published[:10], '%Y-%m-%d').strftime('%Yë…„ %mì›” %dì¼')
            
            # ì¹´í…Œê³ ë¦¬
            categories = entry.findall('atom:category', ns)
            cat_list = [cat.get('term') for cat in categories]
            categories_str = ', '.join(cat_list[:3])
            
            # ë…¼ë¬¸ ë§í¬
            pdf_link = None
            for link in entry.findall('atom:link', ns):
                if link.get('type') == 'application/pdf':
                    pdf_link = link.get('href')
                    break
            
            if not pdf_link:
                pdf_link = entry.find('atom:id', ns).text.replace('abs', 'pdf')
            
            # ê²°ê³¼ í¬ë§·íŒ…
            result_text = f"{i}. ğŸ“„ {title}\n"
            result_text += f"   ì €ì: {author_str}\n"
            result_text += f"   ë°œí–‰ì¼: {pub_date}\n"
            result_text += f"   ë¶„ì•¼: {categories_str}\n"
            result_text += f"   PDF: {pdf_link}\n"
            result_text += f"   ì´ˆë¡: {summary}\n"
            
            results.append(result_text)
        
        # ìµœì¢… ê²°ê³¼ ë°˜í™˜
        result_text = f"arXiv ë…¼ë¬¸ ê²€ìƒ‰ ê²°ê³¼ (ê²€ìƒ‰ì–´: {query}):\n"
        result_text += f"ì´ {len(results)}ê°œ ë…¼ë¬¸ ë°œê²¬\n\n"
        result_text += "\n".join(results)
        
        session_print("arXiv", f"arXiv ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ë…¼ë¬¸")
        return result_text
        
    except Exception as e:
        error_msg = f"arXiv ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        print(f"  - {error_msg}")
        return error_msg
