import json
import sys
import asyncio
from typing import Dict, List, Any, Optional, Tuple, AsyncGenerator
from datetime import datetime
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import ChatOpenAI
import re
import os
import aiofiles

from ..models.models import StreamingAgentState, SearchResult
from .worker_agents import DataGathererAgent, ProcessorAgent
from sentence_transformers import SentenceTransformer
from ...utils.session_logger import get_session_logger

# --- í˜ë¥´ì†Œë‚˜ í”„ë¡¬í”„íŠ¸ ë¡œë“œ ---
PERSONA_PROMPTS = {}
try:
    # í˜„ì¬ íŒŒì¼(orchestrator.py)ì˜ ë””ë ‰í† ë¦¬ ê²½ë¡œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    current_dir = os.path.dirname(__file__)
    # JSON íŒŒì¼ì˜ ì ˆëŒ€ ê²½ë¡œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    file_path = os.path.join(current_dir, "prompts", "persona_prompts.json")

    with open(file_path, "r", encoding="utf-8") as f:
        PERSONA_PROMPTS = json.load(f)
    print(f"OrchestratorAgent: í˜ë¥´ì†Œë‚˜ í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì„±ê³µ. (ê²½ë¡œ: {file_path})")
except FileNotFoundError:
    print(f"ê²½ê³ : ë‹¤ìŒ ê²½ë¡œì—ì„œ persona_prompts.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
except json.JSONDecodeError:
    print(f"ê²½ê³ : {file_path} íŒŒì¼ íŒŒì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. JSON í˜•ì‹ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
# -----------------------------

class TriageAgent:
    """ìš”ì²­ ë¶„ë¥˜ ë° ë¼ìš°íŒ… ë‹´ë‹¹ Agent"""

    def __init__(self, model: str = "gemini-2.5-flash-lite", temperature: float = 0.1):
        self.llm = ChatGoogleGenerativeAI(model=model, temperature=temperature)

    async def classify_request(self, query: str, state: StreamingAgentState) -> StreamingAgentState:
        """ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ flow_type ê²°ì •"""
        session_id = state.get("session_id", "unknown")
        logger = get_session_logger(session_id, "TriageAgent")
        logger.info(f"ìš”ì²­ ë¶„ë¥˜ ì‹œì‘ - '{query}'")

        classification_prompt = f"""
ì‚¬ìš©ì ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ì²˜ë¦¬ ë°©ì‹ì„ ê²°ì •í•˜ì„¸ìš”:

ì‚¬ìš©ì ìš”ì²­: {query}

ë¶„ë¥˜ ê¸°ì¤€:
1. **chat**: ê°„ë‹¨í•œ ì§ˆë¬¸, ì¼ë°˜ì ì¸ ëŒ€í™”, ë‹µë‹´, ê·¸ë¦¬ê³  ê°„ë‹¨í•œ Web Searchë‚˜, ë‚´ë¶€ ì •ë³´ë¥¼ ì¡°íšŒí•˜ì—¬ ë‹µë³€í•  ìˆ˜ ìˆëŠ” ê²½ìš°
   - ì˜ˆ: "ì•ˆë…•í•˜ì„¸ìš”", "ê°ì‚¬í•©ë‹ˆë‹¤", "ê°„ë‹¨í•œ ì„¤ëª… ìš”ì²­", "ìµœê·¼ ~ ì‹œì„¸ ì•Œë ¤ì¤˜", "ìµœê·¼ ì´ìŠˆ Top 10ì´ ë­ì•¼?"

2. **task**: ë³µí•©ì ì¸ ë¶„ì„, ë°ì´í„° ìˆ˜ì§‘, ë¦¬í¬íŠ¸ ìƒì„±ì´ í•„ìš”, ì •í™•íˆëŠ” ì—¬ëŸ¬ ì„¹ì…˜ì— ê±¸ì¹œ ë³´ê³ ì„œ ìƒì„±ì´ í•„ìš”í•œ ì§ˆë¬¸ì¼ ê²½ìš° ë˜ëŠ”, ìì„¸í•œ ì˜ì–‘ ì •ë³´ì™€ ê°™ì€ RDBë¥¼ ì¡°íšŒ í•´ì•¼í•˜ëŠ” ì§ˆë¬¸ì¼ ê²½ìš°
   - ì˜ˆ: "~ë¥¼ ë¶„ì„í•´ì¤˜", "~ì— ëŒ€í•œ ìë£Œë¥¼ ì°¾ì•„ì¤˜", "ë³´ê³ ì„œ ì‘ì„±"

JSONìœ¼ë¡œ ì‘ë‹µ:
{{
    "flow_type": "chat" ë˜ëŠ” "task",
    "reasoning": "ë¶„ë¥˜ ê·¼ê±° ì„¤ëª…",
}}
"""

        try:
            response = await self.llm.ainvoke(classification_prompt)
            response_content = response.content.strip()

            # JSON ì‘ë‹µ ì¶”ì¶œ ì‹œë„
            classification = None
            try:
                # ì§ì ‘ íŒŒì‹± ì‹œë„
                classification = json.loads(response_content)
            except json.JSONDecodeError:
                # JSON ë¸”ë¡ ì°¾ê¸° ì‹œë„
                import re
                json_match = re.search(r'\{.*\}', response_content, re.DOTALL)
                if json_match:
                    classification = json.loads(json_match.group())
                else:
                    raise ValueError("Valid JSON not found in response")

            # í•„ìˆ˜ í•„ë“œ í™•ì¸
            required_fields = ["flow_type", "reasoning"]
            for field in required_fields:
                if field not in classification:
                    raise ValueError(f"Missing required field: {field}")

            # state ì—…ë°ì´íŠ¸ (ë”•ì…”ë„ˆë¦¬ ì ‘ê·¼ ë°©ì‹)
            state["flow_type"] = classification["flow_type"]
            state["metadata"].update({
                "triage_reasoning": classification["reasoning"],
                "classified_at": datetime.now().isoformat()
            })

            logger.info(f"ë¶„ë¥˜ ê²°ê³¼: {classification['flow_type']}")
            logger.info(f"ê·¼ê±°: {classification['reasoning']}")

        except Exception as e:
            logger.error(f"ë¶„ë¥˜ ì‹¤íŒ¨, ê¸°ë³¸ê°’(task) ì ìš©: {e}")
            state["flow_type"] = "task"  # ê¸°ë³¸ê°’
            state["metadata"].update({
                "triage_error": str(e),
                "classified_at": datetime.now().isoformat()
            })

        return state


class OrchestratorAgent:
    """ê³ ì„±ëŠ¥ ë¹„ë™ê¸° ìŠ¤ì¼€ì¤„ëŸ¬ ë° ì§€ëŠ¥í˜• ê³„íš ìˆ˜ë¦½ Agent"""

    def __init__(self, model: str = "gemini-2.5-flash-lite", temperature: float = 0.2):
        self.llm = ChatGoogleGenerativeAI(model=model, temperature=temperature)
        self.llm_openai_mini = ChatOpenAI(model="gpt-4o-mini", temperature=temperature)
        self.data_gatherer = DataGathererAgent()
        self.processor = ProcessorAgent()
        self.personas = PERSONA_PROMPTS


    def get_available_personas(self) -> List[str]:
        """
        í˜„ì¬ ë¡œë“œëœ ëª¨ë“  í˜ë¥´ì†Œë‚˜ì˜ ì´ë¦„ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì„ íƒì§€ë¥¼ ë™ì ìœ¼ë¡œ êµ¬ì„±í•˜ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        """
        if not self.personas:
            return []
        return list(self.personas.keys())

    async def suggest_team_for_query(self, query: str) -> str:
        """
        LLMì´ ì¿¼ë¦¬ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ê°€ì¥ ì í•©í•œ í˜ë¥´ì†Œë‚˜ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤.
        """
        if not self.personas:
            return "ê¸°ë³¸"

        # ì‚¬ìš© ê°€ëŠ¥í•œ í˜ë¥´ì†Œë‚˜ ëª©ë¡ê³¼ ì„¤ëª… ìƒì„±
        persona_descriptions = []
        for persona_name, persona_info in self.personas.items():
            description = persona_info.get("description", "ì„¤ëª… ì—†ìŒ")
            persona_descriptions.append(f"- {persona_name}: {description}")

        personas_text = "\n".join(persona_descriptions)

        # LLMì—ê²Œ í˜ë¥´ì†Œë‚˜ ì¶”ì²œ ìš”ì²­
        prompt = f"""ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê°€ì¥ ì í•©í•œ ì „ë¬¸ê°€ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: "{query}"

ì‚¬ìš© ê°€ëŠ¥í•œ ì „ë¬¸ê°€ë“¤:
{personas_text}

ìœ„ ì „ë¬¸ê°€ë“¤ ì¤‘ì—ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ê°€ì¥ ì í•©í•œ ì „ë¬¸ê°€ í•œ ëª…ì„ ì„ íƒí•˜ì—¬, ì •í™•íˆ ê·¸ ì´ë¦„ë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ì˜ˆ: êµ¬ë§¤ ë‹´ë‹¹ì"""

        try:
            import os
            from langchain_openai import ChatOpenAI

            # OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
            openai_api_key = os.getenv("OPENAI_API_KEY")
            if not openai_api_key:
                print("ğŸ¤– OpenAI API í‚¤ê°€ ì—†ì–´ì„œ ê¸°ë³¸ í˜ë¥´ì†Œë‚˜ ì‚¬ìš©")
                return "ê¸°ë³¸"

            llm = ChatOpenAI(
                model="gpt-4o-mini",
                temperature=0.1,
                max_tokens=50,
                api_key=openai_api_key
            )

            # LangChain HumanMessage, SystemMessage ë°©ì‹ìœ¼ë¡œ í˜¸ì¶œ
            from langchain_core.messages import HumanMessage, SystemMessage

            messages = [
                SystemMessage(content="ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê°€ì¥ ì ì ˆí•œ ì „ë¬¸ê°€ë¥¼ ì¶”ì²œí•˜ëŠ” AIì…ë‹ˆë‹¤. ì •í™•íˆ ì£¼ì–´ì§„ ì „ë¬¸ê°€ ì´ë¦„ ì¤‘ í•˜ë‚˜ë§Œ ë‹µë³€í•˜ì„¸ìš”."),
                HumanMessage(content=prompt)
            ]

            response = await llm.ainvoke(messages)

            suggested_persona = response.content.strip()

            # ì œì•ˆëœ í˜ë¥´ì†Œë‚˜ê°€ ì‹¤ì œ ëª©ë¡ì— ìˆëŠ”ì§€ í™•ì¸
            if suggested_persona in self.personas:
                print(f"ğŸ¤– LLM ìë™ ë¼ìš°íŒ…: '{query[:30]}...' -> '{suggested_persona}'")
                return suggested_persona
            else:
                print(f"ğŸ¤– LLM ìë™ ë¼ìš°íŒ…: ì˜ëª»ëœ ì‘ë‹µ '{suggested_persona}' -> 'ê¸°ë³¸' ì‚¬ìš©")
                return "ê¸°ë³¸"

        except Exception as e:
            print(f"ğŸ¤– LLM ìë™ ë¼ìš°íŒ… ì˜¤ë¥˜: {e} -> 'ê¸°ë³¸' ì‚¬ìš©")
            return "ê¸°ë³¸"

    # âœ… ì¶”ê°€: ì¼ê´€ëœ ìƒíƒœ ë©”ì‹œì§€ ìƒì„±ì„ ìœ„í•œ í—¬í¼ í•¨ìˆ˜
    def _create_status_event(self, stage: str, sub_stage: str, message: str, details: Optional[Dict] = None) -> Dict:
        """í‘œì¤€í™”ëœ ìƒíƒœ ì´ë²¤íŠ¸ ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        return {
            "type": "status",
            "timestamp": datetime.now().isoformat(),
            "data": {
                "agent": "OrchestratorAgent",
                "stage": stage,
                "sub_stage": sub_stage,
                "message": message,
                "details": details or {}
            }
        }

    async def _invoke_with_fallback(self, prompt: str, primary_model, fallback_model):
        """Gemini API rate limit ì‹œ OpenAIë¡œ fallback ì²˜ë¦¬"""
        try:
            result = await primary_model.ainvoke(prompt)
            return result
        except Exception as e:
            error_str = str(e).lower()
            rate_limit_indicators = ['429', 'quota', 'rate limit', 'exceeded', 'resource_exhausted']

            if any(indicator in error_str for indicator in rate_limit_indicators):
                print(f"OrchestratorAgent: Gemini API rate limit ê°ì§€, fallback ì‹œë„: {e}")
                if fallback_model:
                    try:
                        result = await fallback_model.ainvoke(prompt)
                        print("OrchestratorAgent: fallback ì„±ê³µ")
                        return result
                    except Exception as fallback_error:
                        print(f"OrchestratorAgent: fallbackë„ ì‹¤íŒ¨: {fallback_error}")
                        raise fallback_error
                else:
                    print("OrchestratorAgent: fallback ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
                    raise e
            else:
                raise e

    def _inject_context_into_query(self, query: str, context: Dict[int, str]) -> str:
        """'[step-Xì˜ ê²°ê³¼]' í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ì‹¤ì œ ì»¨í…ìŠ¤íŠ¸ë¡œ êµì²´í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
        match = re.search(r"\[step-(\d+)ì˜ ê²°ê³¼\]", query)
        if match:
            step_index = int(match.group(1))
            if step_index in context:
                print(f"  - {step_index}ë‹¨ê³„ ì»¨í…ìŠ¤íŠ¸ ì£¼ì…: '{context[step_index][:]}...'")
                # í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ì´ì „ ë‹¨ê³„ì˜ ìš”ì•½ ê²°ê³¼ë¡œ ì¹˜í™˜
                return query.replace(match.group(0), f"ì´ì „ ë‹¨ê³„ ë¶„ì„ ê²°ê³¼: '{context[step_index]}'")
        return query

    async def generate_plan(self, state: StreamingAgentState) -> StreamingAgentState:
        """í˜ë¥´ì†Œë‚˜ ê´€ì ê³¼ ì˜ì¡´ì„±ì„ ë°˜ì˜í•˜ì—¬ ë‹¨ê³„ë³„ ì‹¤í–‰ ê³„íš(Hybrid Model)ì„ ìˆ˜ë¦½í•©ë‹ˆë‹¤."""
        print(f"\n>> Orchestrator: ì§€ëŠ¥í˜• ë‹¨ê³„ë³„ ê³„íš ìˆ˜ë¦½ ì‹œì‘")
        query = state["original_query"]
        current_date_str = datetime.now().strftime("%Yë…„ %mì›” %dì¼")

        # í˜ë¥´ì†Œë‚˜ ì •ë³´ ì¶”ì¶œ
        persona_name = state.get("persona", "ê¸°ë³¸")
        persona_info = self.personas.get(persona_name, {})
        persona_description = persona_info.get("description", "ì¼ë°˜ì ì¸ ë¶„ì„ê°€")
        print(f"  - ê³„íš ìˆ˜ë¦½ì— '{persona_name}' í˜ë¥´ì†Œë‚˜ ê´€ì  ì ìš©")

        planning_prompt = f"""
ë‹¹ì‹ ì€ **'{persona_name}'ì˜ ìœ ëŠ¥í•œ AI ìˆ˜ì„ ë³´ì¢Œê´€**ì´ì **ì‹¤í–‰ ê³„íš ì„¤ê³„ ì „ë¬¸ê°€**ì…ë‹ˆë‹¤.
ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” **'{persona_name}'ì˜ ê´€ì ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ì •ë³´ë¥¼ ì°¾ì•„ë‚´ê¸° ìœ„í•œ, ë…¼ë¦¬ì ì´ê³  íš¨ìœ¨ì ì¸ ì‹¤í–‰ ê³„íš**ì„ ìˆ˜ë¦½í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

**í•µì‹¬ ì„ë¬´ (ë°˜ë“œì‹œ ì¤€ìˆ˜í•  ê²ƒ)**:
1.  **í˜ë¥´ì†Œë‚˜ ê´€ì  ë°˜ì˜ (What to ask)**: ìƒì„±í•˜ëŠ” ëª¨ë“  í•˜ìœ„ ì§ˆë¬¸ì€ ì² ì €íˆ '{persona_name}'ì˜ ê´€ì‹¬ì‚¬ì— ë¶€í•©í•´ì•¼ í•©ë‹ˆë‹¤. ì´ë“¤ì˜ í•µì‹¬ ë‹ˆì¦ˆ(ì˜ˆ: êµ¬ë§¤-ì›ê°€/ì‹œì„¸, ë§ˆì¼€íŒ…-íŠ¸ë Œë“œ/ì†Œë¹„ì, R&D-ì‹ ì›ë£Œ/ì„±ë¶„)ë¥¼ ë§Œì¡±ì‹œí‚¤ëŠ” ë° ì§‘ì¤‘í•˜ì„¸ìš”.
2.  **ë…¼ë¦¬ì  ê³„íš ìˆ˜ë¦½ (How to ask)**: í˜ë¥´ì†Œë‚˜ì˜ ê´€ì ì—ì„œ ë„ì¶œëœ ì§ˆë¬¸ë“¤ì˜ ì„ í›„ ê´€ê³„ë¥¼ ë¶„ì„í•˜ì—¬, ì˜ì¡´ì„±ì´ ìˆëŠ” ì‘ì—…ì€ ìˆœì°¨ì ìœ¼ë¡œ, ì—†ëŠ” ì‘ì—…ì€ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ëŠ” ìµœì ì˜ ì‹¤í–‰ ë‹¨ê³„ë¥¼ ì„¤ê³„í•˜ì„¸ìš”.

**ì‚¬ìš©ì ì›ë³¸ ìš”ì²­**: "{query}"
**í˜„ì¬ ë‚ ì§œ**: {current_date_str}

---
**## ë³´ìœ  ë„êµ¬ ëª…ì„¸ì„œ ë° ì„ íƒ ê°€ì´ë“œ**

**1. rdb_search (PostgreSQL) - 1ìˆœìœ„ í™œìš©**
   - **ë°ì´í„° ì¢…ë¥˜**: ì •í˜• ë°ì´í„° (í…Œì´ë¸” ê¸°ë°˜: ì‹ìì¬ **ì˜ì–‘ì„±ë¶„**, ë†Â·ì¶•Â·ìˆ˜ì‚°ë¬¼ **ì‹œì„¸/ê°€ê²©/ê±°ë˜ëŸ‰** ë“± ìˆ˜ì¹˜ ë°ì´í„°).
   - **ì‚¬ìš© ì‹œì **: ì˜ì–‘ì„±ë¶„, í˜„ì¬ê°€ê²©, ì‹œì„¸ë³€ë™, ê°€ê²©ë¹„êµ, ìˆœìœ„/í‰ê· /í•©ê³„ ë“± **ì •í™•í•œ ìˆ˜ì¹˜ ì—°ì‚°**ì´ í•„ìš”í•  ë•Œ.
   - **íŠ¹ì§•**: ë‚ ì§œÂ·ì§€ì—­Â·í’ˆëª© ì»¬ëŸ¼ìœ¼ë¡œ **í•„í„°/ì§‘ê³„** ìµœì í™”. ë‹¤ì¤‘ ì¡°ê±´(where)ê³¼ group by, order byë¥¼ í†µí•œ **í†µê³„/ë­í‚¹** ì§ˆì˜ì— ì í•©. (ê´€ê³„ ê·¸ë˜í”„ íƒìƒ‰ì€ ë¹„ê¶Œì¥)
   - **ì˜ˆì‹œ ì§ˆì˜ ì˜ë„**: "ì‚¬ê³¼ ë¹„íƒ€ë¯¼C í•¨ëŸ‰", "ì§€ë‚œë‹¬ ì œì£¼ ê°ê·¤ í‰ê· ê°€", "ì „ë³µ ê°€ê²© ì¶”ì´", "ì˜ì–‘ì„±ë¶„ ìƒìœ„ TOP 10"

**2. vector_db_search (Elasticsearch) - 1ìˆœìœ„ í™œìš©**
   - **ë°ì´í„° ì¢…ë¥˜**: ë¹„ì •í˜• ë°ì´í„° (ë‰´ìŠ¤ê¸°ì‚¬, ë…¼ë¬¸, ë³´ê³ ì„œ ì „ë¬¸).
   - **ì‚¬ìš© ì‹œì **: ì‹œì¥ë¶„ì„, ì •ì±…ë¬¸ì„œ, íŠ¸ë Œë“œë¶„ì„, ë°°ê²½ì •ë³´, ì‹¤ë¬´ê°€ì´ë“œ ë“± ì„œìˆ í˜• ì •ë³´ë‚˜ ë¶„ì„ì´ í•„ìš”í•  ë•Œ.
   - **íŠ¹ì§•**: ì˜ë¯¸ê¸°ë°˜ ê²€ìƒ‰ìœ¼ë¡œ ì§ˆë¬¸ì˜ ë§¥ë½ê³¼ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œë¥¼ ì°¾ì•„ì¤Œ.

**3. graph_db_search (Neo4j) - 1ìˆœìœ„ í™œìš©**
   - **ë°ì´í„° ì¢…ë¥˜**: **ê´€ê³„í˜•(ê·¸ë˜í”„) ë°ì´í„°**. ë…¸ë“œ: í’ˆëª©(ë†ì‚°ë¬¼/ìˆ˜ì‚°ë¬¼/ì¶•ì‚°ë¬¼), **Origin(ì›ì‚°ì§€: city/region)**, **Nutrient(ì˜ì–‘ì†Œ)**.
     ê´€ê³„: `(í’ˆëª©)-[:isFrom]->(Origin)`, `(í’ˆëª©)-[:hasNutrient]->(Nutrient)`. ìˆ˜ì‚°ë¬¼ì€ í’ˆëª© ë…¸ë“œì— `fishState`(í™œì–´/ì„ ì–´/ëƒ‰ë™/ê±´ì–´) ì†ì„± ì¡´ì¬.
   - **ì‚¬ìš© ì‹œì **: **í’ˆëª© â†” ì›ì‚°ì§€**, **í’ˆëª© â†” ì˜ì–‘ì†Œ**ì²˜ëŸ¼ **ì—”í‹°í‹° ê°„ ì—°ê²°**ì´ í•µì‹¬ì¼ ë•Œ. ì§€ì—­Â·ìƒíƒœ(fishState) ì¡°ê±´ì„ ì–¹ì€ **ì›ì‚°ì§€/íŠ¹ì‚°í’ˆ íƒìƒ‰**.
   - **íŠ¹ì§•**: ì§€ì‹ê·¸ë˜í”„ ê²½ë¡œ íƒìƒ‰ì— ìµœì í™”. í‚¤ì›Œë“œëŠ” **í’ˆëª©ëª…/ì§€ì—­ëª…/ì˜ì–‘ì†Œ/ìˆ˜ì‚°ë¬¼ ìƒíƒœ(fishState)**ë¡œ ê°„ê²°íˆ í‘œí˜„í•˜ê³ , ì§ˆë¬¸ì€ **"Aì˜ ì›ì‚°ì§€", "Aì˜ ì˜ì–‘ì†Œ", "ì§€ì—­ Bì˜ íŠ¹ì‚°í’ˆ/ì›ì‚°ì§€", "í™œì–´ Aì˜ ì›ì‚°ì§€"**ì²˜ëŸ¼ **ê´€ê³„ë¥¼ ëª…ì‹œ**í• ìˆ˜ë¡ ì •í™•ë„ ìƒìŠ¹.
   - **ì˜ˆì‹œ ì§ˆì˜ ì˜ë„**: "ì‚¬ê³¼ì˜ ì›ì‚°ì§€", "ì˜¤ë Œì§€ì˜ ì˜ì–‘ì†Œ", "ì œì£¼ë„ì˜ ê°ê·¤ ì›ì‚°ì§€", "í™œì–´ ë¬¸ì–´ ì›ì‚°ì§€", "ê²½ìƒë¶ë„ ì‚¬ê³¼ ì‚°ì§€ ì—°ê²°"

**4. arxiv_search - 1ìˆœìœ„ í™œìš© (í•™ìˆ  ì—°êµ¬ í•„ìš”ì‹œ)**
   - **ë°ì´í„° ì¢…ë¥˜**: ìµœì‹  í•™ìˆ  ë…¼ë¬¸ (arXiv í”„ë¦¬í”„ë¦°íŠ¸ ë…¼ë¬¸).
   - **ì‚¬ìš© ì‹œì **: ì‹ ì œí’ˆ ê°œë°œ, ê³¼í•™ì  ê·¼ê±°, ìµœì‹  ì—°êµ¬ ë™í–¥, AI/ML ê¸°ìˆ , ëŒ€ì²´ì‹í’ˆ ì—°êµ¬, ì§€ì†ê°€ëŠ¥ì„± ì—°êµ¬ê°€ í•„ìš”í•  ë•Œ.
   - **íŠ¹ì§•**: ì˜ë¬¸ ê²€ìƒ‰ í•„ìˆ˜. ì‹í’ˆê³¼í•™(food science), ìƒëª…ê³µí•™(biotechnology), AI/ML, ëŒ€ì²´ë‹¨ë°±ì§ˆ(alternative protein) ë“± í•™ìˆ  ìš©ì–´ë¡œ ê²€ìƒ‰.
   - **ì˜ˆì‹œ ì§ˆì˜ ì˜ë„**: "ëŒ€ì²´ìœ¡ ê°œë°œ ì—°êµ¬", "ë°œíš¨ ê¸°ìˆ  ë…¼ë¬¸", "AI ê¸°ë°˜ í’ˆì§ˆ ì˜ˆì¸¡", "ì§€ì†ê°€ëŠ¥í•œ í¬ì¥ì¬ ì—°êµ¬"

**5. web_search - ìµœì‹  ì •ë³´ í•„ìˆ˜ ì‹œ ìš°ì„  ì‚¬ìš©**
   - **ë°ì´í„° ì¢…ë¥˜**: ì‹¤ì‹œê°„ ìµœì‹  ì •ë³´, ì‹œì‚¬ ì •ë³´, ì¬í•´/ì¬ë‚œ ì •ë³´, ìµœê·¼ ë‰´ìŠ¤.
   - **ì‚¬ìš© ì¡°ê±´**: 
     * **í•„ìˆ˜ ì‚¬ìš©**: 2025ë…„ íŠ¹ì • ì›”/ì¼, ìµœê·¼, í˜„ì¬, ê¸°ìƒì´ë³€, ì§‘ì¤‘í­ìš°, ì¬í•´, ì¬ë‚œ ë“±ì´ í¬í•¨ëœ ì§ˆë¬¸
     * **ìš°ì„  ì‚¬ìš©**: ë‚´ë¶€ DBì— ì—†ì„ ê°€ëŠ¥ì„±ì´ ë†’ì€ ìµœì‹  ì‚¬ê±´/ìƒí™© ì •ë³´
     * **ë³´ì¡° ì‚¬ìš©**: ë‚´ë¶€ DBë¡œ í•´ê²°ë˜ì§€ ì•ŠëŠ” ì¼ë°˜ ì§€ì‹
   - **ì‚¬ìš© ê¸ˆì§€**: ì¼ë°˜ì ì¸ ë†ì¶•ìˆ˜ì‚°ë¬¼ ì‹œì„¸, ì˜ì–‘ì •ë³´, ì›ì‚°ì§€ ë“±ì€ ë‚´ë¶€ DB ìš°ì„  ì‚¬ìš© í›„ ë³´ì™„ì ìœ¼ë¡œë§Œ ì‚¬ìš©.

**ë„êµ¬ ì„ íƒ ìš°ì„ ìˆœìœ„:**
1. **â­ ìµœì‹  ì •ë³´/ì‹œì‚¬ (2025ë…„ íŠ¹ì • ì‹œì , ì¬í•´, ê¸°ìƒì´ë³€, ë‰´ìŠ¤)** â†’ `web_search` **[ìµœìš°ì„ ]**
2. **ìˆ˜ì¹˜/í†µê³„ ë°ì´í„° (ì‹ìì¬ ì˜ì–‘ì„±ë¶„, ë†ì¶•ìˆ˜ì‚°ë¬¼ ì‹œì„¸)** â†’ `rdb_search`
3. **ê´€ê³„/ë¶„ë¥˜ ì •ë³´ (í’ˆëª©-ì›ì‚°ì§€, í’ˆëª©-ì˜ì–‘ì†Œ, ì§€ì—­-íŠ¹ì‚°í’ˆ, ìˆ˜ì‚°ë¬¼ ìƒíƒœë³„ ì›ì‚°ì§€)** â†’ `graph_db_search`
4. **ë¶„ì„/ì—°êµ¬ ë¬¸ì„œ (ì‹œì¥ë¶„ì„, ì†Œë¹„ì ì¡°ì‚¬)** â†’ `vector_db_search`
5. **í•™ìˆ  ë…¼ë¬¸/ì—°êµ¬ (ì‹ ì œí’ˆ ê°œë°œ, ê³¼í•™ì  ê·¼ê±°, AI/ML ì‘ìš©)** â†’ `arxiv_search`

**ê° ë„êµ¬ë³„ ì ìš© ì˜ˆì‹œ:**
- `rdb_search`: "ì‹ìì¬ ì˜ì–‘ì„±ë¶„", "ë†ì¶•ìˆ˜ì‚°ë¬¼ ì‹œì„¸", "ê°€ê²© ì¶”ì´/ë¹„êµ", "ì˜ì–‘ì„±ë¶„ ìƒìœ„ TOP"
- `graph_db_search`: "ì‚¬ê³¼ì˜ ì›ì‚°ì§€", "ì˜¤ë Œì§€ì˜ ì˜ì–‘ì†Œ", "ì œì£¼-ê°ê·¤ ê´€ê³„", "í™œì–´ ë¬¸ì–´ ì›ì‚°ì§€", "ì§€ì—­ë³„ íŠ¹ì‚°í’ˆ ì—°ê²°"
- `vector_db_search`: "ì‹œì¥ ë¶„ì„ ë³´ê³ ì„œ", "ì†Œë¹„ì í–‰ë™ ì—°êµ¬", "ì •ì±… ë¬¸ì„œ"
- `arxiv_search`: "ì‹ë¬¼ì„± ëŒ€ì²´ìœ¡ ê°œë°œ ë…¼ë¬¸", "ë°œíš¨ ê¸°ìˆ  ìµœì‹  ì—°êµ¬", "AI í’ˆì§ˆ ì˜ˆì¸¡ ëª¨ë¸", "ë°”ì´ì˜¤í”Œë¼ìŠ¤í‹± í¬ì¥ì¬"
- `web_search`: "2025ë…„ ìµœì‹  íŠ¸ë Œë“œ", "ì‹¤ì‹œê°„ ì—…ê³„ ë™í–¥", "2025ë…„ 7ì›” ì§‘ì¤‘í­ìš° í”¼í•´ì§€ì—­", "ê¸°ìƒì´ë³€ ë†ì—… í”¼í•´", "ìµœê·¼ ì¬í•´ ë°œìƒ ì§€ì—­", "í˜„ì¬ ë†ì‚°ë¬¼ ê³µê¸‰ ìƒí™©"

---
**## ê³„íš ìˆ˜ë¦½ì„ ìœ„í•œ ë‹¨ê³„ë³„ ì‚¬ê³  í”„ë¡œì„¸ìŠ¤ (ë°˜ë“œì‹œ ì¤€ìˆ˜í•  ê²ƒ)**

**1ë‹¨ê³„: ëª©í‘œ ì¬í•´ì„**
- ì‚¬ìš©ìì˜ ì›ë³¸ ìš”ì²­ì„ ë¶„ì„í•˜ì—¬, ìµœì¢… ëª©í‘œê°€ ë¬´ì—‡ì¸ì§€ **'{persona_name}'ì˜ ì…ì¥ì—ì„œ** ëª…í™•í•˜ê²Œ ì¬ì •ì˜í•©ë‹ˆë‹¤.
- (ì˜ˆ: ì›ë³¸ ìš”ì²­ì´ "ë§Œë‘ ì‹œì¥ ì¡°ì‚¬"ì¼ ë•Œ, í˜ë¥´ì†Œë‚˜ê°€ 'ë§ˆì¼€íŒ… ë‹´ë‹¹ì'ë¼ë©´ ìµœì¢… ëª©í‘œëŠ” 'ì‹ ì œí’ˆ ë§Œë‘ì˜ ì„±ê³µì ì¸ ì‹œì¥ ì§„ì…ì„ ìœ„í•œ ë§ˆì¼€íŒ… ì „ëµ ë³´ê³ ì„œ'ë¡œ êµ¬ì²´í™”í•©ë‹ˆë‹¤.)

**2ë‹¨ê³„: í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ ì •ë³´ ì‹ë³„ ë° ì§ˆë¬¸ êµ¬ì²´í™”**
- 1ë‹¨ê³„ì—ì„œ ì¬ì •ì˜í•œ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´, **'{persona_name}'ê°€ ê°€ì¥ ì¤‘ìš”í•˜ê²Œ ìƒê°í•  í•µì‹¬ í‚¤ì›Œë“œ(ì˜ˆ: êµ¬ë§¤ ë‹´ë‹¹ì-ì›ê°€/ì‹œì„¸, ë§ˆì¼€í„°-íŠ¸ë Œë“œ/ì†Œë¹„ì, R&D-ì‹ ì›ë£Œ/ì„±ë¶„)ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ** í•„ìš”í•œ ì •ë³´ ì¡°ê°ë“¤ì„ ì‹ë³„í•©ë‹ˆë‹¤.
- ì‹ë³„ëœ ì •ë³´ ì¡°ê°ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ, ê°ê° ì™„ê²°ëœ í˜•íƒœì˜ êµ¬ì²´ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ë¶„í•´í•©ë‹ˆë‹¤.
- ìƒì„±ëœ ëª¨ë“  í•˜ìœ„ ì§ˆë¬¸ì€ ì›ë³¸ ìš”ì²­ì˜ í•µì‹¬ ë§¥ë½(ì˜ˆ: 'ëŒ€í•œë¯¼êµ­', 'ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ')ì„ ë°˜ë“œì‹œ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.

**3ë‹¨ê³„: ì§ˆë¬¸ ê°„ ì˜ì¡´ì„± ë¶„ì„ (ê°€ì¥ ì¤‘ìš”í•œ ë‹¨ê³„)**
- ë¶„í•´ëœ ì§ˆë¬¸ë“¤ ê°„ì˜ ì„ í›„ ê´€ê³„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
- **"ì–´ë–¤ ì§ˆë¬¸ì´ ë‹¤ë¥¸ ì§ˆë¬¸ì˜ ê²°ê³¼ë¥¼ ë°˜ë“œì‹œ ì•Œì•„ì•¼ë§Œ ì œëŒ€ë¡œ ìˆ˜í–‰ë  ìˆ˜ ìˆëŠ”ê°€?"**ë¥¼ íŒë‹¨í•©ë‹ˆë‹¤.
- ì˜ˆì‹œ: `Aë¶„ì•¼ì˜ ì‹œì¥ ê·œëª¨`ë¥¼ ì•Œì•„ì•¼ `Aë¶„ì•¼ì˜ ì£¼ìš” ê²½ìŸì‚¬`ë¥¼ ì¡°ì‚¬í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ë‘ ì§ˆë¬¸ì€ ì˜ì¡´ì„±ì´ ìˆìŠµë‹ˆë‹¤. ë°˜ë©´, `Aë¶„ì•¼ì˜ ì‹œì¥ ê·œëª¨`ì™€ `Bë¶„ì•¼ì˜ ì‹œì¥ ê·œëª¨` ì¡°ì‚¬ëŠ” ì„œë¡œ ë…ë¦½ì ì…ë‹ˆë‹¤.

**4ë‹¨ê³„: ì‹¤í–‰ ë‹¨ê³„ ê·¸ë£¹í™” (Grouping)**
- **Step 1**: ì„œë¡œ ì˜ì¡´ì„±ì´ ì—†ëŠ”, ê°€ì¥ ë¨¼ì € ìˆ˜í–‰ë˜ì–´ì•¼ í•  ë³‘ë ¬ ì‹¤í–‰ ê°€ëŠ¥í•œ ì§ˆë¬¸ë“¤ì„ ë°°ì¹˜í•©ë‹ˆë‹¤. (ì˜ˆ: ì‹œì¥ ê·œëª¨ ì¡°ì‚¬, ìµœì‹  íŠ¸ë Œë“œ ì¡°ì‚¬)
- **Step 2 ì´í›„**: ì´ì „ ë‹¨ê³„ì˜ ê²°ê³¼(`[step-Xì˜ ê²°ê³¼]` í”Œë ˆì´ìŠ¤í™€ë” ì‚¬ìš©)ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ì˜ì¡´ì„± ìˆëŠ” ì§ˆë¬¸ë“¤ì„ ë°°ì¹˜í•©ë‹ˆë‹¤. (ì˜ˆ: 1ë‹¨ê³„ì—ì„œ ì°¾ì€ 'ì„±ì¥ ë¶„ì•¼'ì˜ ê²½ìŸì‚¬ ì¡°ì‚¬)

**5ë‹¨ê³„: ê° ì§ˆë¬¸ì— ëŒ€í•œ ìµœì  ë„êµ¬ ì„ íƒ ì „ëµ**
- 'ë³´ìœ  ë„êµ¬ ëª…ì„¸ì„œ'ë¥¼ ì°¸ê³ í•˜ì—¬ ê° í•˜ìœ„ ì§ˆë¬¸ì— ê°€ì¥ ì í•©í•œ ë„êµ¬ë¥¼ **ë‹¨ í•˜ë‚˜ë§Œ** ì‹ ì¤‘í•˜ê²Œ ì„ íƒí•©ë‹ˆë‹¤.
- **ì¤‘ìš”**: ì§ˆë¬¸ì˜ ë³µì¡ì„±ì„ ë¶„ì„í•˜ì—¬ **í•„ìš”í•œ ë„êµ¬ë§Œ ì„ íƒ**í•©ë‹ˆë‹¤:
  * **ë‹¨ìˆœ ì§ˆë¬¸** â†’ 1ê°œ ë„êµ¬ë¡œ ì¶©ë¶„ (ì˜ˆ: "ì‚¬ê³¼ ì˜ì–‘ì„±ë¶„" â†’ `rdb_search`)
  * **ë³µí•© ì§ˆë¬¸** â†’ ì—¬ëŸ¬ ë„êµ¬ ì¡°í•© í•„ìš” (ì˜ˆ: "ìµœì‹  ì¬í•´ + ë†ì—… ë¶„ì„" â†’ ì—¬ëŸ¬ ë‹¨ê³„)
    - **"ì„±ë¶„", "ì˜ì–‘", "ì‹œì„¸", "ê°€ê²©"** í¬í•¨ â†’ `rdb_search`
    - **"ì›ì‚°ì§€", "ê´€ê³„", "ì œì¡°ì‚¬", "íŠ¹ì‚°í’ˆ", "fishState(í™œì–´/ì„ ì–´/ëƒ‰ë™/ê±´ì–´)"** í¬í•¨ â†’ `graph_db_search`
    - **"ë¶„ì„", "ì—°êµ¬", "ì¡°ì‚¬", "ë³´ê³ ì„œ", "ë™í–¥"** í¬í•¨ â†’ `vector_db_search`
    - **"ì‹ ì œí’ˆ ê°œë°œ", "ê³¼í•™ì  ê·¼ê±°", "ë…¼ë¬¸", "í•™ìˆ ", "AI/ML", "ëŒ€ì²´ì‹í’ˆ", "ì§€ì†ê°€ëŠ¥ì„±", "ë°œíš¨ ê¸°ìˆ "** í¬í•¨ â†’ `arxiv_search`
    - **"ìµœì‹  íŠ¸ë Œë“œ", "ì‹¤ì‹œê°„ ì •ë³´", "2025ë…„", "ìµœê·¼", "í˜„ì¬", "ê¸°ìƒì´ë³€", "ì§‘ì¤‘í­ìš°", "í™ìˆ˜", "íƒœí’", "ì¬í•´", "ì¬ë‚œ", "í”¼í•´", "ë‰´ìŠ¤", "ì‚¬ê±´", "ë°œìƒ"** ë“± ìµœì‹ ì„± ê°•ì¡° ë˜ëŠ” ì‹œì‚¬ ì •ë³´ ê´€ë ¨ ì‹œ â†’ `web_search`

**ë„êµ¬ ì„ íƒ ì˜ˆì‹œ**:
- **ë‹¨ìˆœ ì¼€ì´ìŠ¤**: "ì‚¬ê³¼ì˜ ì˜ì–‘ì„±ë¶„" â†’ `rdb_search` 1ê°œë§Œ
- **ì¤‘ê°„ ì¼€ì´ìŠ¤**: "2025ë…„ ì‹í’ˆ íŠ¸ë Œë“œ" â†’ `web_search` 1ê°œë§Œ  
- **ë³µí•© ì¼€ì´ìŠ¤**: "ìµœì‹  ì¬í•´ í”¼í•´ì§€ì—­ ë†ì—… í˜„í™©" â†’ `web_search` + `vector_db_search` + `graph_db_search` ì¡°í•©
- **ê³ ë„ ë³µí•©**: "ì‹ ì œí’ˆ ê°œë°œ ì „ëµ" â†’ `web_search` + `arxiv_search` + `vector_db_search` + `rdb_search` ì¡°í•©

**6ë‹¨ê³„: ìµœì¢… JSON í˜•ì‹í™”**
- ìœ„ì—ì„œ ê²°ì •ëœ ëª¨ë“  ë‚´ìš©ì„ ì•„ë˜ 'ìµœì¢… ì¶œë ¥ í¬ë§·'ì— ë§ì¶° JSONìœ¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.
- **ì¤‘ìš”**: `sub_questions` í‚¤ëŠ” ë°˜ë“œì‹œ `execution_steps` ë°°ì—´ì˜ ê° ìš”ì†Œ ì•ˆì—ë§Œ ì¡´ì¬í•´ì•¼ í•©ë‹ˆë‹¤.

---
**## ê³„íš ìˆ˜ë¦½ ì˜ˆì‹œ**

**ìš”ì²­ (ë‹¨ìˆœ)**: "ì‚¬ê³¼ì˜ ì˜ì–‘ì„±ë¶„ê³¼ ì¹¼ë¡œë¦¬ ì •ë³´ë¥¼ ì•Œë ¤ì¤˜."

**ìƒì„±ëœ ê³„íš(JSON)**:
{{
    "title": "ì‚¬ê³¼ ì˜ì–‘ì„±ë¶„ ë° ì¹¼ë¡œë¦¬ ì •ë³´",
    "reasoning": "ë‹¨ìˆœí•œ ì˜ì–‘ì„±ë¶„ ì¡°íšŒ ì§ˆë¬¸ì´ë¯€ë¡œ RDB ê²€ìƒ‰ 1ê°œ ë„êµ¬ë§Œìœ¼ë¡œ ì¶©ë¶„í•©ë‹ˆë‹¤.",
    "execution_steps": [
        {{
            "step": 1,
            "reasoning": "ì˜ì–‘ì„±ë¶„ê³¼ ì¹¼ë¡œë¦¬ëŠ” RDBì— ì •í˜•í™”ë˜ì–´ ì €ì¥ëœ ë°ì´í„°ì´ë¯€ë¡œ rdb_searchë§Œìœ¼ë¡œ í•´ê²° ê°€ëŠ¥",
            "sub_questions": [
                {{"question": "ì‚¬ê³¼ì˜ ìƒì„¸ ì˜ì–‘ì„±ë¶„ ì •ë³´ ë° ì¹¼ë¡œë¦¬", "tool": "rdb_search"}}
            ]
        }}
    ]
}}

**ìš”ì²­ (ë³µí•©)**: "ë§Œë‘ ì‹ ì œí’ˆ ê°œë°œì„ ìœ„í•´, í•´ì™¸ ìˆ˜ì¶œ ì‚¬ë¡€ì™€ ìµœì‹  ì‹í’ˆ íŠ¸ë Œë“œì— ë§ëŠ” ì›ë£Œë¥¼ ì¶”ì²œí•´ì¤˜."

**ìƒì„±ëœ ê³„íš(JSON)**:
{{
    "title": "ì‹ ì œí’ˆ ë§Œë‘ ê°œë°œì„ ìœ„í•œ ì‹œì¥ ì¡°ì‚¬ ë° ì›ë£Œ ì¶”ì²œ",
    "reasoning": "ì‹œì¥ ì¡°ì‚¬ì™€ íŠ¸ë Œë“œ ë¶„ì„ì„ 1ë‹¨ê³„ì—ì„œ ë³‘ë ¬ë¡œ ìˆ˜í–‰í•œ ë’¤, ê·¸ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ 2ë‹¨ê³„ì—ì„œ êµ¬ì²´ì ì¸ ì›ë£Œë¥¼ ì¶”ì²œí•˜ëŠ” 2ë‹¨ê³„ ê³„íšì„ ìˆ˜ë¦½í•©ë‹ˆë‹¤.",
    "execution_steps": [
        {{
            "step": 1,
            "reasoning": "ê¸°ë°˜ ì •ë³´ì¸ 'ìˆ˜ì¶œ ì‚¬ë¡€'ì™€ 'ìµœì‹  íŠ¸ë Œë“œ'ë¥¼ ë³‘ë ¬ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.",
            "sub_questions": [
                {{"question": "ëŒ€í•œë¯¼êµ­ ëƒ‰ë™ë§Œë‘ í•´ì™¸ ìˆ˜ì¶œ ì„±ê³µ ì‚¬ë¡€ ë° ì¸ê¸° ì œí’ˆ íŠ¹ì§• ë¶„ì„", "tool": "vector_db_search"}},
                {{"question": "2025ë…„ ìµœì‹  ê¸€ë¡œë²Œ ì‹í’ˆ íŠ¸ë Œë“œ ë° ì†Œë¹„ì ì„ í˜¸ ì›ë£Œ", "tool": "web_search"}}
            ]
        }},
        {{
            "step": 2,
            "reasoning": "1ë‹¨ê³„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹ ì œí’ˆì— ì ìš©í•  êµ¬ì²´ì ì¸ ì›ë£Œë¥¼ íƒìƒ‰í•©ë‹ˆë‹¤.",
            "sub_questions": [
                {{"question": "[step-1ì˜ ê²°ê³¼]ë¥¼ ë°”íƒ•ìœ¼ë¡œ, 'ê±´ê°• ë° ì›°ë¹™' íŠ¸ë Œë“œì— ë§ëŠ” ì‹ë¬¼ì„± ë§Œë‘ ì›ë£Œ ì¶”ì²œ", "tool": "vector_db_search"}},
                {{"question": "ì¶”ì²œëœ ì‹ ê·œ ì›ë£Œ(ì˜ˆ: ëŒ€ì²´ìœ¡)ì˜ ì˜ì–‘ì„±ë¶„ ì •ë³´", "tool": "rdb_search"}}
            ]
        }}
    ]
}}

**ìš”ì²­**: "2025ë…„ 7ì›”ê³¼ 8ì›”ì˜ ì§€êµ¬ì˜¨ë‚œí™” ê¸°ìƒì´ë³€ì— ë”°ë¥¸ ì§‘ì¤‘í­ìš° í”¼í•´ì§€ì—­ì—ì„œ ìƒì‚°ë˜ëŠ” ì£¼ìš” ì‹ì¬ë£Œë“¤ ëª©ë¡ê³¼ ìƒì‚°ì§€ë¥¼ í‘œë¡œ ì •ë¦¬í•´ì¤˜"

**ìƒì„±ëœ ê³„íš(JSON)**:
{{
    "title": "2025ë…„ ì—¬ë¦„ ê¸°ìƒì´ë³€ í”¼í•´ì§€ì—­ ì‹ì¬ë£Œ ìƒì‚° í˜„í™© ì¢…í•© ë¶„ì„",
    "reasoning": "ìµœì‹  ì¬í•´ ì •ë³´(web_search), ë†ì—… í”¼í•´ ë¶„ì„(vector_db), ì§€ì—­-ì‹ì¬ë£Œ ë§¤í•‘(graph_db), ê°€ê²©/ìƒì‚°ëŸ‰ í†µê³„(rdb_search)ë¥¼ ì¡°í•©í•œ 3ë‹¨ê³„ ì¢…í•© ë¶„ì„",
    "execution_steps": [
        {{
            "step": 1,
            "reasoning": "ìµœì‹  ì¬í•´ ì •ë³´ì™€ ê¸°ì¡´ ë†ì—… í”¼í•´ ë¶„ì„ì„ ë³‘ë ¬ë¡œ ìˆ˜ì§‘",
            "sub_questions": [
                {{"question": "2025ë…„ 7ì›” 8ì›” ëŒ€í•œë¯¼êµ­ ì§‘ì¤‘í˜¸ìš° í”¼í•´ ì‹¬ê° ì§€ì—­ ëª©ë¡", "tool": "web_search"}},
                {{"question": "ì§‘ì¤‘í˜¸ìš° ë†ì—… í”¼í•´ ë¶„ì„ ë³´ê³ ì„œ ë° ìƒì‚°ëŸ‰ ê°ì†Œ í†µê³„", "tool": "vector_db_search"}}
            ]
        }},
        {{
            "step": 2,
            "reasoning": "1ë‹¨ê³„ì—ì„œ í™•ì¸ëœ í”¼í•´ ì§€ì—­ì˜ ì£¼ìš” ìƒì‚° ì‹ì¬ë£Œ ë§¤í•‘ ë° ì‹œì„¸ ì •ë³´ ìˆ˜ì§‘",
            "sub_questions": [
                {{"question": "[step-1ì˜ ê²°ê³¼] í”¼í•´ ì§€ì—­ë³„ ì£¼ìš” ë†ì‚°ë¬¼ ë° ì¶•ì‚°ë¬¼ ìƒì‚°ì§€ ë§¤í•‘", "tool": "graph_db_search"}},
                {{"question": "[step-1ì˜ ê²°ê³¼] í•´ë‹¹ ì§€ì—­ ì£¼ìš” ì‹ì¬ë£Œ ìµœê·¼ ì‹œì„¸ ë° ê°€ê²© ë³€ë™", "tool": "rdb_search"}}
            ]
        }},
        {{
            "step": 3,
            "reasoning": "ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ í”¼í•´ ê·œëª¨ì™€ ê³µê¸‰ë§ ì˜í–¥ì„ ë¶„ì„",
            "sub_questions": [
                {{"question": "[step-2ì˜ ê²°ê³¼]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§‘ì¤‘í˜¸ìš°ê°€ ë†ì‚°ë¬¼ ê³µê¸‰ë§ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ë¶„ì„", "tool": "vector_db_search"}}
            ]
        }}
    ]
}}

---
**## ìµœì¢… ì¶œë ¥ í¬ë§·**

**ì¤‘ìš” ê·œì¹™**: 
- **ì§ˆë¬¸ ë³µì¡ì„±ì— ë”°ë¼ ì ì ˆí•œ ë„êµ¬ ê°œìˆ˜ ì„ íƒ**: ë‹¨ìˆœí•˜ë©´ 1ê°œ, ë³µí•©ì ì´ë©´ í•„ìš”í•œ ë§Œí¼ë§Œ
- **ìµœì‹  ì •ë³´** í¬í•¨ ì‹œ â†’ **web_search í¬í•¨**, ì¶”ê°€ë¡œ í•„ìš”í•œ ë¶„ì„/í†µê³„ ë„êµ¬ë§Œ ë³´ì™„
- **ì¼ë°˜ ì •ë³´** â†’ ë‚´ë¶€ DB(rdb, vector, graph, arxiv) ì¤‘ ê°€ì¥ ì í•©í•œ ê²ƒ ì„ íƒ
- **ê³¼ë„í•œ ë„êµ¬ ì‚¬ìš© ê¸ˆì§€**: ë¶ˆí•„ìš”í•œ ì¤‘ë³µ ê²€ìƒ‰ìœ¼ë¡œ ì„±ëŠ¥ ì €í•˜ ë°©ì§€
- ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì•¼ í•©ë‹ˆë‹¤.

{{
    "title": "ë¶„ì„ ë³´ê³ ì„œì˜ ì „ì²´ ì œëª©",
    "reasoning": "ì´ëŸ¬í•œ ë‹¨ê³„ë³„ ê³„íšì„ ìˆ˜ë¦½í•œ í•µì‹¬ì ì¸ ì´ìœ .",
    "execution_steps": [
        {{
            "step": 1,
            "reasoning": "1ë‹¨ê³„ ê³„íšì— ëŒ€í•œ ì„¤ëª…. ë³‘ë ¬ ì‹¤í–‰ë  ì‘ì—…ë“¤ì„ ê¸°ìˆ .",
            "sub_questions": [
                {{
                    "question": "1ë‹¨ê³„ì—ì„œ ë³‘ë ¬ë¡œ ì‹¤í–‰í•  ì²« ë²ˆì§¸ í•˜ìœ„ ì§ˆë¬¸",
                    "tool": "ì„ íƒëœ ë„êµ¬ ì´ë¦„"
                }},
                {{
                    "question": "1ë‹¨ê³„ì—ì„œ ë³‘ë ¬ë¡œ ì‹¤í–‰í•  ë‘ ë²ˆì§¸ í•˜ìœ„ ì§ˆë¬¸",
                    "tool": "ì„ íƒëœ ë„êµ¬ ì´ë¦„"
                }}
            ]
        }},
        {{
            "step": 2,
            "reasoning": "2ë‹¨ê³„ ê³„íšì— ëŒ€í•œ ì„¤ëª…. 1ë‹¨ê³„ ê²°ê³¼ì— ì˜ì¡´í•¨ì„ ëª…ì‹œ.",
            "sub_questions": [
                {{
                    "question": "2ë‹¨ê³„ì—ì„œ ì‹¤í–‰í•  í•˜ìœ„ ì§ˆë¬¸ (í•„ìš”ì‹œ '[step-1ì˜ ê²°ê³¼]' í¬í•¨)",
                    "tool": "ì„ íƒëœ ë„êµ¬ ì´ë¦„"
                }}
            ]
        }}
    ]
}}
"""

        try:
            response = await self.llm.ainvoke(planning_prompt)
            content = response.content.strip()

            json_match = re.search(r'```json\s*(\{.*?\})\s*```', content, re.DOTALL)
            if not json_match:
                json_match = re.search(r'\{.*\}', content, re.DOTALL)

            if json_match:
                json_str = json_match.group(1) if '```' in json_match.group(0) else json_match.group(0)
                plan = json.loads(json_str)
            else:
                raise ValueError("Valid JSON plan not found in response")

            print(f"  - ì§€ëŠ¥í˜• ë‹¨ê³„ë³„ ê³„íš ìƒì„± ì™„ë£Œ: {plan.get('title', 'ì œëª© ì—†ìŒ')}")
            print("  - ê³„íš JSON:")
            print(json.dumps(plan, ensure_ascii=False, indent=2))
            state["plan"] = plan

        except Exception as e:
            print(f"  - ì§€ëŠ¥í˜• ê³„íš ìƒì„± ì‹¤íŒ¨, ë‹¨ì¼ ë‹¨ê³„ë¡œ ì§ì ‘ ê²€ìƒ‰ ì‹¤í–‰: {e}")
            state["plan"] = {
                "title": f"{query} ë¶„ì„",
                "reasoning": "ì§€ëŠ¥í˜• ë‹¨ê³„ë³„ ê³„íš ìˆ˜ë¦½ì— ì‹¤íŒ¨í•˜ì—¬, ì‚¬ìš©ì ì›ë³¸ ì¿¼ë¦¬ë¡œ ì§ì ‘ ê²€ìƒ‰ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.",
                "execution_steps": [{
                    "step": 1,
                    "reasoning": "Fallback ì‹¤í–‰",
                    "sub_questions": [{"question": query, "tool": "vector_db_search"}]
                }]
            }

        return state



    # â­ í•µì‹¬ ìˆ˜ì •: ìš”ì•½ëœ ë‚´ìš©ì´ ì•„ë‹Œ ì „ì²´ ì›ë³¸ ë‚´ìš©ì„ LLMì—ê²Œ ì œê³µ
    async def _select_relevant_data_for_step(self, step_info: Dict, current_collected_data: List[SearchResult], query: str) -> List[int]:
        """í˜„ì¬ ë‹¨ê³„ì—ì„œ ìˆ˜ì§‘ëœ ë°ì´í„° ì¤‘ ê´€ë ¨ ìˆëŠ” ê²ƒë§Œ LLMì´ ì„ íƒ (ì „ì²´ ë‚´ìš© ê¸°ë°˜)"""

        step_title = f"Step {step_info['step']}"
        step_reasoning = step_info.get('reasoning', '')
        sub_questions = step_info.get('sub_questions', [])

        # â­ í•µì‹¬ ê°œì„ : ì „ì²´ ì›ë³¸ ë‚´ìš©ì„ LLMì—ê²Œ ì œê³µ (ìš”ì•½ ì—†ì´)
        full_data_context = ""
        for i, res in enumerate(current_collected_data):
            source = getattr(res, 'source', 'Unknown')
            title = getattr(res, 'title', 'No Title')
            content = getattr(res, 'content', '')  # â­ ì „ì²´ ë‚´ìš© (ìš”ì•½ ì•ˆí•¨)

            full_data_context += f"""
    --- ë°ì´í„° ì¸ë±ìŠ¤ [{i}] ---
    ì¶œì²˜: {source}
    ì œëª©: {title}
    ì „ì²´ ë‚´ìš©: {content}

    """

        # í˜„ì¬ ë‹¨ê³„ì˜ ì§ˆë¬¸ë“¤
        questions_summary = ""
        for sq in sub_questions:
            questions_summary += f"- {sq.get('question', '')} ({sq.get('tool', '')})\n"

        # â­ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ê´€ë¦¬ (ì¤‘ìš”í•œ ë¶€ë¶„ë§Œ ì˜ë¼ë‚´ê¸°)
        # ë„ˆë¬´ ê¸¸ë©´ ê° ë°ì´í„°ë‹¹ ìµœëŒ€ 1000ìë¡œ ì œí•œ
        if len(full_data_context) > 15000:
            print(f"  - ì»¨í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸º ({len(full_data_context)}ì), ë°ì´í„°ë³„ 1000ìë¡œ ì œí•œ")

            truncated_context = ""
            for i, res in enumerate(current_collected_data):
                source = getattr(res, 'source', 'Unknown')
                title = getattr(res, 'title', 'No Title')
                content = getattr(res, 'content', '')[:1000]  # 1000ìë¡œ ì œí•œ

                truncated_context += f"""
    --- ë°ì´í„° ì¸ë±ìŠ¤ [{i}] ---
    ì¶œì²˜: {source}
    ì œëª©: {title}
    ë‚´ìš©: {content}{"..." if len(getattr(res, 'content', '')) > 1000 else ""}

    """
            full_data_context = truncated_context

        selection_prompt = f"""
    ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    ìˆ˜ì§‘ëœ ë°ì´í„° ì¤‘ì—ì„œ **ì°¨íŠ¸ ìƒì„±ê³¼ ë³´ê³ ì„œ ì‘ì„±ì— í•„ìš”í•œ ë°ì´í„°**ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì„ íƒí•´ì£¼ì„¸ìš”.

    **ì „ì²´ ì‚¬ìš©ì ì§ˆë¬¸**: "{query}"

    **í˜„ì¬ ë‹¨ê³„ ì •ë³´**:
    - {step_title}: {step_reasoning}
    - ì‹¤í–‰í•œ ì§ˆë¬¸ë“¤:
    {questions_summary}

    **ìˆ˜ì§‘ëœ ì „ì²´ ë°ì´í„°** (ì „ì²´ ë‚´ìš© í¬í•¨):
    {full_data_context}

    **ì„ íƒ ê¸°ì¤€**:
    1. **ì°¨íŠ¸ ìƒì„±ìš© ìˆ˜ì¹˜/í†µê³„ ë°ì´í„° ìµœìš°ì„  ì„ íƒ**:
       - ë§¤ì¶œì•¡, ì‹œì¥ê·œëª¨, ì ìœ ìœ¨, ìƒì‚°ëŸ‰, ê°€ê²© ë“± ìˆ˜ì¹˜ ë°ì´í„°
       - %, ì–µì›, ì¡°ì›, ì²œí†¤ ë“± ë‹¨ìœ„ê°€ í¬í•¨ëœ ë°ì´í„°
       - í‘œ, í†µê³„, í˜„í™©í‘œ, ìˆœìœ„, ë¹„êµ, ë¶„ì„ ë°ì´í„°
       - ì§€ì—­ë³„/í’ˆëª©ë³„/ì‹œê¸°ë³„ ë¹„êµ ë°ì´í„°
       - ì—°ë„ë³„, ì›”ë³„ ì‹œê³„ì—´ ë°ì´í„°
    2. **ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ë°ì´í„°**
    3. **ë°°ê²½ ì •ë³´ ë° ì»¨í…ìŠ¤íŠ¸ ë°ì´í„°** (ì¤‘ìš”í•œ ê²ƒë§Œ)
    4. **ì¤‘ë³µ ì œê±°**: ì™„ì „íˆ ë™ì¼í•œ ë‚´ìš©ì€ í•˜ë‚˜ë§Œ ì„ íƒ

    **ì œì™¸ ê¸°ì¤€**:
    - ì§ˆë¬¸ê³¼ ì „í˜€ ê´€ë ¨ ì—†ëŠ” ì£¼ì œ
    - ì™„ì „íˆ ì¤‘ë³µë˜ëŠ” ë‚´ìš© (ìœ ì‚¬í•˜ì§€ë§Œ ë‹¤ë¥¸ ê´€ì ì´ë©´ í¬í•¨)
    - ê´‘ê³ ì„± ë‚´ìš© (ë‹¨, ì‹œì¥ ë°ì´í„° í¬í•¨ì‹œ ì„ íƒ)
    - ì¼ë°˜ì ì¸ ì„¤ëª…ë§Œ ìˆê³  êµ¬ì²´ì  ë°ì´í„°ê°€ ì—†ëŠ” ë‚´ìš©

    **ëª©í‘œ**: ì°¨íŠ¸ ìƒì„±ì— í•„ìš”í•œ ì¶©ë¶„í•œ ë°ì´í„° í™•ë³´ (í’ˆì§ˆ ì¤‘ì‹¬ ì„ íƒ)

    ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
    {{
        "selected_indexes": [ì„ íƒëœ ì¸ë±ìŠ¤ë“¤],
        "reasoning": "ì„ íƒëœ ê° ë°ì´í„°ê°€ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ì°¨íŠ¸ ìƒì„±ì— ì–´ë–»ê²Œ ê¸°ì—¬í•˜ëŠ”ì§€ ì„¤ëª…",
        "rejected_reason": "ì œì™¸ëœ ë°ì´í„°ë“¤ì˜ ì œì™¸ ì´ìœ "
    }}
    """

        try:
            response = await self._invoke_with_fallback(
                selection_prompt,
                self.llm,
                self.llm_openai_mini
            )

            # JSON íŒŒì‹±
            result = json.loads(re.search(r'\{.*\}', response.content, re.DOTALL).group())
            selected_indexes = result.get("selected_indexes", [])
            reasoning = result.get("reasoning", "")
            rejected_reason = result.get("rejected_reason", "")

            # ì¸ë±ìŠ¤ ìœ íš¨ì„± ê²€ì¦
            max_index = len(current_collected_data) - 1
            valid_indexes = [idx for idx in selected_indexes if isinstance(idx, int) and 0 <= idx <= max_index]
            
            # â­ í•µì‹¬ ìˆ˜ì •: ì°¨íŠ¸ ìƒì„±ìš© ìµœì†Œ ë°ì´í„° í™•ë³´
            total_available = len(current_collected_data)
            min_selection = max(3, min(6, total_available))  # ìµœì†Œ 3ê°œ, ìµœëŒ€ 6ê°œ (ì „ì²´ ê°œìˆ˜ ê³ ë ¤)
            
            if len(valid_indexes) < min_selection:
                print(f"  - âš ï¸ ì°¨íŠ¸ ìƒì„±ì„ ìœ„í•´ ìµœì†Œ {min_selection}ê°œ ë°ì´í„°ê°€ í•„ìš”í•˜ì§€ë§Œ {len(valid_indexes)}ê°œë§Œ ì„ íƒë¨")
                # ì„ íƒë˜ì§€ ì•Šì€ ì¸ë±ìŠ¤ë“¤ ì¤‘ì—ì„œ ì¶”ê°€ ì„ íƒ (ì²« ë²ˆì§¸ë¶€í„° ìˆœì„œëŒ€ë¡œ)
                unselected = [i for i in range(total_available) if i not in valid_indexes]
                additional_needed = min_selection - len(valid_indexes)
                additional_selected = unselected[:additional_needed]
                valid_indexes.extend(additional_selected)
                valid_indexes = sorted(valid_indexes)
                print(f"  - ğŸ”§ ì¶”ê°€ ì„ íƒëœ ì¸ë±ìŠ¤: {additional_selected}")

            print(f"  - LLM ë°ì´í„° ì„ íƒ ì™„ë£Œ:")
            print(f"    ì„ íƒëœ ì¸ë±ìŠ¤: {valid_indexes} (ì´ {len(valid_indexes)}/{total_available}ê°œ)")
            print(f"    ì„ íƒ ì´ìœ : {reasoning}")
            if rejected_reason:
                print(f"    ì œì™¸ ì´ìœ : {rejected_reason}")

            # ì„ íƒëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
            print(f"  - ì„ íƒëœ ë°ì´í„° ëª©ë¡:")
            for idx in valid_indexes:
                data_item = current_collected_data[idx]
                title = getattr(data_item, 'title', 'No Title')[:60]
                source = getattr(data_item, 'source', 'Unknown')
                print(f"    [{idx:2d}] {source:10s} | {title}")

            return valid_indexes

        except Exception as e:
            print(f"  - LLM ë°ì´í„° ì„ íƒ ì‹¤íŒ¨: {e}")
            # fallback: í˜„ì¬ ë‹¨ê³„ì—ì„œ ìˆ˜ì§‘ëœ ëª¨ë“  ë°ì´í„° ìœ ì§€
            return list(range(len(current_collected_data)))


   
    async def execute_report_workflow(self, state: StreamingAgentState) -> AsyncGenerator[str, None]:
        """ë‹¨ê³„ë³„ ê³„íšì— ë”°ë¼ ìˆœì°¨ì , ë³‘ë ¬ì ìœ¼ë¡œ ë°ì´í„° ìˆ˜ì§‘ ë° ë³´ê³ ì„œ ìƒì„±"""
        query = state["original_query"]

        # --- ì¶”ê°€: í˜ë¥´ì†Œë‚˜ í™•ì¸ ë° ìƒíƒœ ì•Œë¦¼ ---
        # ì‚¬ìš©ìê°€ ì„ íƒí•œ í˜ë¥´ì†Œë‚˜ê°€ stateì— ì´ë¯¸ í¬í•¨ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
        # ì˜ˆ: state['persona'] = 'êµ¬ë§¤ ë‹´ë‹¹ì'
        selected_persona = state.get("persona")
        if not selected_persona or selected_persona not in self.personas:
            print(f"ê²½ê³ : ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ ì§€ì •ë˜ì§€ ì•Šì€ í˜ë¥´ì†Œë‚˜ ('{selected_persona}'). 'ê¸°ë³¸'ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
            selected_persona = "ê¸°ë³¸"
            state["persona"] = selected_persona

        yield self._create_status_event("PLANNING", "PERSONA_CONFIRMED", f"'{selected_persona}' í˜ë¥´ì†Œë‚˜ë¡œ ë³´ê³ ì„œ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

        # ì°¨íŠ¸ ì¹´ìš´í„° ì´ˆê¸°í™”
        state['chart_counter'] = 0

        # ëˆ„ì  context ì´ˆê¸°í™”
        accumulated_context = {
            "generated_sections": [],  # ìƒì„±ëœ ì„¹ì…˜ ë‚´ìš©ë“¤
            "chart_data": [],  # ìƒì„±ëœ ì°¨íŠ¸ ë°ì´í„°ë“¤
            "insights": [],  # ê° ì„¹ì…˜ì˜ ì£¼ìš” ì¸ì‚¬ì´íŠ¸
            "persona": selected_persona  # ì„ íƒëœ í˜ë¥´ì†Œë‚˜
        }
        state['accumulated_context'] = accumulated_context

        # 1. ë‹¨ê³„ë³„ ê³„íš ìˆ˜ë¦½
        yield self._create_status_event("PLANNING", "GENERATE_PLAN_START", "ë¶„ì„ ê³„íš ìˆ˜ë¦½ ì¤‘...")
        state_with_plan = await self.generate_plan(state)
        plan = state_with_plan.get("plan", {})

        yield {"type": "plan", "data": {"plan": plan}}

        yield self._create_status_event("PLANNING", "GENERATE_PLAN_COMPLETE", "ë¶„ì„ ê³„íš ìˆ˜ë¦½ ì™„ë£Œ.", details={
            "plan_title": plan.get('title'),
            "plan_reasoning": plan.get('reasoning'),
            "step_count": len(plan.get("execution_steps", []))
        })

        await asyncio.sleep(0.01)

        # 2. ë‹¨ê³„ë³„ ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰
        execution_steps = plan.get("execution_steps", [])
        final_collected_data: List[SearchResult] = []
        step_results_context: Dict[int, str] = {}
        cumulative_selected_indexes: List[int] = []

        for i, step_info in enumerate(execution_steps):
            current_step_index = step_info["step"]
            yield self._create_status_event("GATHERING", "STEP_START", f"ë°ì´í„° ìˆ˜ì§‘ ({i + 1}/{len(execution_steps)}) ì‹œì‘.")

            tasks_for_this_step = []
            for sq in step_info.get("sub_questions", []):
                injected_query = self._inject_context_into_query(sq["question"], step_results_context)
                tasks_for_this_step.append({"tool": sq["tool"], "inputs": {"query": injected_query}})
            if not tasks_for_this_step:
                continue

            step_collected_data: List[SearchResult] = []

            async for event in self.data_gatherer.execute_parallel_streaming(tasks_for_this_step, state=state):
                if event["type"] == "search_results":
                    yield event
                elif event["type"] == "collection_complete":
                    step_collected_data = event["data"]["collected_data"]

            summary_of_step = " ".join([res.content for res in step_collected_data])
            step_results_context[current_step_index] = summary_of_step[:2000]
            final_collected_data.extend(step_collected_data)

            print(f">> {current_step_index}ë‹¨ê³„ ì™„ë£Œ: {len(step_collected_data)}ê°œ ë°ì´í„° ìˆ˜ì§‘. (ì´ {len(final_collected_data)}ê°œ)")

            if len(final_collected_data) > 0:
                yield self._create_status_event("PROCESSING", "FILTER_DATA_START", "ìˆ˜ì§‘ ë°ì´í„° ì„ ë³„ ì¤‘...")

                # reasoningì„ ë°˜í™˜í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ selected_indexesë§Œ ë°›ìŠµë‹ˆë‹¤.
                selected_indexes = await self._select_relevant_data_for_step(
                    step_info, final_collected_data, state["original_query"]
                )

                yield self._create_status_event("PROCESSING", "FILTER_DATA_COMPLETE", f"í•µì‹¬ ë°ì´í„° {len(selected_indexes)}ê°œ ì„ ë³„ ì™„ë£Œ.", details={
                    "selected_indices": selected_indexes
                })
                cumulative_selected_indexes = sorted(list(set(cumulative_selected_indexes + selected_indexes)))

        # >> í•µì‹¬ ìˆ˜ì •: ì „ì²´ ë°ì´í„° ë”•ì…”ë„ˆë¦¬ë¥¼ í”„ë¡ íŠ¸ë¡œ ë¨¼ì € ì „ì†¡
        print(f"\n>> ì „ì²´ ë°ì´í„° ë”•ì…”ë„ˆë¦¬ ìƒì„± ë° ì „ì†¡")

        # ì „ì²´ ë°ì´í„°ë¥¼ ì¸ë±ìŠ¤:ë°ì´í„° í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        full_data_dict = {}
        print(f"\nğŸ” === FULL_DATA_DICT ìƒì„± ë””ë²„ê¹… ===")
        print(f"final_collected_data ì´ ê°œìˆ˜: {len(final_collected_data)}")

        for idx, data in enumerate(final_collected_data):
            full_data_dict[idx] = {
                "title": getattr(data, 'title', 'No Title'),
                "content": getattr(data, 'content', ''),
                "source": getattr(data, 'source', 'Unknown'),
                "url": getattr(data, 'url', ''),
                "source_url": getattr(data, 'source_url', ''),
                "score": getattr(data, 'score', 0.0),
                "document_type": getattr(data, 'document_type', 'unknown')
            }

            # ì²« 5ê°œì™€ ë§ˆì§€ë§‰ 5ê°œë§Œ ìƒì„¸ ë¡œê·¸
            if idx < 5 or idx >= len(final_collected_data) - 5:
                print(f"  [{idx}]: ì œëª©='{getattr(data, 'title', 'No Title')[:50]}...' ì¶œì²˜='{getattr(data, 'source', 'Unknown')}'")

        print(f"ì „ì²´ ë°ì´í„° ë”•ì…”ë„ˆë¦¬ í‚¤ë“¤: {list(full_data_dict.keys())}")
        print(f"ì „ì²´ ë°ì´í„° ë”•ì…”ë„ˆë¦¬ í¬ê¸°: {len(full_data_dict)}ê°œ")

        # ì „ì²´ ë°ì´í„° ë”•ì…”ë„ˆë¦¬ë¥¼ í”„ë¡ íŠ¸ë¡œ ì „ì†¡
        print(f"\nğŸš€ === í”„ë¡ íŠ¸ì—”ë“œë¡œ FULL_DATA_DICT ì „ì†¡ ===")
        print(f"ì „ì†¡í•  ë°ì´í„° êµ¬ì¡°:")
        print(f"  type: 'full_data_dict'")
        print(f"  data.data_dict í‚¤ë“¤: {list(full_data_dict.keys())}")
        print(f"  data.data_dict í¬ê¸°: {len(full_data_dict)}")

        # ìƒ˜í”Œ ë°ì´í„° í™•ì¸ (ì²« ë²ˆì§¸ ê²ƒë§Œ)
        if full_data_dict:
            first_key = list(full_data_dict.keys())[0]
            first_item = full_data_dict[first_key]
            print(f"  ìƒ˜í”Œ [{first_key}]: ì œëª©='{first_item['title'][:30]}...' ì¶œì²˜='{first_item['source']}'")

        yield {"type": "full_data_dict", "data": {"data_dict": full_data_dict}}

        # 3. ì„¹ì…˜ë³„ ë°ì´í„° ìƒíƒœ ë¶„ì„ ë° ë³´ê³ ì„œ êµ¬ì¡° ì„¤ê³„
        # ì¤‘ë³µëœ full_data_dict ìƒì„± ë° ì „ì†¡ ì œê±° (ì´ë¯¸ ìœ„ì—ì„œ ì²˜ë¦¬ë¨)

        yield self._create_status_event("PROCESSING", "DESIGN_STRUCTURE_START", "ë³´ê³ ì„œ êµ¬ì¡° ì„¤ê³„ ì¤‘...")

        design = None
        async for result in self.processor.process("design_report_structure", final_collected_data, cumulative_selected_indexes, query, state=state):
            if result.get("type") == "result":
                design = result.get("data")
                break

        if not design or "structure" not in design or not design["structure"]:
            yield {"type": "error", "data": {"message": "ë³´ê³ ì„œ êµ¬ì¡° ì„¤ê³„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."}}
            return

        # â­ í•µì‹¬ ì¶”ê°€: ë°ì´í„° ë¶€ì¡± ì„¹ì…˜ í™•ì¸ ë° ì¶”ê°€ ê²€ìƒ‰
        insufficient_sections = []
        for i, section in enumerate(design.get("structure", [])):
            if not section.get("is_sufficient", True):
                insufficient_sections.append((i, section))
        
        if insufficient_sections:
            print(f"ğŸ” ë°ì´í„° ë¶€ì¡± ì„¹ì…˜ {len(insufficient_sections)}ê°œ ë°œê²¬, ì¶”ê°€ ê²€ìƒ‰ ì‹¤í–‰")
            
            # ì¶”ê°€ ê²€ìƒ‰ ìˆ˜í–‰
            additional_data_collected = []
            for section_index, section in insufficient_sections:
                feedback = section.get("feedback_for_gatherer", {})
                if isinstance(feedback, dict) and feedback:
                    tool = feedback.get("tool", "vector_db_search")
                    query = feedback.get("query", f"{section.get('section_title', '')} ìƒì„¸ ë°ì´í„°")
                    
                    print(f"  - ì„¹ì…˜ '{section.get('section_title', '')}' ì¶”ê°€ ê²€ìƒ‰: {tool} - '{query}'")
                    
                    yield self._create_status_event("PROCESSING", "ADDITIONAL_SEARCH", f"'{query[:50]}...' ê´€ë ¨ ì¶”ê°€ ë°ì´í„° ìˆ˜ì§‘ ì¤‘")
                    
                    try:
                        # DataGatherer ì¸ìŠ¤í„´ìŠ¤ë¥¼ í†µí•´ ì¶”ê°€ ê²€ìƒ‰
                        from .worker_agents import DataGathererAgent
                        gatherer = DataGathererAgent()
                        additional_results, _ = await gatherer.execute(tool, {"query": query})
                        
                        if additional_results:
                            additional_data_collected.extend(additional_results)
                            print(f"    ì¶”ê°€ ë°ì´í„° {len(additional_results)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
                        else:
                            print(f"    ì¶”ê°€ ë°ì´í„° ì—†ìŒ")
                            
                    except Exception as e:
                        print(f"    ì¶”ê°€ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                        continue
            
            # ì¶”ê°€ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•© (êµ¬ì¡° ì¬ì„¤ê³„ ì—†ì´)
            if additional_data_collected:
                print(f"âœ… ì´ {len(additional_data_collected)}ê°œ ì¶”ê°€ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
                
                # ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•©í•˜ì—¬ ë°ì´í„°ë§Œ ë³´ê°•
                enhanced_data = final_collected_data + additional_data_collected
                final_collected_data = enhanced_data  # ë°ì´í„° ì—…ë°ì´íŠ¸
                
                print(f"ğŸ”„ ë°ì´í„° ë³´ê°• ì™„ë£Œ: ê¸°ì¡´ {len(final_collected_data) - len(additional_data_collected)}ê°œ â†’ {len(final_collected_data)}ê°œ")
                
                # ë³´ê°•ëœ ë°ì´í„°ë¡œ ë¶€ì¡± ì„¹ì…˜ì˜ is_sufficient ìƒíƒœ ì—…ë°ì´íŠ¸
                for section_index, section in insufficient_sections:
                    section["is_sufficient"] = True
                    section["feedback_for_gatherer"] = ""
                    print(f"  - ì„¹ì…˜ '{section.get('section_title', '')}' ë°ì´í„° ë³´ê°•ìœ¼ë¡œ ì¶©ë¶„ ìƒíƒœë¡œ ë³€ê²½")
                
                yield self._create_status_event("PROCESSING", "DATA_ENHANCED", f"ë°ì´í„° ë³´ê°• ì™„ë£Œ. ì´ {len(final_collected_data)}ê°œ ë°ì´í„°ë¡œ ë³´ê³ ì„œ ìƒì„± ì‹œì‘")
            else:
                print(f"âš ï¸ ì¶”ê°€ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨, ê¸°ì¡´ ë°ì´í„°ë¡œ ì§„í–‰")

        section_titles = [s.get('section_title', 'ì œëª© ì—†ìŒ') for s in design.get('structure', [])]
        yield self._create_status_event("PROCESSING", "DESIGN_STRUCTURE_COMPLETE", "ë³´ê³ ì„œ êµ¬ì¡° ì„¤ê³„ ì™„ë£Œ.", details={
            "report_title": design.get("title"),
            "section_titles": section_titles
        })

        # ë³´ê³ ì„œ ì œëª©ì„ ê°€ì¥ ë¨¼ì € ìŠ¤íŠ¸ë¦¬ë°
        yield {"type": "content", "data": {"chunk": f"# {design.get('title', query)}\n\n---\n\n"}}

        # 4. ì „ì²´ ë³´ê³ ì„œ êµ¬ì¡° ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        # ê° ì„¹ì…˜ ìƒì„± Agentê°€ ì „ì²´ ê·¸ë¦¼ì„ ì¸ì§€í•˜ë„ë¡ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë§Œë“¤ì–´ ì „ë‹¬í•©ë‹ˆë‹¤.
        awareness_context = "ì•„ë˜ëŠ” ì „ì²´ ë³´ê³ ì„œì˜ ëª©ì°¨ì…ë‹ˆë‹¤. ë‹¹ì‹ ì€ ì´ ì¤‘ í•˜ë‚˜ì˜ ì„¹ì…˜ ì‘ì„±ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.\n\n"
        structure = design.get("structure", [])
        for i, sec in enumerate(structure):
            awareness_context += f"- **ì„¹ì…˜ {i+1}. {sec.get('section_title', '')}**: {sec.get('description', '')}\n"

        # 5. ê° ì„¹ì…˜ì˜ ê²°ê³¼ë¥¼ ë‹´ì„ Queue ë¦¬ìŠ¤íŠ¸ì™€ ë³‘ë ¬ ì‹¤í–‰í•  Task ë¦¬ìŠ¤íŠ¸ ìƒì„±
        section_queues = [asyncio.Queue() for _ in structure]
        producer_tasks = []

        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ê° ì„¹ì…˜ ë‚´ìš©ì„ ìƒì„±í•˜ê³  Queueì— ë„£ëŠ” ì½”ë£¨í‹´
        async def _produce_section_content(section_index: int):
            section_info = structure[section_index]
            q = section_queues[section_index]

            use_contents = section_info.get("use_contents", [])

            try:
                # generate_section_streamingì„ í˜¸ì¶œí•˜ì—¬ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì²­í¬ë¥¼ ë°›ìŒ
                async for chunk in self.processor.generate_section_streaming(
                    section_info, full_data_dict, query, use_contents,
                    awareness_context=awareness_context,
                    state=state
                ):
                    await q.put(chunk)  # ë°›ì€ ì²­í¬ë¥¼ íì— ë„£ìŒ
            except Exception as e:
                # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ íì— ë„£ìŒ
                error_message = f"*'{section_info.get('section_title', '')}' ì„¹ì…˜ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}*\n\n"
                await q.put(error_message)
                print(f">> ì„¹ì…˜ ìƒì„±(Producer) ì˜¤ë¥˜: {error_message}")
            finally:
                # ìŠ¤íŠ¸ë¦¼ì´ ëë‚˜ë©´ Noneì„ ë„£ì–´ ì¢…ë£Œë¥¼ ì•Œë¦¼
                await q.put(None)

        # ëª¨ë“  ì„¹ì…˜ì— ëŒ€í•œ Producer Taskë¥¼ ìƒì„±í•˜ê³  ì‹¤í–‰ ëª©ë¡ì— ì¶”ê°€
        for i in range(len(structure)):
            task = asyncio.create_task(_produce_section_content(i))
            producer_tasks.append(task)

        # 6. ìˆœì°¨ì ìœ¼ë¡œ Queueì—ì„œ ê²°ê³¼ë¥¼ êº¼ë‚´ ìŠ¤íŠ¸ë¦¬ë° (Consumer)
        for i, section in enumerate(structure):
            section_title = section.get('section_title', f'ì„¹ì…˜ {i+1}')
            use_contents = section.get("use_contents", [])

            yield self._create_status_event("GENERATING", "GENERATE_SECTION_START", f"'{section_title}' ì„¹ì…˜ ìƒì„± ì¤‘...", details={
                "section_index": i, "section_title": section_title, "using_indices": use_contents
            })

            buffer = ""
            # âœ… section_full_content ë³€ìˆ˜ ì´ˆê¸°í™” ì¶”ê°€
            section_full_content = ""
            section_data_list = [final_collected_data[idx] for idx in use_contents if 0 <= idx < len(final_collected_data)]
             # âœ… section_content_generated ë³€ìˆ˜ ì¶”ê°€ (ìƒì„± ì‹¤íŒ¨ ì—¬ë¶€ í™•ì¸ìš©)
            section_content_generated = False

            # í•´ë‹¹ ì„¹ì…˜ì˜ Queueì—ì„œ ê²°ê³¼ê°€ ë‚˜ì˜¬ ë•Œê¹Œì§€ ëŒ€ê¸°
            while True:
                chunk = await section_queues[i].get()

                # Noneì„ ë°›ìœ¼ë©´ í•´ë‹¹ ì„¹ì…˜ ìŠ¤íŠ¸ë¦¬ë°ì´ ëë‚œ ê²ƒ
                if chunk is None:
                    break

                # âœ… ì²­í¬ë¥¼ ë°›ì•˜ë‹¤ë©´ ë‚´ìš©ì´ ìƒì„±ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼
                section_content_generated = True
                buffer += chunk
                # âœ… ì „ì²´ ì„¹ì…˜ ë‚´ìš©ë„ ë³„ë„ë¡œ ëˆ„ì 
                section_full_content += chunk

                # ì°¨íŠ¸ ìƒì„± ë§ˆì»¤ê°€ ìˆëŠ”ì§€ í™•ì¸
                if "[GENERATE_CHART]" in buffer:
                    parts = buffer.split("[GENERATE_CHART]", 1)

                    if parts[0]:
                        yield {"type": "content", "data": {"chunk": parts[0]}}

                    buffer = parts[1]

                    yield self._create_status_event("GENERATING", "GENERATE_CHART_START", f"'{section_title}' ì°¨íŠ¸ ìƒì„± ì¤‘...")

                    # ì°¨íŠ¸ ìƒì„± ê³¼ì •ì˜ ìƒíƒœ ë©”ì‹œì§€ë¥¼ ìœ„í•œ ì½œë°±
                    async def chart_yield_callback(event_data):
                        print(f"ì°¨íŠ¸ ìƒì„± ìƒíƒœ: {event_data}")
                        return event_data

                    # ì´ì „ê¹Œì§€ ìƒì„±ëœ contextë¥¼ ì°¨íŠ¸ ìƒì„±ì— ì „ë‹¬
                    chart_context = {
                        "previous_sections": accumulated_context["generated_sections"],
                        "previous_charts": accumulated_context["chart_data"],
                        "current_section_content": buffer
                    }
                    state['chart_context'] = chart_context

                    chart_data = None
                    async for result in self.processor.process("create_chart_data", section_data_list, section_title, buffer, "", chart_yield_callback, state=state):
                        if result.get("type") == "chart":
                            chart_data = result.get("data")
                            accumulated_context["chart_data"].append({
                                "section": section_title,
                                "chart": chart_data
                            })

                            # ì°¨íŠ¸ ìƒì„± ê²€ì¦ ë¡œê·¸ ê¸°ë¡
                            await self._log_chart_verification(query, section_title, section_data_list, chart_data, state)
                            break

                    if chart_data and "error" not in chart_data:
                        current_chart_index = state.get('chart_counter', 0)
                        chart_placeholder = f"\n\n[CHART-PLACEHOLDER-{current_chart_index}]\n\n"
                        yield {"type": "content", "data": {"chunk": chart_placeholder}}
                        yield {"type": "chart", "data": chart_data}
                        state['chart_counter'] = current_chart_index + 1
                    else:
                        print(f"   ì°¨íŠ¸ ìƒì„± ì‹¤íŒ¨: {chart_data}")
                        yield self._create_status_event("GENERATING", "GENERATE_CHART_FAILURE", f"'{section_title}' ì°¨íŠ¸ ìƒì„±ì´ ì™„ë£Œë˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                        yield {"type": "content", "data": {"chunk": "\n\n*[ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ì°¨íŠ¸ í‘œì‹œê°€ ì œí•œë©ë‹ˆë‹¤]*\n\n"}}

                else:
                    # í…ìŠ¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ë²„í¼ ê´€ë¦¬
                    potential_chart_marker = "[GENERATE_CHART]"
                    # ë²„í¼ ëì— ë§ˆì»¤ ì¼ë¶€ê°€ ê±¸ì³ìˆëŠ”ì§€ í™•ì¸
                    has_partial_marker = any(potential_chart_marker.startswith(buffer[-j:]) for j in range(1, min(len(buffer) + 1, len(potential_chart_marker) + 1)))

                    should_flush = (
                        not has_partial_marker and (
                            len(buffer) >= 120 or
                            buffer.endswith(('.', '!', '?', '\n', 'ë‹¤.', 'ìš”.', 'ë‹ˆë‹¤.', 'ìŠµë‹ˆë‹¤.', 'ë©ë‹ˆë‹¤.', 'ìˆìŠµë‹ˆë‹¤.')) or
                            '\n\n' in buffer
                        )
                    )

                    if should_flush:
                        yield {"type": "content", "data": {"chunk": buffer}}
                        buffer = ""

            # while ë£¨í”„ ì¢…ë£Œ í›„ ë‚¨ì€ ë²„í¼ ì²˜ë¦¬
            if buffer.strip():
                yield {"type": "content", "data": {"chunk": buffer}}

            # ìƒì„±ëœ ì„¹ì…˜ ë‚´ìš©ì„ ëˆ„ì  contextì— ì¶”ê°€
            if section_full_content:
                accumulated_context["generated_sections"].append({
                    "title": section_title,
                    "content": section_full_content,
                    "data_indices": use_contents
                })
                # ì£¼ìš” ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ
                first_paragraph = section_full_content.split("\n\n")[0] if "\n\n" in section_full_content else section_full_content[:200]
                accumulated_context["insights"].append({
                    "section": section_title,
                    "insight": first_paragraph
                })

                # ì„¹ì…˜ ìƒì„± ê²€ì¦ ë¡œê·¸ ê¸°ë¡
                await self._log_section_verification(query, section_title, section_data_list, section_full_content, state)

            # ë‚´ìš©ì´ ì „í˜€ ìƒì„±ë˜ì§€ ì•Šì€ ê²½ìš° ê²½ê³  ì²˜ë¦¬
            if not section_content_generated:
                print(f">> ê²½ê³ : ì„¹ì…˜ '{section_title}' ë‚´ìš© ìƒì„± ì‹¤íŒ¨")
                yield {"type": "content", "data": {"chunk": f"*'{section_title}' ì„¹ì…˜ ìƒì„± ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.*\n\n"}}

            # ê° ì„¹ì…˜ ì‚¬ì´ì— ê³µë°± ì¶”ê°€
            yield {"type": "content", "data": {"chunk": "\n\n"}}

        # ì›Œí¬í”Œë¡œìš° ì™„ë£Œ í›„ ì¶œì²˜ ì •ë³´ ì„¤ì • (ì‹¤ì œ ì‚¬ìš©ëœ ì¸ë±ìŠ¤ë§Œ)
        # ëª¨ë“  ì„¹ì…˜ì—ì„œ ì‚¬ìš©ëœ ì¸ë±ìŠ¤ë“¤ì„ ìˆ˜ì§‘
        used_indexes = set()
        for section in design.get("structure", []):
            use_contents = section.get("use_contents", [])
            used_indexes.update(use_contents)

        print(f">> ì‹¤ì œ ì‚¬ìš©ëœ ì¸ë±ìŠ¤ë“¤: {sorted(used_indexes)}")
        print(f">> final_collected_data ê¸¸ì´: {len(final_collected_data) if final_collected_data else 0}")
        print(f">> used_indexes ê¸¸ì´: {len(used_indexes)}")

        # sources ì´ë²¤íŠ¸ëŠ” ë” ì´ìƒ ë³´ë‚´ì§€ ì•ŠìŒ (full_data_dictë§Œ ì‚¬ìš©)
        # ëŒ€ì‹  ì‚¬ìš©ëœ ì¸ë±ìŠ¤ ì •ë³´ë§Œ ë¡œê¹…
        if final_collected_data and used_indexes:
            print(f">> ë³´ê³ ì„œì—ì„œ ì‹¤ì œ ì‚¬ìš©ëœ ì¸ë±ìŠ¤ë“¤: {sorted(used_indexes)}")
            print(f">> ì´ {len(used_indexes)}ê°œ ì¶œì²˜ ì‚¬ìš© (ì „ì²´ {len(final_collected_data)}ê°œ ì¤‘)")

        yield {"type": "complete", "data": {
            "message": "ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ"
        }}

    async def _update_use_contents_after_recollection(
    self,
    section_info: Dict,
    all_data: List[SearchResult],
    original_indexes: List[int],
    new_data_indexes: List[int],
    query: str
    ) -> List[int]:
        """ë³´ê°• í›„ í•´ë‹¹ ì„¹ì…˜ì˜ use_contentsë¥¼ LLMì´ ì—…ë°ì´íŠ¸ (ì „ì²´ ë‚´ìš© ê¸°ë°˜)"""

        section_title = section_info.get('section_title', 'ì„¹ì…˜')

        # â­ í•µì‹¬ ê°œì„ : ì „ì²´ ë‚´ìš©ì„ LLMì—ê²Œ ì œê³µ
        data_summary = ""

        # ê¸°ì¡´ ë°ì´í„° (ì „ì²´ ë‚´ìš©)
        data_summary += "=== ê¸°ì¡´ ì„ íƒëœ ë°ì´í„° (ì „ì²´ ë‚´ìš©) ===\n"
        for idx in original_indexes[:3]:  # ì²˜ìŒ 3ê°œë§Œ (ê¸¸ì´ ì œí•œ)
            if 0 <= idx < len(all_data):
                res = all_data[idx]
                content = getattr(res, 'content', '')[:800]  # 800ìë¡œ ì œí•œ
                data_summary += f"""
    [{idx:2d}] [{getattr(res, 'source', 'Unknown')}] {getattr(res, 'title', 'No Title')}
    ë‚´ìš©: {content}{"..." if len(getattr(res, 'content', '')) > 800 else ""}

    """

        # ìƒˆ ë°ì´í„° (ì „ì²´ ë‚´ìš©)
        data_summary += "=== ìƒˆë¡œ ì¶”ê°€ëœ ë°ì´í„° (ì „ì²´ ë‚´ìš©) ===\n"
        for idx in new_data_indexes:
            if 0 <= idx < len(all_data):
                res = all_data[idx]
                content = getattr(res, 'content', '')[:800]  # 800ìë¡œ ì œí•œ
                data_summary += f"""
    [{idx:2d}] [NEW] [{getattr(res, 'source', 'Unknown')}] {getattr(res, 'title', 'No Title')}
    ë‚´ìš©: {content}{"..." if len(getattr(res, 'content', '')) > 800 else ""}

    """

        update_prompt = f"""
    "{section_title}" ì„¹ì…˜ì„ ìœ„í•´ ê¸°ì¡´ ë°ì´í„°ì™€ ìƒˆë¡œ ì¶”ê°€ëœ ë°ì´í„°ì˜ **ì „ì²´ ë‚´ìš©ì„ ì½ê³ ** ê°€ì¥ ì í•©í•œ ë°ì´í„°ë“¤ì„ ì„ íƒí•´ì£¼ì„¸ìš”.

    **ì„¹ì…˜**: "{section_title}"
    **ì „ì²´ ì§ˆë¬¸**: "{query}"

    {data_summary[:8000]}

    **ì„ íƒ ê¸°ì¤€**:
    1. **ê° ë°ì´í„°ì˜ ì „ì²´ ë‚´ìš©ì„ ì½ê³ ** ì„¹ì…˜ ì£¼ì œì™€ì˜ ê´€ë ¨ì„± íŒë‹¨
    2. ì œëª©ë§Œ ë³´ê³  ê²°ì •í•˜ì§€ ë§ê³  **ì‹¤ì œ ë‚´ìš©ì˜ ì§ˆê³¼ ê´€ë ¨ì„±** í™•ì¸
    3. ìƒˆ ë°ì´í„°ëŠ” í•´ë‹¹ ì„¹ì…˜ì„ ìœ„í•´ íŠ¹ë³„íˆ ìˆ˜ì§‘ëœ ê²ƒì´ë¯€ë¡œ ì ê·¹ ê³ ë ¤
    4. ì‹¤ì œë¡œ ìœ ìš©í•œ ì •ë³´ê°€ ë‹´ê¸´ ë°ì´í„°ë§Œ ìµœëŒ€ 8ê°œ ì„ ë³„

    **ì›ë³¸**: {original_indexes}
    **ìƒˆ ë°ì´í„°**: {new_data_indexes}

    JSONìœ¼ë¡œë§Œ ì‘ë‹µ:
    {{
        "updated_use_contents": [0, 2, 5, 8],
        "reasoning": "ê° ë°ì´í„°ë¥¼ ì„ íƒ/ì œì™¸í•œ êµ¬ì²´ì  ì´ìœ  (ë‚´ìš© ê¸°ë°˜)"
    }}
    """

        try:
            response = await self._invoke_with_fallback(update_prompt, self.llm, self.llm_openai_mini)
            result = json.loads(re.search(r'\{.*\}', response.content, re.DOTALL).group())

            updated_indexes = result.get("updated_use_contents", [])
            reasoning = result.get("reasoning", "")

            # ìœ íš¨ì„± ê²€ì¦
            max_index = len(all_data) - 1
            valid_indexes = [idx for idx in updated_indexes if isinstance(idx, int) and 0 <= idx <= max_index]

            print(f"  - use_contents ì—…ë°ì´íŠ¸ ì™„ë£Œ (ì „ì²´ ë‚´ìš© ê¸°ë°˜):")
            print(f"    ìµœì¢… ì„ íƒ: {valid_indexes}")
            print(f"    ì„ íƒ ì´ìœ : {reasoning}")

            return valid_indexes

        except Exception as e:
            print(f"  - use_contents ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            # fallback: ì›ë³¸ + ìƒˆ ë°ì´í„° í•©ì¹˜ê¸° (ìµœëŒ€ 8ê°œ)
            combined = original_indexes + new_data_indexes
            return combined[:8]

    async def _log_chart_verification(self, query: str, section_title: str, section_data_list: List[SearchResult], chart_data: dict, state: dict):
        """ì°¨íŠ¸ ìƒì„± ê²€ì¦ì„ ìœ„í•œ ìƒì„¸ ë¡œê·¸ ê¸°ë¡"""
        try:
            # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
            log_base_dir = "/app/logs"
            chart_verification_dir = f"{log_base_dir}/chart_verification"
            os.makedirs(chart_verification_dir, exist_ok=True)

            # í˜„ì¬ ì‹œê°„ìœ¼ë¡œ íŒŒì¼ëª… ìƒì„±
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]  # ë°€ë¦¬ì´ˆ í¬í•¨
            session_id = state.get("session_id", "unknown")
            filename = f"chart_verification_{timestamp}_{session_id}.txt"
            filepath = f"{chart_verification_dir}/{filename}"

            # ê²€ì¦ ë¡œê·¸ ë‚´ìš© ìƒì„±
            verification_log = self._generate_chart_verification_content(
                query, section_title, section_data_list, chart_data, timestamp
            )

            # íŒŒì¼ì— ë¹„ë™ê¸°ë¡œ ì“°ê¸°
            async with aiofiles.open(filepath, 'w', encoding='utf-8') as f:
                await f.write(verification_log)

            print(f"ğŸ“‹ ì°¨íŠ¸ ê²€ì¦ ë¡œê·¸ ì €ì¥ì™„ë£Œ: {filepath}")

        except Exception as e:
            print(f"âŒ ì°¨íŠ¸ ê²€ì¦ ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}")

    async def _log_section_verification(self, query: str, section_title: str, section_data_list: List[SearchResult], section_content: str, state: dict):
        """ì„¹ì…˜ ìƒì„± ê²€ì¦ì„ ìœ„í•œ ìƒì„¸ ë¡œê·¸ ê¸°ë¡"""
        try:
            # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
            log_base_dir = "/app/logs"
            section_verification_dir = f"{log_base_dir}/section_verification"
            os.makedirs(section_verification_dir, exist_ok=True)

            # í˜„ì¬ ì‹œê°„ìœ¼ë¡œ íŒŒì¼ëª… ìƒì„±
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]  # ë°€ë¦¬ì´ˆ í¬í•¨
            session_id = state.get("session_id", "unknown")
            filename = f"section_verification_{timestamp}_{session_id}.txt"
            filepath = f"{section_verification_dir}/{filename}"

            # ê²€ì¦ ë¡œê·¸ ë‚´ìš© ìƒì„±
            verification_log = self._generate_section_verification_content(
                query, section_title, section_data_list, section_content, timestamp
            )

            # íŒŒì¼ì— ë¹„ë™ê¸°ë¡œ ì“°ê¸°
            async with aiofiles.open(filepath, 'w', encoding='utf-8') as f:
                await f.write(verification_log)

            print(f"ğŸ“„ ì„¹ì…˜ ê²€ì¦ ë¡œê·¸ ì €ì¥ì™„ë£Œ: {filepath}")

        except Exception as e:
            print(f"âŒ ì„¹ì…˜ ê²€ì¦ ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}")

    def _generate_chart_verification_content(self, query: str, section_title: str, section_data_list: List[SearchResult], chart_data: dict, timestamp: str) -> str:
        """ì°¨íŠ¸ ê²€ì¦ ë¡œê·¸ ë‚´ìš© ìƒì„±"""
        content = f"""
================================================================================
                          ì°¨íŠ¸ ìƒì„± ê²€ì¦ ë¡œê·¸
================================================================================

ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
íƒ€ì„ìŠ¤íƒ¬í”„: {timestamp}

================================================================================
ì°¨íŠ¸ ìƒì„± ì •ë³´
================================================================================

ì‚¬ìš©ì ì¿¼ë¦¬:
{query}

ì„¹ì…˜ ì œëª©:
{section_title}

================================================================================
ì„¹ì…˜ì—ì„œ ì‚¬ìš©í•œ ë°ì´í„° (ì´ {len(section_data_list)}ê°œ)
================================================================================
"""

        # ì„¹ì…˜ ë°ì´í„° ìƒì„¸ ì •ë³´
        for i, data_item in enumerate(section_data_list):
            source = getattr(data_item, 'source', 'Unknown')
            title = getattr(data_item, 'title', 'No Title')
            content_text = getattr(data_item, 'content', '')
            url = getattr(data_item, 'url', '')
            score = getattr(data_item, 'score', 0.0)
            doc_type = getattr(data_item, 'document_type', 'unknown')

            content += f"""
[ë°ì´í„° ì¸ë±ìŠ¤ {i}]
  ì¶œì²˜: {source}
  ì œëª©: {title}
  URL: {url}
  ì ìˆ˜: {score:.3f}
  íƒ€ì…: {doc_type}
  ë‚´ìš©:
  {content_text[:]}

"""

        content += f"""
================================================================================
ìƒì„±ëœ ì°¨íŠ¸ ë°ì´í„° (JSON)
================================================================================

{json.dumps(chart_data, ensure_ascii=False, indent=2)}

================================================================================
ê²€ì¦ í¬ì¸íŠ¸
================================================================================

1. ë°ì´í„° ì •í™•ì„±: ì°¨íŠ¸ì˜ ìˆ˜ì¹˜ê°€ ì‹¤ì œ ë°ì´í„°ì™€ ì¼ì¹˜í•˜ëŠ”ê°€?
2. ë°ì´í„° ì¶œì²˜: ì°¨íŠ¸ì— ì‚¬ìš©ëœ ì •ë³´ê°€ ìœ„ì˜ ì„¹ì…˜ ë°ì´í„°ì— ê¸°ë°˜í•˜ëŠ”ê°€?
3. ë…¼ë¦¬ì  ì¼ê´€ì„±: ì°¨íŠ¸ ìœ í˜•ì´ ë°ì´í„° íŠ¹ì„±ì— ì í•©í•œê°€?
4. í• ë£¨ì‹œë„¤ì´ì…˜ ì—¬ë¶€: ì‹¤ì œ ë°ì´í„°ì— ì—†ëŠ” ë‚´ìš©ì´ ì°¨íŠ¸ì— í¬í•¨ë˜ì—ˆëŠ”ê°€?

================================================================================
ê²€ì¦ ì™„ë£Œ í›„ ì´ ë¶€ë¶„ì— ê²€í†  ê²°ê³¼ë¥¼ ê¸°ë¡í•˜ì„¸ìš”
================================================================================

[ ] ê²€ì¦ ì™„ë£Œ
[ ] ë°ì´í„° ì •í™•ì„± í™•ì¸
[ ] í• ë£¨ì‹œë„¤ì´ì…˜ ì—†ìŒ í™•ì¸
[ ] ì°¨íŠ¸ ìœ í˜• ì ì ˆì„± í™•ì¸

ê²€ì¦ì: ___________
ê²€ì¦ì¼: ___________
ê²€ì¦ ê²°ê³¼:
_________________________________________________________________________

================================================================================
"""

        return content

    def _generate_section_verification_content(self, query: str, section_title: str, section_data_list: List[SearchResult], section_content: str, timestamp: str) -> str:
        """ì„¹ì…˜ ê²€ì¦ ë¡œê·¸ ë‚´ìš© ìƒì„±"""
        content = f"""
================================================================================
                          ì„¹ì…˜ ìƒì„± ê²€ì¦ ë¡œê·¸
================================================================================

ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
íƒ€ì„ìŠ¤íƒ¬í”„: {timestamp}

================================================================================
ì„¹ì…˜ ìƒì„± ì •ë³´
================================================================================

ì‚¬ìš©ì ì¿¼ë¦¬:
{query}

ì„¹ì…˜ ì œëª©:
{section_title}

================================================================================
ì„¹ì…˜ì—ì„œ ì‚¬ìš©í•œ ë°ì´í„° (ì´ {len(section_data_list)}ê°œ)
================================================================================
"""

        # ì„¹ì…˜ ë°ì´í„° ìƒì„¸ ì •ë³´
        for i, data_item in enumerate(section_data_list):
            source = getattr(data_item, 'source', 'Unknown')
            title = getattr(data_item, 'title', 'No Title')
            content_text = getattr(data_item, 'content', '')
            url = getattr(data_item, 'url', '')
            score = getattr(data_item, 'score', 0.0)
            doc_type = getattr(data_item, 'document_type', 'unknown')

            content += f"""
[ë°ì´í„° ì¸ë±ìŠ¤ {i}]
  ì¶œì²˜: {source}
  ì œëª©: {title}
  URL: {url}
  ì ìˆ˜: {score:.3f}
  íƒ€ì…: {doc_type}
  ë‚´ìš©:
  {content_text[:]}

"""

        content += f"""
================================================================================
ìƒì„±ëœ ì„¹ì…˜ ë‚´ìš©
================================================================================

{section_content}

================================================================================
ê²€ì¦ í¬ì¸íŠ¸
================================================================================

1. ë‚´ìš© ì •í™•ì„±: ì„¹ì…˜ ë‚´ìš©ì´ ì‹¤ì œ ë°ì´í„°ì— ê¸°ë°˜í•˜ëŠ”ê°€?
2. ë°ì´í„° ì¶œì²˜: ì„¹ì…˜ì— ì–¸ê¸‰ëœ ì •ë³´ê°€ ìœ„ì˜ ì„¹ì…˜ ë°ì´í„°ì—ì„œ ë‚˜ì˜¨ ê²ƒì¸ê°€?
3. ë…¼ë¦¬ì  ì¼ê´€ì„±: ì„¹ì…˜ êµ¬ì¡°ì™€ ë‚´ìš©ì´ ë…¼ë¦¬ì ìœ¼ë¡œ ì¼ê´€ëœê°€?
4. í• ë£¨ì‹œë„¤ì´ì…˜ ì—¬ë¶€: ì‹¤ì œ ë°ì´í„°ì— ì—†ëŠ” ë‚´ìš©ì´ ì„¹ì…˜ì— í¬í•¨ë˜ì—ˆëŠ”ê°€?
5. ì™„ì„±ë„: ì„¹ì…˜ì´ ì‚¬ìš©ì ì§ˆë¬¸ì— ì ì ˆíˆ ë‹µë³€í•˜ê³  ìˆëŠ”ê°€?

================================================================================
ê²€ì¦ ì™„ë£Œ í›„ ì´ ë¶€ë¶„ì— ê²€í†  ê²°ê³¼ë¥¼ ê¸°ë¡í•˜ì„¸ìš”
================================================================================

[ ] ê²€ì¦ ì™„ë£Œ
[ ] ë‚´ìš© ì •í™•ì„± í™•ì¸
[ ] í• ë£¨ì‹œë„¤ì´ì…˜ ì—†ìŒ í™•ì¸
[ ] ë°ì´í„° ì¶œì²˜ ì ì ˆì„± í™•ì¸
[ ] ì™„ì„±ë„ í™•ì¸

ê²€ì¦ì: ___________
ê²€ì¦ì¼: ___________
ê²€ì¦ ê²°ê³¼:
_________________________________________________________________________

================================================================================
"""

        return content

